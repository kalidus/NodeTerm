/**
 * ModelMemoryService - Monitor PASIVO de memoria para modelos de IA locales
 * 
 * ‚öôÔ∏è ARQUITECTURA:
 * - MONITOREO: Observa RAM y modelos cada 30 segundos (SIN acciones autom√°ticas)
 * - ESTAD√çSTICAS: Emite eventos con datos actualizados para widget
 * - CARGA DE MODELOS: Usa /api/generate con keep_alive para mantener en memoria
 * - SIN ELIMINACI√ìN: NUNCA usa /api/delete (eso borra los archivos permanentemente)
 * 
 * Funcionalidades:
 * - ‚úÖ Detecta RAM disponible en el sistema
 * - ‚úÖ Detecta modelos cargados en Ollama
 * - ‚úÖ Monitorea GPU memory (NVIDIA/AMD/Apple)
 * - ‚úÖ Estad√≠sticas y reportes en tiempo real
 * - ‚úÖ Contexto din√°mico seg√∫n RAM disponible
 * - ‚úÖ Carga modelos en memoria usando /api/generate (NO delete)
 * - ‚úÖ Conf√≠a en que Ollama descarga autom√°ticamente modelos inactivos
 * 
 * ‚ùå NUNCA: /api/delete (borra archivos permanentemente)
 * ‚ùå OBSOLETO: Auto-descarga LRU (ahora conf√≠a en Ollama)
 * ‚ùå OBSOLETO: L√≠mites autom√°ticos (ahora es solo informaci√≥n)
 */

// ‚úÖ Detectar si estamos en Node.js o navegador
const isNodeEnvironment = typeof window === 'undefined';

// ‚úÖ Cargar m√≥dulos de Node.js SOLO si estamos en Node.js
let os = null;
if (isNodeEnvironment) {
  try {
    os = require('os');
  } catch (e) {
    // Fallback
    os = null;
  }
}

// ‚úÖ Fallback para EventEmitter en navegador
let EventEmitter = null;
if (isNodeEnvironment) {
  try {
    EventEmitter = require('events');
  } catch (e) {
    // Fallback a polyfill
    EventEmitter = null;
  }
}

// Si estamos en navegador, usar polyfill de EventEmitter
if (!EventEmitter) {
  EventEmitter = class {
    constructor() {
      this.listeners = {};
    }
    on(event, fn) {
      if (!this.listeners[event]) this.listeners[event] = [];
      this.listeners[event].push(fn);
      return this;
    }
    emit(event, ...args) {
      if (this.listeners[event]) {
        this.listeners[event].forEach(fn => fn(...args));
      }
    }
    removeListener(event, fn) {
      if (this.listeners[event]) {
        this.listeners[event] = this.listeners[event].filter(f => f !== fn);
      }
      return this;
    }
    addListener(event, fn) {
      return this.on(event, fn);
    }
  };
}

// ‚úÖ Importar GPUMemoryService
let gpuMemoryService = null;
if (isNodeEnvironment) {
  try {
    gpuMemoryService = require('./GPUMemoryService').default;
  } catch (e) {
    // GPUMemoryService no disponible
    gpuMemoryService = null;
  }
}

class ModelMemoryService extends EventEmitter {
  constructor(ollamaUrl = 'http://localhost:11434') {
    super();
    
    this.ollamaUrl = ollamaUrl;
    this.loadedModels = new Map(); // { modelName: { size, memory, loadedAt } }
    this.monitoringInterval = null;
    this.monitoringEnabled = false;
    this.checkInterval = 30000; // 30 segundos - solo para actualizar datos
    this.lastSystemMemory = null; // Cache del √∫ltimo estado del sistema
  }

  /**
   * ‚úÖ 1. OBTENER MEMORIA DEL SISTEMA (RAM + GPU)
   * Retorna informaci√≥n de RAM disponible en el SO
   * 
   * Primero intenta obtener datos REALES v√≠a IPC (Electron - window.electron.ipcRenderer)
   * Si no est√° disponible, usa el m√≥dulo 'os' de Node.js
   * Si nada funciona, devuelve valores por defecto
   */
  async getSystemMemory() {
    // Opci√≥n 1: Intentar obtener datos REALES v√≠a IPC (Electron)
    if (typeof window !== 'undefined' && window.electron && window.electron.ipcRenderer) {
      try {
        const stats = await window.electron.ipcRenderer.invoke('system:get-memory-stats');
        if (stats && stats.ok) {
          return {
            totalMB: stats.totalMB,
            freeMB: stats.freeMB,
            usedMB: stats.usedMB,
            usagePercent: stats.usagePercent
          };
        }
      } catch (error) {
        console.warn('[ModelMemory] ‚ö†Ô∏è IPC error:', error.message);
      }
    }

    // Opci√≥n 2: Fallback - usar m√≥dulo 'os' si est√° disponible
    if (os) {
      try {
        const totalMemory = os.totalmem();
        const freeMemory = os.freemem();
        const usedMemory = totalMemory - freeMemory;

        return {
          totalMB: Math.round(totalMemory / 1024 / 1024),
          freeMB: Math.round(freeMemory / 1024 / 1024),
          usedMB: Math.round(usedMemory / 1024 / 1024),
          usagePercent: Math.round((usedMemory / totalMemory) * 100)
        };
      } catch (error) {
        console.warn('[ModelMemory] ‚ö†Ô∏è Error con Node.js os module');
      }
    }

    // Opci√≥n 3: Fallback final - valores por defecto
    console.warn('[ModelMemory] ‚ö†Ô∏è Usando valores por defecto (no se pudo obtener datos reales)');
    return {
      totalMB: 16000,
      freeMB: 8000,
      usedMB: 8000,
      usagePercent: 50
    };
  }

  /**
   * üéÆ NUEVO: Detectar memoria de GPU si est√° disponible
   * Retorna { model: string, totalVRAM_MB, usedVRAM_MB } o null
   */
  _detectGPUMemory() {
    // Nota: Esta es una funci√≥n stub que podr√≠a conectar con:
    // - nvidia-smi para NVIDIA GPUs (requiere system call)
    // - ROCm para AMD GPUs
    // - Metal para Apple Silicon
    // Por ahora, retorna null (ser√≠a necesario acceso a proceso del sistema)
    
    // Futuro: Implementar si se necesita
    return null;
  }

  /**
   * ‚úÖ 2. OBTENER MODELOS CARGADOS EN OLLAMA
   * Usa /api/ps para detectar qu√© modelos est√°n actualmente en RAM
   */
  async getLoadedModels() {
    try {
      const response = await fetch(`${this.ollamaUrl}/api/ps`);
      
      if (!response.ok) {
        console.warn(`[ModelMemory] /api/ps no disponible (HTTP ${response.status})`);
        return new Map();
      }

      const data = await response.json();
      
      this.loadedModels.clear();
      
      if (data.models && Array.isArray(data.models)) {
        for (const model of data.models) {
          this.loadedModels.set(model.name, {
            size: model.size || 0,
            memory: Math.round((model.size || 0) / 1024 / 1024), // Convertir a MB
            loadedAt: new Date(model.loaded_at || Date.now()),
            lastUsedAt: new Date(model.expires_at || Date.now())
          });
        }
      }

      this.emit('modelsUpdated', this.loadedModels);
      return this.loadedModels;
    } catch (error) {
      console.error('[ModelMemory] Error detectando modelos:', error.message);
      return new Map();
    }
  }

  /**
   * ‚úÖ 3. CARGAR MODELO EN MEMORIA
   * Usa /api/generate con keep_alive para mantener el modelo en memoria
   * NO usa /api/delete (que borra archivos permanentemente)
   * 
   * ‚ö†Ô∏è IMPORTANTE: Este m√©todo CARGA el modelo cuando se selecciona
   * Ollama autom√°ticamente descargar√° de RAM modelos inactivos seg√∫n OLLAMA_MAX_LOADED_MODELS
   */
  async loadModelToMemory(modelName) {
    try {
      // Usar /api/generate para hacer "warm up" del modelo
      // keep_alive: -1 = mantener indefinidamente (hasta que Ollama decida descargarlo por otros modelos)
      // stream: false = no queremos generar respuesta, solo cargar el modelo
      
      const response = await fetch(`${this.ollamaUrl}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: modelName,
          prompt: '', // Prompt vac√≠o, solo para cargar
          stream: false,
          keep_alive: -1 // Mantener modelo en memoria indefinidamente
        })
      });

      if (response.ok) {
        this.emit('modelLoaded', modelName);
        return true;
      } else {
        console.warn(`[ModelMemory] ‚ö†Ô∏è HTTP ${response.status} al cargar ${modelName}`);
        return false;
      }
    } catch (error) {
      console.error(`[ModelMemory] ‚ö†Ô∏è Error cargando ${modelName}:`, error.message);
      this.emit('loadFailed', { modelName, error: error.message });
      return false;
    }
  }

  /**
   * ‚úÖ DESCARGAR MODELO DE RAM (Opci√≥n B: Control Manual)
   * 
   * Descarga el modelo de RAM INMEDIATAMENTE usando keep_alive: 0
   * ‚ö†Ô∏è El archivo del modelo PERMANECE en disco (~/.ollama/models)
   * Solo se descarga de RAM, nunca se borra el archivo
   * 
   * Ollama autom√°ticamente tambi√©n descarga modelos cuando:
   * 1. El modelo est√° inactivo por tiempo (configurable con keep_alive en requests)
   * 2. Se alcanza OLLAMA_MAX_LOADED_MODELS (por defecto 3 modelos)
   * 3. Se necesita memoria para otros modelos
   */
  async unloadModel(modelName) {
    try {
      // Usar /api/generate con keep_alive: 0 para descargar inmediatamente
      // stream: false = no queremos generar respuesta
      const response = await fetch(`${this.ollamaUrl}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: modelName,
          prompt: '',
          stream: false,
          keep_alive: 0  // ‚Üê Descargar inmediatamente de RAM
        })
      });

      if (response.ok) {
        this.emit('modelUnloaded', modelName);
        return true;
      } else {
        console.warn(`[ModelMemory] ‚ö†Ô∏è HTTP ${response.status} al descargar ${modelName}`);
        return false;
      }
    } catch (error) {
      console.error(`[ModelMemory] ‚ö†Ô∏è Error descargando ${modelName}:`, error.message);
      this.emit('unloadFailed', { modelName, error: error.message });
      return false;
    }
  }

  /**
   * ‚úÖ 4. DESCARGAR M√öLTIPLES MODELOS
   */
  async unloadMultiple(modelNames) {
    const results = [];
    for (const modelName of modelNames) {
      const success = await this.unloadModel(modelName);
      results.push({ modelName, success });
    }
    return results;
  }

  /**
   * ‚úÖ 5. OBTENER ESTAD√çSTICAS DETALLADAS
   * Usa memoria cacheada del √∫ltimo monitoreo
   */
  getMemoryStats() {
    // Usar el √∫ltimo sistema memory cacheado
    const systemMem = this.lastSystemMemory || {
      totalMB: 16000,
      freeMB: 8000,
      usedMB: 8000,
      usagePercent: 50
    };

    const models = Array.from(this.loadedModels.entries()).map(([name, info]) => ({
      name,
      sizeGB: (info.size / 1024 / 1024 / 1024).toFixed(2),
      sizeMB: info.memory,
      loadedSince: info.loadedAt.toLocaleTimeString(),
      minutesAgo: Math.round((Date.now() - info.loadedAt.getTime()) / 60000)
    }));

    const totalMemoryUsedByModels = models.reduce((sum, m) => sum + m.sizeMB, 0);

    return {
      system: systemMem,
      models,
      totalModelMemoryMB: totalMemoryUsedByModels,
      totalModelMemoryGB: (totalMemoryUsedByModels / 1024).toFixed(2),
      modelsCount: models.length,
      memoryLimitMB: this.memoryLimit,
      isOverLimit: totalMemoryUsedByModels > this.memoryLimit,
      exceededByMB: Math.max(0, totalMemoryUsedByModels - this.memoryLimit)
    };
  }

  /**
   * ‚úÖ 6. MONITOREO PASIVO
   * Solo reporta datos, SIN tomar acciones autom√°ticas
   * 
   * ‚ö†Ô∏è NOTA: Descarga solo por acci√≥n manual del usuario (bot√≥n en widget)
   */
  async monitorMemory() {
    // El monitoreo obtiene datos REALES del sistema
    await this.getLoadedModels();
    
    // ‚úÖ Obtener datos REALES de RAM (v√≠a IPC si est√° disponible)
    this.lastSystemMemory = await this.getSystemMemory();
    
    const stats = this.getMemoryStats();
    
    // Solo emitir evento para que el widget se actualice
    this.emit('memoryUpdated', stats);
    
    return stats;
  }

  /**
   * ‚úÖ 7. VALIDAR DISPONIBILIDAD PARA CARGAR MODELO
   */
  async canLoadModel(modelSizeGB) {
    const stats = this.getMemoryStats();
    const modelSizeMB = modelSizeGB * 1024;
    const availableMB = stats.system.freeMB;

    const canFit = modelSizeMB <= availableMB;
    const wouldExceedLimit = (stats.totalModelMemoryMB + modelSizeMB) > this.memoryLimit;

    return {
      canFit,
      wouldExceedLimit,
      availableMB,
      neededMB: modelSizeMB,
      message: canFit 
        ? 'OK: Suficiente RAM disponible'
        : `ERROR: Necesitas ${Math.round((modelSizeMB - availableMB) / 1024)}GB m√°s`
    };
  }

  /**
   * ‚úÖ 8. CALCULAR CONTEXTO DIN√ÅMICO
   * Ajusta tama√±o de contexto seg√∫n RAM disponible
   */
  calcDynamicContext(freeRAMMB) {
    if (freeRAMMB < 1000) return 1000;    // Crisis
    if (freeRAMMB < 2000) return 2000;    // Bajo
    if (freeRAMMB < 4000) return 4000;    // Normal
    if (freeRAMMB < 8000) return 6000;    // Bueno
    return 8000;                          // √ìptimo
  }

  /**
   * ‚úÖ 9. INICIAR MONITOREO CONTINUO (PASIVO)
   * 
   * Solo actualiza datos cada 30 segundos, SIN tomar acciones
   */
  startMonitoring() {
    if (this.monitoringEnabled) {
      console.warn('[ModelMemory] Monitoreo ya est√° activo');
      return;
    }

    this.monitoringEnabled = true;

    const monitor = async () => {
      try {
        // Solo obtener datos, sin acciones autom√°ticas
        await this.monitorMemory();

      } catch (error) {
        console.error('[ModelMemory] Error en monitoreo:', error.message);
      }

      // Programar siguiente chequeo
      if (this.monitoringEnabled) {
        setTimeout(monitor, this.checkInterval);
      }
    };

    monitor();
  }

  /**
   * ‚úÖ 10. DETENER MONITOREO
   */
  stopMonitoring() {
    this.monitoringEnabled = false;
    this.emit('monitoringStopped');
  }

  /**
   * ‚ÑπÔ∏è NOTA: Sin setMemoryLimit - no hay auto-unload basado en l√≠mites
   * El usuario solo descarga manualmente via bot√≥n en widget
   */

  /**
   * ‚úÖ 12. LIMPIAR AL CERRAR
   */
  async cleanup() {
    this.stopMonitoring();
  }

  /**
   * üéÆ NUEVO: Obtener estad√≠sticas de GPU v√≠a IPC
   * Soporta NVIDIA, AMD, Apple Silicon
   */
  async getGPUStats() {
    try {
      // Opci√≥n 1: Intentar obtener datos REALES v√≠a IPC (Electron)
      if (typeof window !== 'undefined' && window.electron && window.electron.system) {
        try {
          const gpuStats = await window.electron.system.getGPUStats();
          
          if (gpuStats && gpuStats.ok && gpuStats.type) {
            return {
              available: true,
              gpus: [{
                name: `${gpuStats.type.toUpperCase()} GPU`,
                totalGB: (gpuStats.totalMB / 1024).toFixed(2),
                usedGB: (gpuStats.usedMB / 1024).toFixed(2),
                freeGB: (gpuStats.freeMB / 1024).toFixed(2),
                usagePercent: gpuStats.usagePercent || 0,
                status: '‚úÖ Activa'
              }]
            };
          }
        } catch (error) {
          console.warn('[ModelMemory] IPC GPU error:', error.message);
        }
      }

      // Fallback: Si no hay GPU o IPC no disponible
      return {
        available: false,
        gpus: []
      };
    } catch (error) {
      console.warn('[ModelMemory] Error obteniendo stats GPU:', error.message);
      return {
        available: false,
        gpus: []
      };
    }
  }

  /**
   * ‚úÖ 13. OBTENER INFO FORMATEADA PARA UI (con GPU)
   */
  async formatStats() {
    const stats = this.getMemoryStats();
    const gpuStats = await this.getGPUStats();

    return {
      header: {
        systemUsage: `${stats.system.usedMB}MB / ${stats.system.totalMB}MB (${stats.system.usagePercent}%)`,
        modelCount: stats.modelsCount,
        modelTotalGB: stats.totalModelMemoryGB,
        limitGB: (stats.memoryLimitMB / 1024).toFixed(1),
        status: stats.isOverLimit ? '‚ö†Ô∏è SOBRE L√çMITE' : '‚úÖ OK'
      },
      gpu: gpuStats, // üéÆ Agregar stats de GPU
      models: stats.models.map(m => ({
        name: m.name,
        size: m.sizeGB,
        age: `${m.minutesAgo}m`,
        summary: `${m.name} (${m.sizeGB}GB, hace ${m.minutesAgo}m)`
      }))
    };
  }
}

// Singleton instance
const modelMemoryService = new ModelMemoryService();

export default modelMemoryService;

