/**
 * ModelManager - Gestión centralizada de modelos de IA
 * 
 * Este módulo contiene:
 * - Configuración de todos los modelos disponibles (remotos y locales)
 * - Métodos para consultar y validar modelos
 * - Métodos para obtener modelos funcionales
 */

// Configuración de modelos remotos
const REMOTE_MODELS = [
  { 
    id: 'gpt-4', 
          name: 'GPT-4', 
          provider: 'openai', 
          endpoint: 'https://api.openai.com/v1/chat/completions', 
          performance: 'high',
          description: 'El modelo más avanzado de OpenAI. Excelente para tareas complejas, programación, análisis y razonamiento profundo.',
          useCases: ['Programación avanzada', 'Análisis de datos', 'Investigación', 'Escritura técnica', 'Resolución de problemas complejos'],
          strengths: ['Razonamiento superior', 'Código de alta calidad', 'Análisis detallado', 'Creatividad avanzada'],
          bestFor: 'Desarrolladores, investigadores, analistas y usuarios que necesitan la máxima calidad',
          context: '128K tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '220B',
          quantization: 'Full Precision'
        },
        { 
          id: 'gpt-3.5-turbo', 
          name: 'GPT-3.5 Turbo', 
          provider: 'openai', 
          endpoint: 'https://api.openai.com/v1/chat/completions', 
          performance: 'medium',
          description: 'Modelo rápido y eficiente de OpenAI. Ideal para uso general, programación básica y conversaciones.',
          useCases: ['Programación básica', 'Asistencia general', 'Escritura', 'Traducción', 'Resumen de textos'],
          strengths: ['Velocidad alta', 'Costo eficiente', 'Buena calidad general', 'Respuestas rápidas'],
          bestFor: 'Uso diario, programación básica, estudiantes y usuarios que buscan velocidad',
          context: '16K tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '175B',
          quantization: 'Full Precision'
        },
        { 
          id: 'claude-3-opus', 
          name: 'Claude 3 Opus', 
          provider: 'anthropic', 
          endpoint: 'https://api.anthropic.com/v1/messages', 
          performance: 'high',
          description: 'El modelo más potente de Anthropic. Destaca en análisis, escritura creativa y comprensión de contexto.',
          useCases: ['Análisis profundo', 'Escritura creativa', 'Investigación académica', 'Edición de textos', 'Análisis de documentos'],
          strengths: ['Comprensión superior', 'Escritura excelente', 'Análisis detallado', 'Creatividad'],
          bestFor: 'Escritores, investigadores, analistas y usuarios que necesitan análisis profundo',
          context: '200K tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '200B',
          quantization: 'Full Precision'
        },
        { 
          id: 'claude-3-sonnet', 
          name: 'Claude 3 Sonnet', 
          provider: 'anthropic', 
          endpoint: 'https://api.anthropic.com/v1/messages', 
          performance: 'medium',
          description: 'Modelo equilibrado de Anthropic. Buen balance entre velocidad y calidad para uso general.',
          useCases: ['Programación', 'Asistencia general', 'Análisis de datos', 'Escritura', 'Resolución de problemas'],
          strengths: ['Balance velocidad/calidad', 'Buena programación', 'Análisis sólido', 'Respuestas coherentes'],
          bestFor: 'Desarrolladores, profesionales y usuarios que buscan un buen balance',
          context: '200K tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '100B',
          quantization: 'Full Precision'
        },
        { 
          id: 'gemini-2.5-flash', 
          name: 'Gemini 2.5 Flash', 
          provider: 'google', 
          endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent', 
          performance: 'high',
          description: 'El modelo más reciente y rápido de Google. Excelente para tareas generales y programación con velocidad superior.',
          useCases: ['Programación rápida', 'Análisis de datos', 'Investigación', 'Escritura técnica', 'Resolución de problemas'],
          strengths: ['Velocidad excepcional', 'Código de calidad', 'Análisis rápido', 'Multimodal'],
          bestFor: 'Desarrolladores que necesitan velocidad, analistas y usuarios que buscan respuestas rápidas',
          context: '1M tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '~100B',
          quantization: 'Full Precision'
        },
        { 
          id: 'gemini-2.5-pro', 
          name: 'Gemini 2.5 Pro', 
          provider: 'google', 
          endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent', 
          performance: 'high',
          description: 'El modelo más potente de Google. Máxima calidad para tareas complejas y análisis profundos.',
          useCases: ['Programación avanzada', 'Análisis complejo', 'Investigación profunda', 'Escritura técnica', 'Resolución de problemas difíciles'],
          strengths: ['Máxima calidad', 'Razonamiento superior', 'Análisis profundo', 'Código avanzado'],
          bestFor: 'Desarrolladores senior, investigadores y usuarios que necesitan la máxima calidad',
          context: '2M tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '~200B',
          quantization: 'Full Precision'
        },
        { 
          id: 'gemini-2.0-flash-exp', 
          name: 'Gemini 2.0 Flash', 
          provider: 'google', 
          endpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent', 
          performance: 'high',
          description: 'Modelo experimental de Google con capacidades avanzadas. Ideal para probar nuevas funcionalidades.',
          useCases: ['Experimentación', 'Funcionalidades nuevas', 'Análisis innovador', 'Programación experimental'],
          strengths: ['Funcionalidades nuevas', 'Capacidades experimentales', 'Innovación', 'Tecnología de vanguardia'],
          bestFor: 'Usuarios avanzados, investigadores y desarrolladores que quieren probar lo último',
          context: '1M tokens',
          ramRequired: 'N/A (Cloud)',
          parameters: '~100B',
          quantization: 'Full Precision'
        }
];

// Configuración de modelos locales (Ollama)
const LOCAL_OLLAMA_MODELS = [
  { 
    id: 'llama3.2:latest', 
            name: 'Llama 3.2 (Latest - 3B)', 
            size: '2GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo predeterminado de Llama 3.2 optimizado para texto. Versión recomendada para la mayoría de usuarios.',
            useCases: ['Programación general', 'Asistencia técnica', 'Procesamiento de texto', 'Uso general'],
            strengths: ['Versión recomendada', 'Optimizado para texto', 'Excelente balance', 'Versatilidad'],
            bestFor: 'Usuarios que quieren la versión más reciente optimizada para texto',
            context: '8K tokens',
            ramRequired: '6-8GB',
            parameters: '3B',
            quantization: 'Q4, Q5, Q8'
          },
{id: 'llama3.2:1b', 
            name: 'Llama 3.2 (1B)', 
            size: '1.3GB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo más ligero de Llama 3.2 optimizado para dispositivos con recursos limitados. Ideal para uso básico.',
            useCases: ['Procesamiento básico', 'Dispositivos móviles', 'Uso ligero', 'Tareas simples'],
            strengths: ['Muy ligero', 'Rápido', 'Eficiente', 'Ideal para móviles'],
            bestFor: 'Usuarios con dispositivos limitados o que necesitan máxima velocidad',
            context: '4K tokens',
            ramRequired: '2-4GB',
            parameters: '1B',
            quantization: 'Q4'
          },
{id: 'llama3.1:8b', 
            name: 'Llama 3.1 (8B)', 
            size: '4.7GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'good',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión moderna de Llama 3.1 con contexto extendido hasta 128K tokens. Excelente para análisis de documentos largos y conversaciones extensas.',
            useCases: ['Análisis de documentos largos', 'Conversaciones extensas', 'Programación compleja', 'Investigación', 'Análisis de código masivo'],
            strengths: ['Contexto ultra-largo (128K nativo)', 'Optimizado para instrucciones', 'Excelente para documentos', 'Análisis profundo'],
            bestFor: 'Análisis de documentos extensos, conversaciones largas y tareas que requieren contexto amplio',
            context: '128K tokens',
            ramRequired: '8-10GB',
            parameters: '8B',
            quantization: 'Q4, Q5, Q8'
          },
{id: 'llama3.1:70b', 
            name: 'Llama 3.1 (70B)', 
            size: '40GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente + GPU potente',
            description: 'Modelo más potente de Llama 3.1 con contexto extendido a 128K tokens. Excelente para programación avanzada, análisis complejos y documentos largos con máxima calidad.',
            useCases: ['Programación avanzada', 'Análisis de código complejo', 'Investigación', 'Razonamiento profundo', 'Análisis masivo de documentos', 'Documentos muy largos'],
            strengths: ['Contexto ultra-largo (128K)', 'Excelente programación', 'Razonamiento superior', 'Código de máxima calidad', 'Análisis profundo'],
            bestFor: 'Desarrolladores senior, investigadores y usuarios con hardware potente que necesitan máxima calidad y contextos largos',
            context: '128K tokens',
            ramRequired: '45-70GB',
            parameters: '70B',
            quantization: 'Q2, Q3, Q4'
          },
{id: 'llama3', 
            name: 'Llama 3 (8B)', 
            size: '4.7GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo estable y confiable de Meta. Buen balance entre rendimiento y recursos para uso general.',
            useCases: ['Programación general', 'Asistencia técnica', 'Análisis de datos', 'Escritura', 'Resolución de problemas'],
            strengths: ['Estabilidad', 'Buena programación', 'Análisis sólido', 'Respuestas coherentes'],
            bestFor: 'Uso general, programación básica-intermedia y usuarios que buscan estabilidad',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '8B',
            quantization: 'Q4, Q5, Q8'
          },
{id: 'llama3:70b', 
            name: 'Llama 3 (70B)', 
            size: '40GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo más potente de Llama 3. Excelente para programación avanzada y análisis complejos con máxima calidad.',
            useCases: ['Programación avanzada', 'Análisis de código complejo', 'Investigación', 'Razonamiento profundo', 'Tareas de alta complejidad'],
            strengths: ['Excelente programación', 'Razonamiento superior', 'Código de máxima calidad', 'Análisis profundo', 'Máxima potencia'],
            bestFor: 'Desarrolladores senior, investigadores y usuarios que necesitan máxima calidad en programación',
            context: '8K tokens',
            ramRequired: '40-64GB',
            parameters: '70B',
            quantization: 'Q2, Q3, Q4, Q5'
          },
{id: 'llama2', 
            name: 'Llama 2 (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo clásico de Meta. Versión anterior estable y confiable para uso general.',
            useCases: ['Programación básica', 'Asistencia general', 'Análisis de texto', 'Resolución de problemas'],
            strengths: ['Estabilidad', 'Buena comprensión', 'Respuestas coherentes', 'Amplio conocimiento'],
            bestFor: 'Uso general, programación básica y usuarios que buscan estabilidad',
            context: '4K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5, Q8'
          },
{id: 'deepseek-r1:latest', 
            name: 'DeepSeek R1 (Latest)', 
            size: '5.2GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'DeepSeek-R1 es una familia de modelos de razonamiento abierto con rendimiento que se acerca a modelos líderes como O3 y Gemini 2.5 Pro. Versión latest optimizada.',
            useCases: ['Razonamiento profundo', 'Programación compleja', 'Análisis matemático', 'Resolución de problemas complejos', 'Pensamiento crítico'],
            strengths: ['Razonamiento superior', 'Programación avanzada', 'Lógica matemática', 'Análisis profundo', 'Contexto extenso'],
            bestFor: 'Desarrolladores senior, matemáticos, investigadores y usuarios que necesitan razonamiento profundo y contexto extenso',
            context: '128K tokens',
            ramRequired: '10-12GB',
            parameters: '8B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:1.5b', 
            name: 'DeepSeek R1 (1.5B)', 
            size: '1.1GB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión compacta de DeepSeek-R1. Ideal para dispositivos con recursos limitados pero que requieren razonamiento avanzado.',
            useCases: ['Razonamiento básico', 'Programación ligera', 'Análisis rápido', 'Dispositivos con recursos limitados'],
            strengths: ['Muy compacto', 'Razonamiento eficiente', 'Bajo consumo', 'Contexto extenso'],
            bestFor: 'Usuarios con recursos limitados que buscan razonamiento avanzado en un paquete compacto',
            context: '128K tokens',
            ramRequired: '4-6GB',
            parameters: '1.5B',
            quantization: 'Q4'
          },
{id: 'deepseek-r1:7b', 
            name: 'DeepSeek R1 (7B)', 
            size: '4.7GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 7B de DeepSeek-R1. Excelente balance entre rendimiento y eficiencia para razonamiento profundo.',
            useCases: ['Razonamiento profundo', 'Programación compleja', 'Análisis matemático', 'Resolución de algoritmos', 'Debugging avanzado'],
            strengths: ['Razonamiento superior', 'Balance rendimiento/eficiencia', 'Lógica matemática', 'Análisis profundo', 'Contexto extenso'],
            bestFor: 'Desarrolladores que buscan excelente razonamiento con eficiencia de recursos',
            context: '128K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:8b', 
            name: 'DeepSeek R1 (8B)', 
            size: '5.2GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'DeepSeek-R1 8B. Modelo especializado en razonamiento y programación. Ideal para tareas que requieren lógica profunda con contexto extenso.',
            useCases: ['Programación compleja', 'Razonamiento lógico', 'Análisis matemático', 'Resolución de algoritmos', 'Debugging avanzado'],
            strengths: ['Razonamiento superior', 'Programación avanzada', 'Lógica matemática', 'Análisis profundo', 'Contexto extenso'],
            bestFor: 'Desarrolladores senior, matemáticos, investigadores y usuarios que necesitan razonamiento profundo',
            context: '128K tokens',
            ramRequired: '10-12GB',
            parameters: '8B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:14b', 
            name: 'DeepSeek R1 (14B)', 
            size: '9.0GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 14B de DeepSeek-R1. Mayor capacidad de razonamiento y análisis para tareas complejas que requieren comprensión profunda.',
            useCases: ['Razonamiento avanzado', 'Programación de nivel experto', 'Análisis matemático complejo', 'Investigación', 'Resolución de problemas complejos'],
            strengths: ['Razonamiento excepcional', 'Comprensión profunda', 'Análisis complejo', 'Contexto extenso', 'Precisión superior'],
            bestFor: 'Investigadores, desarrolladores expertos y profesionales que requieren máximo razonamiento',
            context: '128K tokens',
            ramRequired: '16-20GB',
            parameters: '14B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:32b', 
            name: 'DeepSeek R1 (32B)', 
            size: '20GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 32B de DeepSeek-R1. Modelo de alto rendimiento para razonamiento extremadamente complejo y análisis de nivel experto.',
            useCases: ['Razonamiento de nivel experto', 'Programación avanzada', 'Análisis matemático complejo', 'Investigación avanzada', 'Resolución de problemas extremadamente complejos'],
            strengths: ['Razonamiento excepcional', 'Comprensión profunda', 'Análisis complejo', 'Contexto extenso', 'Precisión máxima'],
            bestFor: 'Investigadores avanzados, desarrolladores expertos y profesionales que requieren máximo razonamiento y capacidad de análisis',
            context: '128K tokens',
            ramRequired: '40-48GB',
            parameters: '32B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:70b', 
            name: 'DeepSeek R1 (70B)', 
            size: '43GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 70B de DeepSeek-R1. Modelo de máximo rendimiento para razonamiento de nivel experto, comparable con modelos líderes como O3 y Gemini 2.5 Pro.',
            useCases: ['Razonamiento de nivel experto', 'Programación avanzada', 'Análisis matemático complejo', 'Investigación avanzada', 'Resolución de problemas extremadamente complejos'],
            strengths: ['Razonamiento excepcional', 'Comprensión profunda', 'Análisis complejo', 'Contexto extenso', 'Precisión máxima', 'Rendimiento de nivel líder'],
            bestFor: 'Investigadores avanzados, desarrolladores expertos y profesionales que requieren máximo razonamiento y capacidad de análisis',
            context: '128K tokens',
            ramRequired: '80-96GB',
            parameters: '70B',
            quantization: 'Q4, Q5'
          },
{id: 'deepseek-r1:671b', 
            name: 'DeepSeek R1 (671B)', 
            size: '404GB', 
            downloaded: false, 
            performance: 'high',
            mcpCompatibility: 'excellent',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente + GPU muy potente',
            description: 'Versión completa 671B de DeepSeek-R1. El modelo más grande de la familia, con rendimiento que se acerca a modelos líderes como O3 y Gemini 2.5 Pro. Requiere hardware de nivel enterprise.',
            useCases: ['Razonamiento de nivel experto', 'Programación avanzada', 'Análisis matemático complejo', 'Investigación avanzada', 'Resolución de problemas extremadamente complejos', 'Benchmarks de investigación'],
            strengths: ['Razonamiento excepcional', 'Comprensión profunda', 'Análisis complejo', 'Contexto extenso (160K)', 'Precisión máxima', 'Rendimiento de nivel líder', 'Máxima capacidad'],
            bestFor: 'Investigadores avanzados, instituciones de investigación y usuarios con hardware enterprise que requieren máxima capacidad de razonamiento',
            context: '160K tokens',
            ramRequired: '450-500GB',
            parameters: '671B',
            quantization: 'MXFP4'
          },
{id: 'mistral', 
            name: 'Mistral (7B)', 
            size: '4.1GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo europeo eficiente. Excelente para programación y tareas técnicas con buen rendimiento.',
            useCases: ['Programación', 'Análisis técnico', 'Escritura técnica', 'Resolución de problemas', 'Asistencia general'],
            strengths: ['Eficiencia', 'Buena programación', 'Análisis técnico', 'Respuestas precisas'],
            bestFor: 'Desarrolladores, técnicos y usuarios que buscan eficiencia en programación',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'qwen2.5', 
            name: 'Qwen 2.5 (7B)', 
            size: '4.5GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo chino con capacidades multilingües. Excelente para programación y análisis en múltiples idiomas.',
            useCases: ['Programación multilingüe', 'Análisis de código', 'Traducción técnica', 'Asistencia general'],
            strengths: ['Multilingüe', 'Buena programación', 'Análisis de código', 'Flexibilidad'],
            bestFor: 'Desarrolladores internacionales, programación multilingüe y análisis de código',
            context: '128K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'qwen3:8b', 
            name: 'Qwen 3 (8B)', 
            size: '5.0GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo Qwen3 de 8B con contexto extendido hasta 128K tokens. Optimizado para GPU y conversaciones largas.',
            useCases: ['Análisis de documentos largos', 'Conversaciones extensas', 'Programación compleja', 'Análisis profundo', 'Contextos largos'],
            strengths: ['Contexto ultra-largo (128K)', 'Optimización GPU', 'Multilingüe avanzado', 'Razonamiento profundo'],
            bestFor: 'Análisis de documentos extensos, conversaciones largas y tareas que requieren contexto amplio',
            context: '128K tokens',
            ramRequired: '10-12GB',
            parameters: '8B',
            quantization: 'Q4, Q5'
          },
{id: 'qwen3:30b', 
            name: 'Qwen 3 (30B-A3B)', 
            size: '18.6GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente + GPU potente',
            description: 'Modelo Qwen3 de 30B con arquitectura MoE (solo 3B activos). Contexto de 128K tokens para máximo rendimiento.',
            useCases: ['Análisis masivos', 'Investigación compleja', 'Documentos muy largos', 'Razonamiento avanzado', 'Tareas complejas'],
            strengths: ['Contexto ultra-largo (128K)', 'Arquitectura MoE eficiente', 'Rendimiento superior', 'GPU optimizado'],
            bestFor: 'Usuarios con hardware potente que necesitan máximo rendimiento y contextos extremadamente largos',
            context: '128K tokens',
            ramRequired: '20-32GB',
            parameters: '30B (3B activos)',
            quantization: 'Q4, Q5'
          },
{id: 'dolphin-mixtral', 
            name: 'Dolphin Mixtral (8x7B)', 
            size: '26GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente + GPU potente',
            description: 'Dolphin fine-tuned sobre Mixtral MoE. Excelente razonamiento con arquitectura eficiente.',
            useCases: ['Razonamiento avanzado', 'Programación compleja', 'Análisis profundo', 'Instrucciones precisas'],
            strengths: ['Razonamiento superior', 'MoE eficiente', 'Análisis profundo', 'Instrucciones precisas'],
            bestFor: 'Usuarios con GPU que necesitan razonamiento avanzado y eficiencia',
            context: '32K tokens',
            ramRequired: '30-40GB',
            parameters: '8x7B (MoE)',
            quantization: 'Q4, Q5'
          },
{id: 'gpt-oss:20b', 
            name: 'GPT-OSS (20B)', 
            size: '14GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'OpenAI\'s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. Modelo de 20B optimizado para baja latencia.',
            useCases: ['Razonamiento poderoso', 'Tareas agentic', 'Desarrollo versátil', 'Programación', 'Resolución de problemas complejos'],
            strengths: ['Razonamiento superior', 'Versatilidad', 'Calidad de código', 'Análisis complejo', 'Tareas agentic'],
            bestFor: 'Desarrolladores, investigadores y usuarios que necesitan razonamiento y versatilidad',
            context: '128K tokens',
            ramRequired: '16-20GB',
            parameters: '20B',
            quantization: 'MXFP4'
          },
{id: 'gpt-oss:120b', 
            name: 'GPT-OSS (120B)', 
            size: '65GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente + GPU muy potente',
            description: 'OpenAI\'s open-weight 120B model. Modelo más potente para razonamiento y tareas complejas con máxima calidad.',
            useCases: ['Razonamiento avanzado', 'Programación compleja', 'Análisis profundo', 'Tareas agentic', 'Investigación de alto nivel'],
            strengths: ['Razonamiento superior', 'Mayor capacidad', 'Versatilidad', 'Análisis profundo', 'Código de máxima calidad'],
            bestFor: 'Desarrolladores senior, investigadores y usuarios con GPU potente que necesitan máxima capacidad',
            context: '128K tokens',
            ramRequired: '70-80GB',
            parameters: '120B',
            quantization: 'MXFP4'
          },
{id: 'gemma3:latest', 
            name: 'Gemma 3 (Latest)', 
            size: '3.3GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'The current, most capable model that runs on a single GPU. Modelo de Google con soporte para texto e imágenes.',
            useCases: ['Análisis de imágenes', 'Programación con contexto visual', 'Análisis de datos', 'Escritura técnica', 'Tareas multimodales'],
            strengths: ['Multimodal (texto e imagen)', 'Eficiencia', 'Contexto extenso', 'Versatilidad'],
            bestFor: 'Desarrolladores que necesitan análisis multimodal y contexto extenso',
            context: '128K tokens',
            ramRequired: '6-8GB',
            parameters: '4B',
            quantization: 'Q4, Q5'
          },
{id: 'gemma3:270m', 
            name: 'Gemma 3 (270M)', 
            size: '292MB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo ultra-compacto de Gemma 3. Ideal para dispositivos con recursos muy limitados.',
            useCases: ['Tareas básicas', 'Respuestas rápidas', 'Dispositivos móviles', 'Prototipado ligero'],
            strengths: ['Muy compacto', 'Velocidad alta', 'Bajo consumo', 'Fácil instalación'],
            bestFor: 'Dispositivos con recursos muy limitados y tareas básicas rápidas',
            context: '32K tokens',
            ramRequired: '1-2GB',
            parameters: '270M',
            quantization: 'Q4'
          },
{id: 'gemma3:1b', 
            name: 'Gemma 3 (1B)', 
            size: '815MB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión compacta de Gemma 3. Ideal para dispositivos con recursos limitados.',
            useCases: ['Asistencia básica', 'Respuestas rápidas', 'Dispositivos móviles', 'Tareas simples', 'Prototipado'],
            strengths: ['Compacto', 'Velocidad alta', 'Bajo consumo', 'Fácil instalación'],
            bestFor: 'Dispositivos con recursos limitados y tareas básicas rápidas',
            context: '32K tokens',
            ramRequired: '2-4GB',
            parameters: '1B',
            quantization: 'Q4'
          },
{id: 'gemma3:4b', 
            name: 'Gemma 3 (4B)', 
            size: '3.3GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'The current, most capable model that runs on a single GPU. Versión 4B con soporte para texto e imágenes.',
            useCases: ['Análisis de imágenes', 'Programación con contexto visual', 'Análisis de datos', 'Escritura técnica', 'Tareas multimodales'],
            strengths: ['Multimodal (texto e imagen)', 'Eficiencia', 'Contexto extenso', 'Balance rendimiento/recursos'],
            bestFor: 'Desarrolladores que necesitan análisis multimodal con eficiencia de recursos',
            context: '128K tokens',
            ramRequired: '6-8GB',
            parameters: '4B',
            quantization: 'Q4, Q5'
          },
{id: 'gemma3:12b', 
            name: 'Gemma 3 (12B)', 
            size: '8.1GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 12B de Gemma 3. Mayor capacidad para análisis multimodal y tareas complejas.',
            useCases: ['Análisis avanzado de imágenes', 'Programación compleja con contexto visual', 'Análisis profundo', 'Investigación multimodal', 'Tareas agentic'],
            strengths: ['Multimodal avanzado', 'Mayor capacidad', 'Contexto extenso', 'Análisis profundo'],
            bestFor: 'Desarrolladores y investigadores que necesitan análisis multimodal avanzado',
            context: '128K tokens',
            ramRequired: '16-20GB',
            parameters: '12B',
            quantization: 'Q4, Q5'
          },
{id: 'gemma3:27b', 
            name: 'Gemma 3 (27B)', 
            size: '17GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 27B de Gemma 3. Máxima capacidad para análisis multimodal y tareas complejas de nivel experto.',
            useCases: ['Análisis experto de imágenes', 'Programación avanzada con contexto visual', 'Investigación multimodal', 'Análisis profundo', 'Tareas agentic complejas'],
            strengths: ['Multimodal de nivel experto', 'Máxima capacidad', 'Contexto extenso', 'Análisis profundo', 'Precisión superior'],
            bestFor: 'Investigadores avanzados y desarrolladores expertos que requieren máxima capacidad multimodal',
            context: '128K tokens',
            ramRequired: '32-40GB',
            parameters: '27B',
            quantization: 'Q4, Q5'
          },
{id: 'phi3', 
            name: 'Phi-3 (3.8B)', 
            size: '2.3GB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo compacto de Microsoft. Ideal para dispositivos con recursos limitados y tareas básicas.',
            useCases: ['Asistencia básica', 'Respuestas rápidas', 'Dispositivos móviles', 'Tareas simples', 'Prototipado'],
            strengths: ['Muy compacto', 'Velocidad alta', 'Bajo consumo', 'Fácil instalación'],
            bestFor: 'Dispositivos con recursos limitados, desarrollo móvil y tareas básicas rápidas',
            context: '4K tokens',
            ramRequired: '4-6GB',
            parameters: '3.8B',
            quantization: 'Q4'
          },
{id: 'codellama', 
            name: 'Code Llama (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo de programación de Meta basado en Llama. Excelente para generación y comprensión de código.',
            useCases: ['Programación en múltiples lenguajes', 'Generación de código', 'Explicación de código', 'Refactoring', 'Debugging'],
            strengths: ['Multilenguaje', 'Comprensión de código', 'Generación eficiente', 'Soporte para múltiples paradigmas'],
            bestFor: 'Desarrolladores que trabajan con múltiples lenguajes de programación',
            context: '16K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'wizardcoder', 
            name: 'WizardCoder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo especializado en programación con capacidades avanzadas de generación de código.',
            useCases: ['Generación de código', 'Programación asistida', 'Resolución de problemas', 'Optimización de código'],
            strengths: ['Generación de código limpio', 'Comprensión de requerimientos', 'Optimización', 'Buena documentación'],
            bestFor: 'Desarrolladores que buscan asistencia inteligente en programación',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'starcoder', 
            name: 'StarCoder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo de código de BigCode entrenado en repositorios públicos. Excelente para programación.',
            useCases: ['Programación en múltiples lenguajes', 'Generación de código', 'Completado de código', 'Refactoring'],
            strengths: ['Gran conocimiento de código', 'Soporte multilenguaje', 'Comprensión de patrones', 'Generación eficiente'],
            bestFor: 'Desarrolladores que necesitan un modelo con amplio conocimiento de código',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'mistral-instruct', 
            name: 'Mistral Instruct (7B)', 
            size: '4.1GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión Instruct de Mistral. Modelo europeo eficiente optimizado para seguimiento de instrucciones.',
            useCases: ['Seguimiento de instrucciones', 'Programación', 'Análisis técnico', 'Respuestas precisas'],
            strengths: ['Eficiencia superior', 'Instrucciones precisas', 'Respuestas relevantes', 'Bajo consumo'],
            bestFor: 'Desarrolladores que buscan eficiencia y precisión en seguimiento de instrucciones',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'neural-chat', 
            name: 'Neural Chat (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo especializado en conversaciones naturales y chat. Entrenado con RLHF para diálogos fluidos.',
            useCases: ['Chat natural', 'Conversaciones', 'Asistencia general', 'Diálogos prolongados', 'Entretenimiento'],
            strengths: ['Conversaciones naturales', 'Diálogos fluidos', 'Comprensión contextual', 'Respuestas personalizadas'],
            bestFor: 'Usuarios que buscan conversaciones naturales y asistencia conversacional',
            context: '4K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'vicuna', 
            name: 'Vicuña (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Modelo de conversación basado en LLaMA, fine-tuned con instrucciones de alta calidad de usuarios.',
            useCases: ['Chat general', 'Asistencia', 'Brainstorming', 'Escritura', 'Respuestas creativas'],
            strengths: ['Chat natural', 'Versatilidad', 'Respuestas creativas', 'Buena comprensión'],
            bestFor: 'Usuarios que buscan un asistente de conversación versátil y accesible',
            context: '2K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'orca-mini:3b', 
            name: 'Orca Mini (3B)', 
            size: '2.0GB', 
            downloaded: false, 
            performance: 'low',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión mini ultra-compacta de Orca. Ideal para dispositivos con recursos limitados.',
            useCases: ['Asistencia básica', 'Respuestas rápidas', 'Dispositivos móviles', 'Prototipado', 'Testing'],
            strengths: ['Muy ligero', 'Rápido', 'Bajo consumo', 'Fácil instalación'],
            bestFor: 'Dispositivos con recursos muy limitados y tareas de respuesta rápida',
            context: '32K tokens',
            ramRequired: '3-4GB',
            parameters: '3B',
            quantization: 'Q4'
          },
{id: 'orca-mini:7b', 
            name: 'Orca Mini (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 7B de Orca Mini. Modelo de propósito general adecuado para hardware de nivel de entrada.',
            useCases: ['Asistencia general', 'Programación básica', 'Análisis de texto', 'Resolución de problemas', 'Uso general'],
            strengths: ['Balance rendimiento/recursos', 'Buena comprensión', 'Respuestas coherentes', 'Amplio conocimiento'],
            bestFor: 'Usuarios con hardware básico que buscan buen rendimiento general',
            context: '32K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'orca-mini:13b', 
            name: 'Orca Mini (13B)', 
            size: '7.4GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 13B de Orca Mini. Mayor capacidad para tareas más complejas manteniendo eficiencia.',
            useCases: ['Programación avanzada', 'Análisis profundo', 'Razonamiento', 'Escritura técnica', 'Asistencia avanzada'],
            strengths: ['Mayor capacidad', 'Mejor razonamiento', 'Análisis profundo', 'Versatilidad'],
            bestFor: 'Usuarios que buscan mayor capacidad sin requerir hardware extremo',
            context: '32K tokens',
            ramRequired: '16-20GB',
            parameters: '13B',
            quantization: 'Q4, Q5'
          },
{id: 'orca-mini:70b', 
            name: 'Orca Mini (70B)', 
            size: '39GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Versión 70B de Orca Mini. Máxima capacidad de la serie Orca Mini para tareas complejas.',
            useCases: ['Programación avanzada', 'Análisis profundo', 'Razonamiento complejo', 'Investigación', 'Tareas complejas'],
            strengths: ['Máxima capacidad', 'Razonamiento superior', 'Análisis profundo', 'Precisión superior'],
            bestFor: 'Investigadores y desarrolladores expertos que requieren máxima capacidad',
            context: '32K tokens',
            ramRequired: '80-96GB',
            parameters: '70B',
            quantization: 'Q4, Q5'
          },
{id: 'orca2', 
            name: 'Orca 2 (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Orca 2 con mejoras en razonamiento y seguimiento de instrucciones. Versión mejorada de Orca basada en Llama 2.',
            useCases: ['Razonamiento', 'Instrucciones complejas', 'Análisis', 'Programación', 'Resolución de problemas'],
            strengths: ['Razonamiento mejorado', 'Instrucciones precisas', 'Análisis profundo', 'Versatilidad'],
            bestFor: 'Usuarios que buscan mejor razonamiento y comprensión de instrucciones complejas',
            context: '32K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'neural-chat-instruct', 
            name: 'Neural Chat Instruct (8B)', 
            size: '4.2GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'ollama',
            platformName: 'Ollama',
            platformDescription: 'Requiere Ollama instalado localmente',
            description: 'Intel Neural Chat Instruct - Modelo optimizado para instrucciones. Excelente balance velocidad/calidad.',
            useCases: ['Seguimiento de instrucciones', 'Conversaciones', 'Programación', 'Asistencia general', 'Escritura técnica'],
            strengths: ['Instrucciones precisas', 'Chat natural', 'Optimizado por Intel', 'Balance velocidad/calidad'],
            bestFor: 'Usuarios que buscan seguimiento preciso de instrucciones con buena velocidad',
            context: '4K tokens',
            ramRequired: '10-12GB',
            parameters: '8B',
            quantization: 'Q4, Q5'
          },];

// Configuración de modelos locales independientes
const LOCAL_INDEPENDENT_MODELS = [
  { 
    id: 'deepseek-coder:6.7b', 
            name: 'DeepSeek Coder (6.7B)', 
            size: '4.1GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Modelo especializado en programación y generación de código. Optimizado para tareas de desarrollo.',
            useCases: ['Generación de código', 'Refactoring', 'Debugging', 'Documentación técnica', 'Análisis de código'],
            strengths: ['Excelente en programación', 'Generación de código de calidad', 'Comprensión de sintaxis', 'Refactoring inteligente'],
            bestFor: 'Desarrolladores, programadores y equipos de desarrollo que necesitan asistencia en código',
            context: '4K tokens',
            ramRequired: '8-10GB',
            parameters: '6.7B',
            quantization: 'Q4, Q5'
          },
{id: 'magicoder:7b', 
            name: 'Magicoder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Modelo de programación con capacidades mágicas para generación de código y resolución de problemas.',
            useCases: ['Generación de código', 'Resolución de bugs', 'Optimización', 'Documentación automática'],
            strengths: ['Generación creativa', 'Resolución de problemas', 'Optimización inteligente', 'Código bien documentado'],
            bestFor: 'Desarrolladores que buscan soluciones creativas y optimizadas',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'phind-codellama:7b', 
            name: 'Phind CodeLlama (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Versión optimizada de CodeLlama por Phind. Mejorado para búsqueda y generación de código.',
            useCases: ['Búsqueda de código', 'Generación eficiente', 'Análisis de código', 'Documentación'],
            strengths: ['Búsqueda inteligente', 'Generación rápida', 'Análisis profundo', 'Documentación clara'],
            bestFor: 'Desarrolladores que necesitan búsqueda y análisis eficiente de código',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'octocoder:7b', 
            name: 'OctoCoder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'medium',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Modelo especializado en desarrollo web y aplicaciones. Optimizado para tecnologías modernas.',
            useCases: ['Desarrollo web', 'Aplicaciones modernas', 'Frontend/Backend', 'APIs', 'Frameworks populares'],
            strengths: ['Conocimiento web', 'Frameworks modernos', 'APIs y servicios', 'Desarrollo full-stack'],
            bestFor: 'Desarrolladores web y de aplicaciones modernas',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'sqlcoder:7b', 
            name: 'SQLCoder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Modelo especializado en SQL y bases de datos. Excelente para consultas y optimización de bases de datos.',
            useCases: ['Consultas SQL', 'Optimización de bases de datos', 'Análisis de datos', 'Diseño de esquemas', 'Migración de datos'],
            strengths: ['SQL avanzado', 'Optimización de consultas', 'Análisis de rendimiento', 'Diseño de bases de datos'],
            bestFor: 'Desarrolladores de bases de datos, analistas de datos y administradores de sistemas',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          },
{id: 'python-coder:7b', 
            name: 'Python Coder (7B)', 
            size: '3.8GB', 
            downloaded: false, 
            performance: 'high',
            platform: 'independent',
            platformName: 'Independiente',
            platformDescription: 'No requiere Ollama - Funciona directamente',
            description: 'Modelo especializado en Python. Optimizado para desarrollo, análisis de datos y machine learning.',
            useCases: ['Desarrollo Python', 'Data Science', 'Machine Learning', 'Automatización', 'Scripting'],
            strengths: ['Python puro', 'Librerías populares', 'Data Science', 'ML/AI', 'Automatización'],
            bestFor: 'Desarrolladores Python, científicos de datos y profesionales de ML/AI',
            context: '8K tokens',
            ramRequired: '8-10GB',
            parameters: '7B',
            quantization: 'Q4, Q5'
          }
];

class ModelManager {
  constructor() {
    // Crear copias profundas para evitar mutaciones
    this.models = {
      remote: JSON.parse(JSON.stringify(REMOTE_MODELS)),
      local: {
        ollama: JSON.parse(JSON.stringify(LOCAL_OLLAMA_MODELS)),
        independent: JSON.parse(JSON.stringify(LOCAL_INDEPENDENT_MODELS))
      }
    };
  }

  /**
   * Obtener todos los modelos locales (Ollama + Independientes)
   */
  getAllLocalModels() {
    return [...this.models.local.ollama, ...this.models.local.independent];
  }

  /**
   * Obtener lista de modelos disponibles
   * @param {string|null} type - 'remote', 'local' o null para todos
   * @returns {Array|Object} Lista de modelos o objeto con remote y local
   */
  getAvailableModels(type = null) {
    if (type) {
      if (type === 'local') {
        return this.getAllLocalModels();
      }
      return this.models[type] || [];
    }
    return {
      remote: this.models.remote,
      local: this.getAllLocalModels()
    };
  }

  /**
   * Obtener modelo por ID y tipo
   * @param {string} modelId - ID del modelo
   * @param {string} type - 'remote' o 'local'
   * @returns {Object|null} Modelo encontrado o null
   */
  getModelById(modelId, type) {
    if (!modelId || !type) return null;
    
    if (type === 'remote') {
      return this.models.remote.find(m => m.id === modelId) || null;
    } else if (type === 'local') {
      return this.getAllLocalModels().find(m => m.id === modelId) || null;
    }
    
    return null;
  }

  /**
   * Obtener todos los modelos (para compatibilidad con código existente)
   */
  getAllModels() {
    return this.models;
  }
}

// Exportar instancia singleton
export const modelManager = new ModelManager();
export default modelManager;
