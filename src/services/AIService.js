/**
 * AIService - Servicio principal para manejar las APIs de IA
 * Soporta modelos remotos (GPT, Claude, etc.) y locales (Llama, Qwen, DeepSeek)
 */

import { conversationService } from './ConversationService';
import debugLogger from '../utils/debugLogger';

import fileAnalysisService from './FileAnalysisService';
import mcpClient from './MCPClientService';
import toolOrchestrator from './ToolOrchestrator';
import modelMemoryService from './ModelMemoryService';
import { summarizeToolResult } from '../utils/toolResultSummarizer';
import { rememberToolExecution, getRecentToolExecution } from './ToolExecutionCache';
import { modelManager } from './ai/ModelManager';

const TOOLS_REQUIRE_FULL_CONTEXT = new Set([
  'search_nodeterm'
]);

class AIService {
  constructor() {
    this.currentModel = null;
    this.modelType = 'remote'; // 'remote', 'local' o 'remote-ollama'
    this.apiKey = null;
    this.remoteOllamaUrl = null;
    this.performanceConfig = null; // Configuraci√≥n manual de rendimiento
    // Cach√© simple para los directorios permitidos de MCP (evita pedirlos repetidamente)
    this.allowedDirectoriesCache = { value: null, fetchedAt: 0 };
    this.mcpDefaultDirs = {}; // Map<serverId, { raw, normalized }>
    // Flag para invalidar informaci√≥n del filesystem cuando se modifica
    this._filesystemModified = false;
    // Feature flags y orquestador
    this.featureFlags = { structuredToolMessages: true };
    this.toolOrchestrator = toolOrchestrator;
    // Servicio de gesti√≥n de memoria
    this.memoryService = modelMemoryService;
    // Usar ModelManager para obtener todos los modelos
    this.models = modelManager.getAllModels();
    // Nota: Los modelos ahora est√°n centralizados en ModelManager
    // El c√≥digo anterior que defin√≠a this.models aqu√≠ ha sido movido a ModelManager
    this.conversationHistory = [];
    this.loadConfig();
  }

  _setMcpDefaultDir(serverId, path) {
    if (!serverId || !path || typeof path !== 'string') return;
    const trimmed = path.trim();
    if (!trimmed) return;
    const normalized = trimmed.replace(/\\/g, '/');
    this.mcpDefaultDirs[serverId] = {
      raw: trimmed,
      normalized
    };
  }

  _getMcpDefaultDir(serverId) {
    if (!serverId) return null;
    return this.mcpDefaultDirs[serverId] || null;
  }

  /**
   * Validar si un modelo est√° disponible para uso
   */
  validateModelAvailability(modelId, type) {
    if (!modelId) return false;
    
    if (type === 'local') {
      const model = this.getAllLocalModels().find(m => m.id === modelId);
      if (!model) return false;
      // Solo permitir modelos que est√©n marcados como descargados
      return model.downloaded === true;
    } else if (type === 'remote') {
      const model = this.models.remote.find(m => m.id === modelId);
      if (!model) return false;
      // Solo permitir modelos remotos con API key configurada
      const apiKey = this.getApiKey(model.provider);
      return !!apiKey;
    }
    
    return false;
  }

  /**
   * Cargar configuraci√≥n desde localStorage
   */
  loadConfig() {
    try {
      const config = localStorage.getItem('ai-service-config');
      if (config) {
        const parsed = JSON.parse(config);
        
        // Restaurar API keys primero (necesario para validaci√≥n de modelos remotos)
        this.apiKey = parsed.apiKey || null;
        this.remoteOllamaUrl = parsed.remoteOllamaUrl || null;
        this.performanceConfig = parsed.performanceConfig || null;
        
        // Cargar estado de modelos locales descargados
        if (parsed.localModels) {
          const allLocalModels = this.getAllLocalModels();
          allLocalModels.forEach(model => {
            const saved = parsed.localModels.find(m => m.id === model.id);
            if (saved) {
              model.downloaded = saved.downloaded;
            }
          });
        }
        
        // Validar que el modelo guardado est√© disponible antes de usarlo
        const savedModel = parsed.currentModel;
        const savedType = parsed.modelType || 'remote';
        
        if (savedModel && this.validateModelAvailability(savedModel, savedType)) {
          // Modelo v√°lido, usar configuraci√≥n guardada
          this.currentModel = savedModel;
          this.modelType = savedType;
        } else {
          // Modelo no v√°lido o no disponible, limpiar selecci√≥n
          // El usuario deber√° seleccionar un modelo v√°lido manualmente
          this.currentModel = null;
          this.modelType = 'remote';
          
          if (savedModel) {
            debugLogger.warn('AIService', `Modelo guardado ${savedModel} (${savedType}) no est√° disponible. Limpiando selecci√≥n.`);
          }
        }
      }
    } catch (error) {
      console.error('Error cargando configuraci√≥n de AI:', error);
    }
  }

  /**
   * Guardar configuraci√≥n en localStorage
   */
  saveConfig() {
    try {
      const config = {
        currentModel: this.currentModel,
        modelType: this.modelType,
        apiKey: this.apiKey,
        remoteOllamaUrl: this.remoteOllamaUrl,
        performanceConfig: this.performanceConfig,
        localModels: this.getAllLocalModels().map(m => ({ id: m.id, downloaded: m.downloaded }))
      };
      localStorage.setItem('ai-service-config', JSON.stringify(config));
    } catch (error) {
      console.error('Error guardando configuraci√≥n de AI:', error);
    }
  }

  /**
   * Obtener todos los modelos locales (Ollama + Independientes)
   */
  getAllLocalModels() {
    return [...this.models.local.ollama, ...this.models.local.independent];
  }

  /**
   * Obtener lista de modelos disponibles
   */
  getAvailableModels(type = null) {
    if (type) {
      if (type === 'local') {
        return this.getAllLocalModels();
      }
      return this.models[type] || [];
    }
    return {
      remote: this.models.remote,
      local: this.getAllLocalModels()
    };
  }

  /**
   * Obtener solo los modelos funcionales (con API key o descargados)
   */
  getFunctionalModels() {
    const functional = [];
    
    // Modelos remotos con API key configurada
    this.models.remote.forEach(model => {
      const apiKey = this.getApiKey(model.provider);
      if (apiKey) {
        functional.push({
          ...model,
          type: 'remote',
          displayName: `${model.name} (${model.provider})`
        });
      }
    });
    
    // Modelos locales descargados
    this.getAllLocalModels().forEach(model => {
      if (model.downloaded) {
        functional.push({
          ...model,
          type: 'local',
          displayName: `${model.name} (Local)`
        });
      }
    });
    
    return functional;
  }

  /**
   * Obtener configuraci√≥n de rendimiento para un modelo
   */
  getModelPerformanceConfig(modelId, modelType) {
    debugLogger.debug('AIService', `getModelPerformanceConfig - Modelo: ${modelId}, Tipo: ${modelType}`);
    
    // Configuraciones espec√≠ficas por modelo cloud
    const cloudModelConfigs = {
      // OpenAI Models
      'gpt-4': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 128000 // 128K contexto
      },
      'gpt-4-turbo': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 128000 // 128K contexto
      },
      'gpt-3.5-turbo': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 16000 // 16K contexto
      },
      'gpt-3.5-turbo-16k': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 16000 // 16K contexto
      },
      
      // Anthropic Models
      'claude-3-opus': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 200000 // 200K contexto
      },
      'claude-3-sonnet': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 200000 // 200K contexto
      },
      'claude-3-haiku': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 200000 // 200K contexto
      },
      'claude-2': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 100000 // 100K contexto
      },
      
      // Google Models
      'gemini-2.5-flash': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 1000000 // 1M contexto (Flash tiene contexto muy alto)
      },
      'gemini-2.5-pro': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 2000000 // 2M contexto (Pro tiene el contexto m√°s alto)
      },
      'gemini-2.0-flash-exp': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 1000000 // 1M contexto (experimental)
      },
      // Modelos legacy (por compatibilidad)
      'gemini-pro': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 32000 // 32K contexto
      },
      'gemini-pro-vision': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 32000 // 32K contexto
      }
    };

    // Si es un modelo cloud, usar configuraci√≥n espec√≠fica (prioridad m√°xima)
    if (modelType === 'remote' && cloudModelConfigs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para ${modelId}:`, cloudModelConfigs[modelId]);
      return cloudModelConfigs[modelId];
    }

    // Para modelos locales, verificar configuraci√≥n guardada manualmente (prioridad alta)
    if (modelType === 'local') {
      try {
        const localConfigs = JSON.parse(localStorage.getItem('local-model-performance-configs') || '{}');
        
        // Buscar configuraci√≥n exacta primero
        if (localConfigs[modelId]) {
          debugLogger.debug('AIService', `Usando configuraci√≥n guardada para ${modelId}`);
          return localConfigs[modelId];
        }
        
        // Si no se encuentra, intentar con el nombre base (sin tags como :latest, :8b, etc.)
        // Esto permite que modelos como "llama3.2:latest" usen la configuraci√≥n de "llama3.2"
        const baseModelId = modelId.split(':')[0];
        if (baseModelId !== modelId && localConfigs[baseModelId]) {
          debugLogger.debug('AIService', `Usando configuraci√≥n guardada para ${baseModelId} (base de ${modelId})`);
          return localConfigs[baseModelId];
        }
        
        // Buscar tambi√©n por coincidencias parciales (para modelos personalizados)
        // Por ejemplo, si tenemos "mistral:7b" instalado y configuramos "mistral"
        const matchingKey = Object.keys(localConfigs).find(key => {
          return modelId.includes(key) || key.includes(baseModelId);
        });
        if (matchingKey) {
          debugLogger.debug('AIService', `Usando configuraci√≥n guardada (coincidencia parcial) ${matchingKey} para ${modelId}`);
          return localConfigs[matchingKey];
        }
      } catch (e) {
        debugLogger.warn('AIService', `Error al cargar configuraci√≥n individual de ${modelId}:`, e.message);
      }
    }

    // Si no, usar configuraci√≥n autom√°tica basada en performance
    let model;
    if (modelType === 'local') {
      model = this.getAllLocalModels().find(m => m.id === modelId);
    } else {
      model = this.models[modelType].find(m => m.id === modelId);
    }
    
    if (!model) {
      debugLogger.warn('AIService', `Modelo no encontrado, usando configuraci√≥n por defecto`);
      return this.getDefaultPerformanceConfig();
    }

    // Configuraciones espec√≠ficas para modelos Qwen con contextos largos
    const qwen3Configs = {
      'qwen2.5': {
        maxTokens: 12000,
        temperature: 0.7,
        maxHistory: 16,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Qwen 2.5
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'qwen3:8b': {
        maxTokens: 15000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Qwen3
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'qwen3:30b': {
        maxTokens: 20000,
        temperature: 0.7,
        maxHistory: 25,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Qwen3
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    // Configuraciones espec√≠ficas para modelos GPT-OSS con contextos largos
    const gptOssConfigs = {
      'gpt-oss:20b': {
        maxTokens: 12000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de GPT-OSS 20B
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gpt-oss:120b': {
        maxTokens: 16000,
        temperature: 0.7,
        maxHistory: 24,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de GPT-OSS 120B
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    // Configuraciones espec√≠ficas para Llama 3.2
    const llama32Configs = {
      'llama3.2:latest': {
        maxTokens: 6000,
        temperature: 0.7,
        maxHistory: 12,
        useStreaming: true,
        contextLimit: 8000,    // 8K contexto nativo de Llama 3.2 3B
        num_ctx: 8000          // Para Ollama
      },
      'llama3.2:1b': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 4000,    // 4K contexto nativo de Llama 3.2 1B
        num_ctx: 4000          // Para Ollama
      }
    };

    // Configuraciones espec√≠ficas para Llama 3.1 con contexto extendido
    const llama31Configs = {
      'llama3.1:8b': {
        maxTokens: 8000,
        temperature: 0.7,
        maxHistory: 16,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Llama 3.1 8B
        num_ctx: 128000        // Para Ollama
      },
      'llama3.1:latest': {
        maxTokens: 8000,
        temperature: 0.7,
        maxHistory: 16,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Llama 3.1
        num_ctx: 128000        // Para Ollama
      },
      'llama3.1:70b': {
        maxTokens: 12000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo de Llama 3.1 70B
        num_ctx: 128000        // Para Ollama
      }
    };

    // Configuraciones espec√≠ficas para Llama 3 (versi√≥n anterior)
    const llama3Configs = {
      'llama3': {
        maxTokens: 6000,
        temperature: 0.7,
        maxHistory: 12,
        useStreaming: true,
        contextLimit: 8000,    // 8K contexto nativo de Llama 3 8B
        num_ctx: 8000          // Para Ollama
      },
      'llama3:70b': {
        maxTokens: 10000,
        temperature: 0.7,
        maxHistory: 16,
        useStreaming: true,
        contextLimit: 8000,    // 8K contexto nativo de Llama 3 70B
        num_ctx: 8000          // Para Ollama
      }
    };

    // Configuraciones espec√≠ficas para DeepSeek R1 (todos tienen 128K contexto)
    const deepseekR1Configs = {
      'deepseek-r1:latest': {
        maxTokens: 2000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 8192,  // Reducido para velocidad
        num_ctx: 8192,      // Para Ollama - m√°s r√°pido
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:1.5b': {
        maxTokens: 2000,
        temperature: 0.7,
        maxHistory: 16,
        useStreaming: true,
        contextLimit: 8192,  // Reducido para velocidad
        num_ctx: 8192,       // Para Ollama - m√°s r√°pido
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:7b': {
        maxTokens: 2000,
        temperature: 0.7,
        maxHistory: 18,
        useStreaming: true,
        contextLimit: 8192,  // Reducido para velocidad
        num_ctx: 8192,       // Para Ollama - m√°s r√°pido
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:8b': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 30,
        useStreaming: true,
        contextLimit: 32768,  // 32K contexto - DeepSeek-R1 8B soporta hasta 128K nativo
        num_ctx: 32768,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:14b': {
        maxTokens: 14000,
        temperature: 0.7,
        maxHistory: 24,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:32b': {
        maxTokens: 16000,
        temperature: 0.7,
        maxHistory: 28,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:70b': {
        maxTokens: 20000,
        temperature: 0.7,
        maxHistory: 32,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'deepseek-r1:671b': {
        maxTokens: 24000,
        temperature: 0.7,
        maxHistory: 36,
        useStreaming: true,
        contextLimit: 160000,  // 160K contexto nativo
        num_ctx: 160000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    // Configuraciones espec√≠ficas para Gemma 3 (multimodal)
    const gemma3Configs = {
      'gemma3:latest': {
        maxTokens: 12000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gemma3:270m': {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 12,
        useStreaming: true,
        contextLimit: 32000,   // 32K contexto nativo
        num_ctx: 32000,        // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gemma3:1b': {
        maxTokens: 6000,
        temperature: 0.7,
        maxHistory: 14,
        useStreaming: true,
        contextLimit: 32000,   // 32K contexto nativo
        num_ctx: 32000,        // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gemma3:4b': {
        maxTokens: 12000,
        temperature: 0.7,
        maxHistory: 20,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo (multimodal)
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gemma3:12b': {
        maxTokens: 16000,
        temperature: 0.7,
        maxHistory: 24,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo (multimodal)
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      },
      'gemma3:27b': {
        maxTokens: 20000,
        temperature: 0.7,
        maxHistory: 28,
        useStreaming: true,
        contextLimit: 128000,  // 128K contexto nativo (multimodal)
        num_ctx: 128000,       // Para Ollama
        top_k: 40,
        top_p: 0.9,
        repeat_penalty: 1.1
      }
    };

    // Si es un modelo Gemma 3, usar configuraci√≥n espec√≠fica
    if (gemma3Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para Gemma 3 ${modelId}:`, gemma3Configs[modelId]);
      return gemma3Configs[modelId];
    }

    // Si es un modelo DeepSeek R1, usar configuraci√≥n espec√≠fica
    if (deepseekR1Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para DeepSeek R1 ${modelId}:`, deepseekR1Configs[modelId]);
      return deepseekR1Configs[modelId];
    }

    // Si es un modelo Qwen, usar configuraci√≥n espec√≠fica
    if (qwen3Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para Qwen ${modelId}:`, qwen3Configs[modelId]);
      return qwen3Configs[modelId];
    }

    // Si es un modelo GPT-OSS, usar configuraci√≥n espec√≠fica
    if (gptOssConfigs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para GPT-OSS ${modelId}:`, gptOssConfigs[modelId]);
      return gptOssConfigs[modelId];
    }

    // Si es un modelo Llama 3.2, usar configuraci√≥n espec√≠fica
    if (llama32Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para Llama 3.2 ${modelId}:`, llama32Configs[modelId]);
      return llama32Configs[modelId];
    }

    // Si es un modelo Llama 3.1, usar configuraci√≥n espec√≠fica
    if (llama31Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para Llama 3.1 ${modelId}:`, llama31Configs[modelId]);
      return llama31Configs[modelId];
    }

    // Si es un modelo Llama 3, usar configuraci√≥n espec√≠fica
    if (llama3Configs[modelId]) {
      debugLogger.trace('AIService', `Usando configuraci√≥n espec√≠fica para Llama 3 ${modelId}:`, llama3Configs[modelId]);
      return llama3Configs[modelId];
    }

    const performanceLevel = model.performance || 'medium';
    debugLogger.debug('AIService', `Usando configuraci√≥n por performance (${performanceLevel}) para ${modelId}`);
    
    const configs = {
      low: {
        maxTokens: 4000,
        temperature: 0.7,
        maxHistory: 5,
        useStreaming: false,
        contextLimit: 2000
      },
      medium: {
        maxTokens: 6000,  // Reducido de 7000 para mejor coherencia con modelos cloud
        temperature: 0.7,
        maxHistory: 8,
        useStreaming: true,
        contextLimit: 8000  // Aumentado de 4000 para mejor contexto
      },
      high: {
        maxTokens: 8000,  // Reducido de 12000 para mantener consistencia
        temperature: 0.7,
        maxHistory: 10,
        useStreaming: true,
        contextLimit: 16000  // Aumentado de 8000 para aprovechar mejor los modelos grandes
      }
    };

    const finalConfig = configs[performanceLevel] || configs.medium;
    debugLogger.trace('AIService', `Configuraci√≥n final para ${modelId}:`, finalConfig);
    return finalConfig;
  }

  /**
   * Configuraci√≥n por defecto
   */
  getDefaultPerformanceConfig() {
    return {
      maxTokens: 7000,
      temperature: 0.7,
      maxHistory: 8,
      useStreaming: true,
      contextLimit: 4000
    };
  }

  /**
   * Establecer configuraci√≥n manual de rendimiento
   */
  setPerformanceConfig(config) {
    this.performanceConfig = config;
    this.saveConfig();
  }

  /**
   * Obtener configuraci√≥n manual de rendimiento
   */
  getPerformanceConfig() {
    return this.performanceConfig;
  }

  /**
   * Limpiar configuraci√≥n manual (volver a autom√°tica)
   */
  clearPerformanceConfig() {
    this.performanceConfig = null;
    this.saveConfig();
  }

  /**
   * Activar/desactivar logging de debug detallado para AIService
   */
  setDebugLogging(enabled) {
    if (enabled) {
      debugLogger.enableModule('AIService');
    } else {
      debugLogger.disableModule('AIService');
    }
  }

  /**
   * Obtener estado del debug logging
   */
  isDebugLoggingEnabled() {
    return debugLogger.isModuleEnabled('AIService');
  }

  /**
   * Detectar modelos instalados en Ollama
   */
  async detectOllamaModels() {
    try {
      const ollamaUrl = this.getOllamaUrl();
      const response = await fetch(`${ollamaUrl}/api/tags`);
      if (!response.ok) {
        throw new Error(`No se pudo conectar con Ollama en ${ollamaUrl}`);
      }
      
      const data = await response.json();
      
      // Actualizar lista de modelos locales con los detectados
      if (data.models && Array.isArray(data.models)) {
        // Primero marcar todos los modelos como no instalados
        this.models.local.ollama.forEach(model => {
          model.downloaded = false;
        });
        this.models.local.independent.forEach(model => {
          model.downloaded = false;
        });
        
        // Luego marcar como instalados solo los que est√°n en Ollama
        const installedModelNames = data.models.map(model => model.name);
        
        installedModelNames.forEach(modelName => {
          // Extraer el nombre base del modelo (sin tags como :latest, :8b, etc.)
          const baseModelName = modelName.split(':')[0];
          
          // Buscar en modelos de Ollama prefiriendo coincidencia exacta, luego coincidencia de nombre base
          let ollamaIndex = this.models.local.ollama.findIndex(m => m.id === modelName);
          if (ollamaIndex === -1) {
            ollamaIndex = this.models.local.ollama.findIndex(m => m.id === baseModelName);
          }
          
          if (ollamaIndex >= 0) {
            this.models.local.ollama[ollamaIndex].downloaded = true;
            // Actualizar el ID para que coincida con el nombre exacto instalado
            this.models.local.ollama[ollamaIndex].id = modelName;
            // NO actualizar el nombre para mantener el nombre configurado con el n√∫mero de B
          } else {
            // Buscar en modelos independientes
            let independentIndex = this.models.local.independent.findIndex(m => m.id === modelName);
            if (independentIndex === -1) {
              independentIndex = this.models.local.independent.findIndex(m => m.id === baseModelName);
            }
            
            if (independentIndex >= 0) {
              this.models.local.independent[independentIndex].downloaded = true;
              // Actualizar el ID para que coincida con el nombre exacto instalado
              this.models.local.independent[independentIndex].id = modelName;
              // NO actualizar el nombre para mantener el nombre configurado
            } else {
              // Modelo no conocido, no agregarlo a la lista predefinida
              // Solo marcar como no disponible en la configuraci√≥n
              debugLogger.debug('AIService.Models', 'Modelo detectado fuera de la configuraci√≥n predefinida', {
                modelName
              });
            }
          }
        });
        
        // Limpiar duplicados - remover modelos predefinidos que ya est√°n instalados con tags
        this.cleanDuplicateModels();
        
        // Validar que el modelo actual siga siendo v√°lido despu√©s de la detecci√≥n
        if (this.currentModel && this.modelType === 'local') {
          if (!this.validateModelAvailability(this.currentModel, this.modelType)) {
            debugLogger.warn('AIService', `El modelo actual ${this.currentModel} ya no est√° instalado. Limpiando selecci√≥n.`);
            this.currentModel = null;
            this.modelType = 'remote';
          }
        }
        
        this.saveConfig();
        
        return installedModelNames;
      }
      
      return [];
    } catch (error) {
      console.error('Error detectando modelos de Ollama:', error);
      return [];
    }
  }

  /**
   * Limpiar modelos duplicados - remover modelos predefinidos que ya est√°n instalados con tags
   */
  cleanDuplicateModels() {
    // Crear un mapa de modelos instalados por nombre base
    const installedBaseNames = new Set();
    this.models.local.ollama.forEach(model => {
      if (model.downloaded && model.id.includes(':')) {
        const baseName = model.id.split(':')[0];
        installedBaseNames.add(baseName);
      }
    });

    // Remover modelos predefinidos que ya est√°n instalados con tags
    this.models.local.ollama = this.models.local.ollama.filter(model => {
      if (!model.downloaded && installedBaseNames.has(model.id)) {
        return false; // Remover modelo predefinido que ya est√° instalado con tag
      }
      return true;
    });
  }

  /**
   * Agregar modelo personalizado
   */
  addCustomModel(modelId, modelName = null) {
    const existingModel = this.getAllLocalModels().find(m => m.id === modelId);
    if (!existingModel) {
      this.models.local.ollama.push({
        id: modelId,
        name: modelName || modelId,
        size: 'Personalizado',
        downloaded: true,
        custom: true
      });
      this.saveConfig();
    }
  }

  /**
   * Seleccionar modelo actual
   */
  setCurrentModel(modelId, type) {
    // Validar que el modelo est√© disponible antes de seleccionarlo
    if (!this.validateModelAvailability(modelId, type)) {
      const modelName = modelId || 'desconocido';
      const errorMsg = type === 'local' 
        ? `El modelo ${modelName} no est√° instalado. Inst√°lalo primero en Ollama.`
        : `El modelo ${modelName} no est√° disponible. Configura la API Key correspondiente.`;
      debugLogger.error('AIService', 'Modelo no disponible al intentar seleccionarlo', {
        modelId,
        modelType: type,
        message: errorMsg
      });
      throw new Error(errorMsg);
    }
    
    this.currentModel = modelId;
    this.modelType = type;
    this.saveConfig();
    
    debugLogger.info('AIService', `Modelo seleccionado: ${modelId} (${type})`);
  }

  /**
   * Configurar API Key para modelos remotos
   */
  setApiKey(provider, key) {
    if (!this.apiKey) {
      this.apiKey = {};
    }
    this.apiKey[provider] = key;
    this.saveConfig();
  }

  /**
   * Obtener API Key para un provider
   */
  getApiKey(provider) {
    return this.apiKey?.[provider] || null;
  }

  /**
   * Configurar URL de Ollama remoto
   */
  setRemoteOllamaUrl(url) {
    this.remoteOllamaUrl = url;
    this.saveConfig();
  }

  /**
   * Obtener URL de Ollama (local o remoto)
   */
  getOllamaUrl() {
    return this.remoteOllamaUrl || 'http://localhost:11434';
  }

  /**
   * Enviar mensaje al modelo de IA
   */
  async sendMessage(message, options = {}) {
    if (!this.currentModel) {
      throw new Error('No se ha seleccionado ning√∫n modelo');
    }

    // Obtener configuraci√≥n de rendimiento autom√°tica
    const perfConfig = this.getModelPerformanceConfig(this.currentModel, this.modelType);
    
    // Combinar opciones con configuraci√≥n autom√°tica
    const finalOptions = {
      ...perfConfig,
      ...options
    };

    // Limitar historial si es necesario
    if (this.conversationHistory.length > finalOptions.maxHistory) {
      this.conversationHistory = this.conversationHistory.slice(-finalOptions.maxHistory);
    }

    // Agregar mensaje al historial
    this.conversationHistory.push({
      role: 'user',
      content: message,
      timestamp: Date.now()
    });

    try {
      let response;
      
      if (this.modelType === 'remote') {
        response = await this.sendToRemoteModel(message, finalOptions);
      } else {
        response = await this.sendToLocalModel(message, finalOptions);
      }

      // Agregar respuesta al historial
      this.conversationHistory.push({
        role: 'assistant',
        content: response,
        timestamp: Date.now()
      });

      return response;
    } catch (error) {
      console.error('Error enviando mensaje a IA:', error);
      throw error;
    }
  }

  /**
   * FILTRAR tools por contexto/relevancia
   * ‚ú® MEJORADO: Filtrado m√°s agresivo con scoring y l√≠mite m√°ximo
   */
  filterToolsByContext(tools, message = '') {
    const MAX_TOOLS = 6; // L√≠mite m√°ximo de tools para no abrumar al modelo
    const lowerMsg = message.toLowerCase();
    
    // Calcular score de relevancia para cada tool
    const toolsWithScore = tools.map(tool => {
      let score = 0;
      const toolName = tool.name.toLowerCase();
      const toolDesc = (tool.description || '').toLowerCase();
      
      // B√∫squedas en web/internet
      if (lowerMsg.match(/busca|search|google|web|internet|sitio|p√°gina|url|http|\.com/)) {
        if (toolName.includes('search') || toolName.includes('web') || toolName.includes('fetch')) score += 10;
        if (toolName.includes('goto') || toolName.includes('screenshot')) score += 5;
      }
      
      // Archivos: LEER
      if (lowerMsg.match(/lee|leer|muestra|abre|ver|contenido|archivo|file/)) {
        if (toolName.includes('read_file') || toolName.includes('read_text')) score += 10;
        if (toolName.includes('get_file_info')) score += 5;
      }
      
      // Archivos: CREAR/ESCRIBIR
      if (lowerMsg.match(/crea|crear|escribe|guarda|guardar|genera|nuevo archivo|write/)) {
        if (toolName.includes('write_file') || toolName.includes('create')) score += 10;
        if (toolName.includes('append')) score += 5;
      }
      
      // Archivos: EDITAR/MODIFICAR
      if (lowerMsg.match(/edita|editar|modifica|cambia|actualiza|reemplaza|edit/)) {
        if (toolName.includes('edit_file') || toolName.includes('update')) score += 10;
        if (toolName.includes('write_file')) score += 3; // Fallback
      }
      
      // SSH/Terminal: HOSTS y CONEXIONES (ALTA PRIORIDAD)
      if (lowerMsg.match(/ssh|host|conexi√≥n|remota|servidor|terminal remota|red|conecta|conectar|ejecuta en|comando remoto|archivo en el servidor/i)) {
        if (toolName.includes('execute_ssh')) score += 20; // M√ÅS ALTO que read_text_file
        if (toolName.includes('search_nodeterm')) score += 18; // B√∫squeda inteligente de SSH hosts y credenciales
        if (toolName.includes('test_ssh')) score += 12;
        // PENALIZAR herramientas de filesystem cuando estamos en contexto SSH
        if (toolName.includes('read_text_file')) score -= 10;
        if (toolName.includes('list_directory')) score -= 10;
      }
      
      // Terminal/Comandos LOCALES
      if (lowerMsg.match(/local|m√°quina local|powershell|wsl|terminal local/i)) {
        if (toolName.includes('execute_local')) score += 12;
      }
      
      // Directorios: LISTAR (pero SIN interferir con SSH)
      if (lowerMsg.match(/lista|listar/) && !lowerMsg.match(/ssh|host|conexi√≥n|remota|servidor|conecta|ejecuta/i)) {
        if (lowerMsg.match(/directorio|carpeta|folder|contenido|archivos en/)) {
          if (toolName.includes('list_directory')) score += 10;
          if (toolName.includes('directory_tree')) score += 7;
          if (toolName.includes('list_directory_with_sizes')) score += 8;
        }
      }
      
      // PENALIZAR read_text_file si hay contexto de SSH
      if (lowerMsg.match(/ssh|remoto|servidor|conecta/i) && toolName.includes('read_text_file')) {
        score -= 15; // Castigar fuertemente
      }
      
      // Archivos: BUSCAR
      if (lowerMsg.match(/busca archivo|encuentra archivo|search.*file|find.*file|patr√≥n/)) {
        if (toolName.includes('search_files') || toolName.includes('find')) score += 10;
      }
      
      // Archivos: MOVER/RENOMBRAR
      if (lowerMsg.match(/mueve|mover|renombra|rename|move/)) {
        if (toolName.includes('move_file') || toolName.includes('rename')) score += 10;
      }
      
      // Archivos: ELIMINAR
      if (lowerMsg.match(/elimina|borrar|delete|remove/)) {
        if (toolName.includes('delete') || toolName.includes('remove')) score += 10;
      }
      
      // Comandos/CLI - SOLO si hay palabras expl√≠citas de ejecuci√≥n
      // "crea un script" NO debe activar execute_local, solo "ejecuta el script" o "run script"
      if (lowerMsg.match(/ejecuta|comando|command|run|terminal|shell/) && 
          !lowerMsg.match(/crea|crear|create|genera|generar|generate|escribe|escribir|write|guarda|guardar|save/)) {
        if (toolName.includes('run_command') || toolName.includes('execute')) score += 10;
      }
      // Si dice "script" PERO tambi√©n dice "ejecuta/run", entonces s√≠ activar execute
      if (lowerMsg.match(/script/) && lowerMsg.match(/ejecuta|ejecutar|run|corre|correr|lanza|lanzar/)) {
        if (toolName.includes('run_command') || toolName.includes('execute')) score += 10;
      }
      
      // Base de datos
      if (lowerMsg.match(/query|sql|database|tabla|consulta/)) {
        if (toolName.includes('query') || toolName.includes('sql')) score += 10;
      }
      
      // üîí TENABLE.IO - Detecci√≥n de intenciones relacionadas con seguridad/vulnerabilidades/activos
      const tenableKeywords = /tenable|vulnerabilidad|vulnerabilidades|activo|activos|asset|assets|seguridad|security|scanner|scan|escaneo|escaneado|cve|exploit|parche|patch|riesgo|risk|severidad|severity|cr√≠tico|critical|alto|high|medio|medium|bajo|low|hostname|ip address|direcci√≥n ip/i;
      const isTenableTool = tool.serverId === 'tenable' || toolName.includes('asset') || toolName.includes('vulnerability');
      
      if (lowerMsg.match(tenableKeywords) || isTenableTool) {
        // Alta prioridad para herramientas de Tenable cuando se menciona
        if (isTenableTool) {
          if (lowerMsg.match(/lista|listar|muestra|mostrar|obt√©n|obtener|get|busca|buscar|search|find/i)) {
            if (toolName.includes('get_assets') || toolName.includes('list')) score += 25;
            if (toolName.includes('search_assets')) score += 23;
            if (toolName.includes('get_asset_details')) score += 22;
          }
          if (lowerMsg.match(/vulnerabilidad|vulnerabilidades|vulnerability|cve|exploit|riesgo|risk|severidad|severity/i)) {
            if (toolName.includes('vulnerability') || toolName.includes('vulnerabilities')) score += 25;
            if (toolName.includes('get_asset_details')) score += 20; // Los detalles pueden incluir vulnerabilidades
          }
          if (lowerMsg.match(/detalle|detalles|detail|informaci√≥n|info|espec√≠fico|specific/i)) {
            if (toolName.includes('get_asset_details')) score += 24;
          }
          // Si es herramienta de Tenable pero no hay keywords espec√≠ficos, dar score base
          if (isTenableTool && score === 0) {
            score += 15; // Score base para herramientas de Tenable
          }
        }
      }
      
      // Penalizar tools gen√©ricas si hay espec√≠ficas
      if (toolName === 'write_file' && lowerMsg.includes('edit')) score -= 3;
      if (toolName === 'read_file' && lowerMsg.includes('list')) score -= 3;
      
      return { tool, score };
    });
    
    // Ordenar por score descendente
    toolsWithScore.sort((a, b) => b.score - a.score);
    
    // Tomar las top N m√°s relevantes
    const topTools = toolsWithScore
      .filter(t => t.score > 0) // Solo las que tienen score positivo
      .slice(0, MAX_TOOLS)
      .map(t => t.tool);
    
    // Si no hay tools con score, usar un conjunto m√≠nimo por defecto
    if (topTools.length === 0) {
      const defaultNames = ['read_file', 'list_directory', 'write_file'];
      topTools.push(...tools.filter(t => defaultNames.some(dn => t.name.includes(dn))).slice(0, 3));
    }
    
    // üîí CR√çTICO: SIEMPRE incluir herramientas de Tenable si est√°n disponibles
    // Esto asegura que el modelo las conozca y pueda usarlas cuando sea necesario
    const tenableTools = tools.filter(t => t.serverId === 'tenable');
    const hasTenableInTop = topTools.some(t => t.serverId === 'tenable');
    
    if (tenableTools.length > 0 && !hasTenableInTop) {
      // Agregar TODAS las herramientas de Tenable (son solo 4, no afecta mucho el l√≠mite)
      // Priorizar get_assets primero, luego las dem√°s
      const sortedTenable = [
        tenableTools.find(t => t.name === 'get_assets'),
        tenableTools.find(t => t.name === 'search_assets'),
        tenableTools.find(t => t.name === 'get_asset_details'),
        tenableTools.find(t => t.name === 'get_asset_vulnerabilities')
      ].filter(Boolean);
      
      // Agregar las que no est√°n ya en topTools
      sortedTenable.forEach(tool => {
        if (!topTools.some(t => t.serverId === tool.serverId && t.name === tool.name)) {
          topTools.unshift(tool); // Agregar al inicio para m√°xima prioridad
        }
      });
      
      debugLogger.debug('AIService.ToolsFilter', 'Herramientas de Tenable agregadas forzosamente', {
        cantidad: sortedTenable.length,
        herramientas: sortedTenable.map(t => t.name)
      });
    }

    debugLogger.debug('AIService.ToolsFilter', 'Tools filtrados con scoring', {
      disponibles: tools.length,
      relevantes: topTools.length,
      topScores: toolsWithScore.slice(0, 3).map(t => ({ name: t.tool.name, score: t.score }))
    });
    
    return topTools;
  }

  /**
   * Inyectar contexto MCP (tools, resources, prompts) en los mensajes
   */
  async injectMCPContext(message = '') {
    try {
      // Verificar si hay MCPs activos
      if (!mcpClient.hasActiveServers()) {
        return { tools: [], resources: [], prompts: [], hasTools: false };
      }

      // Obtener tools disponibles
      let tools = mcpClient.getAvailableTools();
      const resources = mcpClient.getAvailableResources();
      const prompts = mcpClient.getAvailablePrompts();

      // üîí DEBUG: Log de herramientas disponibles antes del filtro
      const tenableToolsBefore = tools.filter(t => t.serverId === 'tenable');
      if (tenableToolsBefore.length > 0) {
        debugLogger.debug('AIService.MCP', 'Herramientas de Tenable disponibles antes del filtro', {
          cantidad: tenableToolsBefore.length,
          herramientas: tenableToolsBefore.map(t => t.name)
        });
      }

      // üîç FILTRAR TOOLS (contextual)
      tools = this.filterToolsByContext(tools, message);
      
      // üîí DEBUG: Log de herramientas despu√©s del filtro
      const tenableToolsAfter = tools.filter(t => t.serverId === 'tenable');
      if (tenableToolsBefore.length > 0 && tenableToolsAfter.length === 0) {
        debugLogger.warn('AIService.MCP', '‚ö†Ô∏è Herramientas de Tenable fueron filtradas completamente', {
          antes: tenableToolsBefore.length,
          despues: 0,
          mensaje: message.substring(0, 100)
        });
      }

      debugLogger.debug('AIService.MCP', 'Contexto MCP generado', {
        tools: tools.length,
        resources: resources.length,
        prompts: prompts.length
      });

      return {
        tools,
        resources,
        prompts,
        hasTools: tools.length > 0
      };
    } catch (error) {
      console.error('[MCP] Error obteniendo contexto MCP:', error);
      return { tools: [], resources: [], prompts: [], hasTools: false };
    }
  }

  /**
   * Obtener lista de directorios permitidos (con cach√© de 5 minutos)
   */
  async getAllowedDirectoriesCached() {
    try {
      const now = Date.now();
      const TTL_MS = 5 * 60 * 1000; // 5 minutos
      if (this.allowedDirectoriesCache.value && (now - this.allowedDirectoriesCache.fetchedAt) < TTL_MS) {
        return this.allowedDirectoriesCache.value;
      }

      // Llamar a la tool solo si existe en el servidor filesystem
      const tools = mcpClient.getAvailableTools() || [];
      const hasFilesystem = tools.some(t => t.name === 'list_allowed_directories');
      if (!hasFilesystem) return null;

      const result = await mcpClient.callTool('list_allowed_directories', {});
      let dirsText = null;
      if (result && result.content && Array.isArray(result.content) && result.content.length > 0) {
        const text = result.content[0].text || '';
        // Formato esperado: "Allowed directories:\nC:\\path1"
        const match = text.match(/Allowed directories:\s*([\s\S]+)/i);
        if (match) {
          dirsText = match[1].trim();
        }
      }

      this.allowedDirectoriesCache = {
        value: dirsText,
        fetchedAt: now
      };

      if (dirsText) {
        const firstLine = dirsText.split('\n').map(l => l.trim()).find(Boolean);
        if (firstLine) {
          this._setMcpDefaultDir('filesystem', firstLine);
        }
      }
      return dirsText;
    } catch (err) {
      debugLogger.warn('AIService.MCP', 'No se pudieron cachear los directorios permitidos', {
        error: err.message
      });
      return null;
    }
  }

  /**
   * Generar few-shot examples para mejorar comprensi√≥n de tools
   * üìö Los ejemplos ayudan al modelo a entender el uso correcto
   */
  generateToolExamples(tools, provider) {
    // Seleccionar hasta 3 ejemplos representativos
    const exampleTools = tools.slice(0, 3);
    
    if (exampleTools.length === 0) return '';
    
    let examples = '\nüéØ EJEMPLOS DE USO:\n\n';
    
    exampleTools.forEach((tool, idx) => {
      const params = tool.inputSchema?.properties || {};
      const required = tool.inputSchema?.required || [];
      
      // Generar ejemplo de par√°metros
      const exampleArgs = {};
      Object.keys(params).forEach(key => {
        if (required.includes(key)) {
          const param = params[key];
          // Generar valor de ejemplo seg√∫n el tipo
          if (param.type === 'string') {
            if (key === 'path' || key === 'file' || key === 'source' || key === 'destination') {
              exampleArgs[key] = '/ruta/archivo.txt';
            } else if (key === 'content' || key === 'text') {
              exampleArgs[key] = 'contenido del texto';
            } else if (key === 'pattern') {
              exampleArgs[key] = '*.txt';
            } else {
              exampleArgs[key] = 'valor';
            }
          } else if (param.type === 'number' || param.type === 'integer') {
            exampleArgs[key] = 100;
          } else if (param.type === 'boolean') {
            exampleArgs[key] = true;
          } else if (param.type === 'array') {
            exampleArgs[key] = [];
          } else {
            exampleArgs[key] = param.example || 'valor';
          }
        }
      });
      
      const toolName = tool.serverId ? `${tool.serverId}__${tool.name}` : tool.name;
      
      examples += `Ejemplo ${idx + 1}:\n`;
      examples += `Usuario: "${this._generateUserExampleForTool(tool)}"\n`;
      examples += `Tool llamada: ${toolName}\n`;
      examples += `Argumentos: ${JSON.stringify(exampleArgs, null, 2)}\n\n`;
    });
    
    return examples;
  }
  
  /**
   * Generar ejemplo de petici√≥n del usuario para una tool
   */
  _generateUserExampleForTool(tool) {
    const name = tool.name;
    
    if (name.includes('list') && name.includes('directory')) {
      return 'Lista los archivos del directorio actual';
    } else if (name.includes('read') && name.includes('file')) {
      return 'Lee el contenido del archivo config.json';
    } else if (name.includes('write') && name.includes('file')) {
      return 'Crea un archivo llamado notas.txt con el texto "Hola mundo"';
    } else if (name.includes('edit') && name.includes('file')) {
      return 'Cambia la l√≠nea que dice "version: 1.0" por "version: 2.0"';
    } else if (name.includes('search')) {
      return 'Busca todos los archivos .txt en el directorio';
    } else if (name.includes('create') && name.includes('directory')) {
      return 'Crea una carpeta llamada "proyectos"';
    } else if (name.includes('move')) {
      return 'Mueve el archivo documento.txt a la carpeta backup';
    } else if (name.includes('delete')) {
      return 'Elimina el archivo temporal.tmp';
    } else if (name.includes('get') && name.includes('info')) {
      return 'Dame informaci√≥n del archivo imagen.png';
    }
    
    return `Usa la herramienta ${name}`;
  }

  /**
   * Convertir tools MCP a formato function calling del proveedor
   * ‚ú® MEJORADO: Enriquece descripciones autom√°ticamente para mejor comprensi√≥n
   */
  convertMCPToolsToProviderFormat(tools, provider, options = {}) {
    if (!tools || tools.length === 0) return [];

    return tools.map(tool => {
      // üîç Enriquecer descripci√≥n autom√°ticamente
      let enrichedDescription = tool.description || `Herramienta ${tool.name}`;
      
      // Agregar contexto de par√°metros importantes a la descripci√≥n
      if (tool.inputSchema && tool.inputSchema.properties) {
        const params = tool.inputSchema.properties;
        const required = tool.inputSchema.required || [];
        
        // Si hay par√°metros requeridos, mencionarlos en la descripci√≥n
        if (required.length > 0) {
          const requiredParams = required.map(r => `'${r}'`).join(', ');
          enrichedDescription += `. Requiere: ${requiredParams}`;
        }
        
        // Agregar hints espec√≠ficos por tipo de tool
        if (tool.name.includes('read') || tool.name.includes('list')) {
          enrichedDescription += '. Use esta herramienta cuando necesite OBTENER o VER informaci√≥n';
        } else if (tool.name.includes('write') || tool.name.includes('create')) {
          enrichedDescription += '. Use esta herramienta cuando necesite CREAR o GUARDAR contenido';
        } else if (tool.name.includes('edit') || tool.name.includes('update') || tool.name.includes('modify')) {
          enrichedDescription += '. Use esta herramienta cuando necesite MODIFICAR contenido existente';
        } else if (tool.name.includes('delete') || tool.name.includes('remove')) {
          enrichedDescription += '. Use esta herramienta cuando necesite ELIMINAR algo';
        } else if (tool.name.includes('search') || tool.name.includes('find')) {
          enrichedDescription += '. Use esta herramienta cuando necesite BUSCAR o ENCONTRAR algo';
        } else if (tool.name.includes('move') || tool.name.includes('rename')) {
          enrichedDescription += '. Use esta herramienta cuando necesite MOVER o RENOMBRAR';
        }
      }

      // Formato com√∫n para function calling
      const toolDef = {
        name: (options.namespace ? `${tool.serverId}__${tool.name}` : tool.name),
        description: enrichedDescription,
        parameters: tool.inputSchema || { type: 'object', properties: {} }
      };

      // Adaptar seg√∫n el proveedor
      if (provider === 'openai') {
        return {
          type: 'function',
          function: toolDef
        };
      } else if (provider === 'anthropic') {
        return {
          name: toolDef.name,
          description: toolDef.description,
          input_schema: toolDef.parameters
        };
      } else if (provider === 'google') {
        return {
          name: toolDef.name,
          description: toolDef.description,
          parameters: toolDef.parameters
        };
      }

      return toolDef;
    });
  }

  /**
   * Generar system prompt UNIVERSAL para MCP (modelos sin function calling)
   * ‚ú® MEJORADO: M√°s simple, directo y con ejemplos
   */
  generateUniversalMCPSystemPrompt(tools, options = {}) {
    if (!tools || tools.length === 0) return '';

    const maxPerServer = typeof options.maxPerServer === 'number' ? options.maxPerServer : 6;
    const serverHints = options.serverHints || {};

    // Agrupar tools por servidor
    const serverIdToTools = tools.reduce((acc, t) => {
      const sid = t.serverId || 'unknown';
      if (!acc[sid]) acc[sid] = [];
      acc[sid].push(t);
      return acc;
    }, {});

    let out = '';
    out += 'HERRAMIENTAS DISPONIBLES:\n\n';
    out += 'Formato JSON: {"tool":"<server>__<name>","arguments":{...}}\n';
    out += 'Usa estas herramientas cuando el usuario pida ejecutar comandos, listar archivos, o trabajar con servidores.\n\n';

    const serverIds = Object.keys(serverIdToTools).sort();
    serverIds.forEach((serverId, sidx) => {
      const list = serverIdToTools[serverId] || [];
      const selected = list.slice(0, Math.max(1, maxPerServer));

      out += `[${serverId}]\n`;

      // Hints espec√≠ficos del servidor (p.ej., filesystem)
      const hints = serverHints[serverId] || {};
      if (hints.allowedDirsText) {
        const lines = String(hints.allowedDirsText).split('\n').map(l => l.trim()).filter(Boolean).slice(0, 2);
        out += `Dirs: ${lines.join(', ')}${lines.length < 2 ? '' : '...'}\n`;
      }
      if (hints.defaultRaw) {
        out += `‚ö†Ô∏è DEFAULT PATH: ${hints.defaultRaw}\n`;
        out += `CR√çTICO: SIEMPRE usa rutas ABSOLUTAS comenzando con este directorio.\n`;
        out += `Ejemplo correcto: "${hints.defaultRaw}\\archivo.txt"\n`;
        out += `Ejemplo INCORRECTO: "archivo.txt" (ruta relativa - NO usar)\n`;
      }

      // Acciones comunes (s√≥lo si es filesystem) - OPTIMIZADO
      if (serverId === 'filesystem') {
        const names = list.map(t => t.name);
        const has = (n) => names.includes(n);
        const actions = [];
        if (has('list_directory')) actions.push('listar‚Üílist_directory');
        if (has('read_text_file')) actions.push('leer‚Üíread_text_file');
        if (has('write_file')) actions.push('crear‚Üíwrite_file');
        if (has('edit_file')) actions.push('editar‚Üíedit_file');
        if (has('move_file')) actions.push('mover‚Üímove_file(source,destination con nombre completo)');
        if (has('search_files')) actions.push('buscar‚Üísearch_files(pattern,path)');
        if (actions.length > 0) {
          out += `Acciones: ${actions.join(', ')}\n`;
        }
      }

      selected.forEach((tool, tidx) => {
        const schema = tool.inputSchema || {};
        const properties = schema.properties || {};
        const required = schema.required || [];
        const keys = Object.keys(properties);

        // Formato compacto: nombre(params) - descripci√≥n COMPLETA (sin truncar)
        const reqParams = keys.filter(k => required.includes(k));
        const optParams = keys.filter(k => !required.includes(k));
        const paramsList = [...reqParams, ...optParams.map(p => `${p}?`)];
        
        out += `${tool.name}(${paramsList.join(',')})`;
        if (tool.description) {
          // ‚úÖ Mostrar descripci√≥n completa (era 60 caracteres, muy corto)
          out += ` - ${tool.description}`;
        }
        // üîß CR√çTICO: Reforzar nombre correcto para search_nodeterm
        if (tool.name === 'search_nodeterm') {
          out += ' ‚ö†Ô∏è NOMBRE CORRECTO: "search_nodeterm" (NO "search_noderm", NO "search_nodeterms")';
        }
        out += '\n';

        // Ejemplo compacto
        const exampleArgs = {};
        reqParams.forEach((p) => {
          const prop = properties[p] || {};
          if (prop.type === 'string') {
            const isPathLike = /path|file|dir|directory/i.test(p);
            if (isPathLike && hints.defaultRaw) {
              const baseRaw = hints.defaultRaw;
              const needsSep = !baseRaw.endsWith('\\') && !baseRaw.endsWith('/');
              if (/dir|directory/i.test(p)) {
                exampleArgs[p] = baseRaw;
              } else {
                const sep = needsSep ? (baseRaw.includes('\\') ? '\\' : '/') : '';
                exampleArgs[p] = `${baseRaw}${sep}file.txt`;
              }
            } else {
              exampleArgs[p] = 'value';
            }
          }
          else if (prop.type === 'array') exampleArgs[p] = [];
          else if (prop.type === 'object') exampleArgs[p] = {};
          else exampleArgs[p] = prop.type === 'number' ? 0 : true;
        });

        if (keys.length > 0) {
          out += `  {"tool":"${serverId}__${tool.name}","arguments":${JSON.stringify(exampleArgs)}}\n`;
        }
      });

      if (sidx < serverIds.length - 1) {
        out += '\n';
      }
    });

    // Ejemplos compactos con rutas absolutas
    out += '\nEJEMPLOS:\n';
    // Usar el defaultRaw si est√° disponible para filesystem
    const fsHints = serverHints['filesystem'] || {};
    const basePath = fsHints.defaultRaw || 'C:\\path\\to\\dir';
    const sep = basePath.includes('\\') ? '\\' : '/';
    out += `Listar: {"tool":"filesystem__list_directory","arguments":{"path":"${basePath}"}}\n`;
    out += `Leer: {"tool":"filesystem__read_file","arguments":{"path":"${basePath}${sep}config.json"}}\n`;
    out += `Crear: {"tool":"filesystem__write_file","arguments":{"path":"${basePath}${sep}file.txt","content":"texto"}}\n`;
    
    // üîß CR√çTICO: Ejemplo expl√≠cito para search_nodeterm (NO search_noderm)
    const hasSearchNodeterm = tools.some(t => t.name === 'search_nodeterm');
    if (hasSearchNodeterm) {
      out += `Buscar SSH: {"tool":"ssh-terminal__search_nodeterm","arguments":{"query":"AC68U"}}\n`;
      out += `Listar todos SSH: {"tool":"ssh-terminal__search_nodeterm","arguments":{}}\n`;
      out += '‚ö†Ô∏è IMPORTANTE: El nombre correcto es "search_nodeterm" (NO "search_noderm", NO "search_nodeterms").\n';
      out += 'üö´ CR√çTICO: search_nodeterm SOLO debe usarse cuando el usuario EXPL√çCITAMENTE pide buscar, listar o conectar a una conexi√≥n SSH. NO lo uses proactivamente ni para sugerencias.\n';
    }
    
    // üîí Ejemplos espec√≠ficos para Tenable.io
    const hasTenableTools = tools.some(t => t.serverId === 'tenable');
    if (hasTenableTools) {
      out += '\nüîí EJEMPLOS TENABLE.IO:\n';
      out += `Listar activos: {"tool":"tenable__get_assets","arguments":{"limit":"50","offset":"0"}}\n`;
      out += `Buscar activo: {"tool":"tenable__search_assets","arguments":{"search_term":"servidor01","limit":"50"}}\n`;
      out += `Detalles de activo: {"tool":"tenable__get_asset_details","arguments":{"asset_id":"uuid-del-activo"}}\n`;
      out += `Vulnerabilidades: {"tool":"tenable__get_asset_vulnerabilities","arguments":{"asset_id":"uuid-del-activo","severity":"critical","limit":"100"}}\n`;
      out += '‚ö†Ô∏è IMPORTANTE: Cuando el usuario mencione Tenable, vulnerabilidades, activos o seguridad, usa estas herramientas autom√°ticamente.\n';
    }
    
    out += '\nCR√çTICO: USA SIEMPRE RUTAS ABSOLUTAS. NO uses rutas relativas.\n';
    out += '\nüî¥ FORMATO DE RESPUESTA - CR√çTICO:\n';
    out += '‚Ä¢ Si el objetivo requiere M√öLTIPLES herramientas ‚Üí Usa formato PLAN: {"plan":[{"tool":"...","arguments":{...}},{"tool":"...","arguments":{...}}]}\n';
    out += '‚Ä¢ Si solo necesitas UNA herramienta ‚Üí {"tool":"<server>__<name>","arguments":{...}}\n';
    out += '‚Ä¢ NO preguntes, NO expliques, NO uses campos como "messages" o "response"\n';
    out += '‚Ä¢ Solo responde en texto natural cuando hayas completado TODAS las acciones\n';
    out += '‚Ä¢ Ejemplo PLAN: {"plan":[{"tool":"ssh-terminal__search_nodeterm","arguments":{"query":"Kepler"}},{"tool":"ssh-terminal__execute_ssh","arguments":{"hostId":"Kepler","command":"free -h"}}]}\n';
    out += '‚Ä¢ Ejemplo correcto: {"tool":"ssh-terminal__search_nodeterm","arguments":{"query":"Kepler"}}\n';
    out += '‚Ä¢ Ejemplo INCORRECTO: {"messages":["¬øPuedo usar search_nodeterm?"]}\n';
    out += '\n‚ö†Ô∏è NOMBRES DE HERRAMIENTAS: Usa EXACTAMENTE los nombres mostrados arriba. NO inventes nombres similares.\n';
    out += 'üö´ NO USES HERRAMIENTAS PROACTIVAMENTE: Solo ejecuta herramientas cuando el usuario lo pida expl√≠citamente.\n';
    out += '\nüî¥ REGLA CR√çTICA - CREAR vs EJECUTAR:\n';
    out += '‚Ä¢ "crea un script" / "crea un archivo" ‚Üí SOLO usa write_file (NO execute_local)\n';
    out += '‚Ä¢ "ejecuta el script" / "run script" / "corre el comando" ‚Üí USA execute_local\n';
    out += '‚Ä¢ "crea y ejecuta" ‚Üí PRIMERO write_file, LUEGO execute_local\n';
    out += '‚Ä¢ Si el usuario SOLO pide CREAR/GENERAR/ESCRIBIR ‚Üí NO ejecutes comandos autom√°ticamente\n\n';

    return out;
  }

  /**
   * Detectar PLAN de herramientas (modo ReAct) en la respuesta
   * Retorna: { isPlan: true, tools: [{tool, arguments}, ...] } o null
   */
  _detectToolPlan(response) {
    if (!response || typeof response !== 'string') return null;
    
    // Buscar JSON con estructura de plan: {"plan": [...]}
    // Mejorado: buscar tambi√©n variantes como "tools", "steps", "actions"
    const jsonPatterns = [
      /\{[\s\S]*?"plan"[\s\S]*?\[[\s\S]*?\][\s\S]*?\}/g,
      /\{[\s\S]*?"tools"[\s\S]*?\[[\s\S]*?\][\s\S]*?\}/g,
      /\{[\s\S]*?"steps"[\s\S]*?\[[\s\S]*?\][\s\S]*?\}/g
    ];
    
    let matches = [];
    for (const pattern of jsonPatterns) {
      const found = response.match(pattern);
      if (found) matches = matches.concat(found);
    }
    
    if (!matches || matches.length === 0) return null;
    
    for (const jsonStr of matches) {
      try {
        const data = JSON.parse(jsonStr);
        // Buscar plan, tools, steps, o actions
        const planArray = data.plan || data.tools || data.steps || data.actions;
        
        if (planArray && Array.isArray(planArray) && planArray.length > 0) {
          // Validar que cada elemento del plan tenga tool
          const validTools = planArray.filter(t => {
            if (!t) return false;
            // Aceptar tool, toolName, name, o function
            return !!(t.tool || t.toolName || t.name || t.function);
          });
          
          if (validTools.length > 0) {
            debugLogger.debug('AIService.MCP', 'Plan detectado con herramientas v√°lidas', {
              herramientas: validTools.length,
              formato: data.plan ? 'plan' : (data.tools ? 'tools' : (data.steps ? 'steps' : 'actions'))
            });
            return {
              isPlan: true,
              tools: validTools.map(t => ({
                tool: t.tool || t.toolName || t.name || t.function,
                toolName: t.tool || t.toolName || t.name || t.function,
                arguments: t.arguments || t.args || t.params || {},
                serverId: t.serverId || t.server || null
              }))
            };
          }
        }
      } catch (e) {
        // Intentar extraer JSON m√°s espec√≠fico si falla el parse completo
        try {
          // Buscar solo el array del plan
          const arrayMatch = jsonStr.match(/\[[\s\S]*\]/);
          if (arrayMatch) {
            const arrayData = JSON.parse(arrayMatch[0]);
            if (Array.isArray(arrayData) && arrayData.length > 0) {
              const validTools = arrayData.filter(t => t && (t.tool || t.toolName || t.name));
              if (validTools.length > 0) {
                debugLogger.debug('AIService.MCP', 'Plan detectado (array directo)', {
                  herramientas: validTools.length
                });
                return {
                  isPlan: true,
                  tools: validTools.map(t => ({
                    tool: t.tool || t.toolName || t.name,
                    toolName: t.tool || t.toolName || t.name,
                    arguments: t.arguments || t.args || {},
                    serverId: t.serverId || t.server || null
                  }))
                };
              }
            }
          }
        } catch (e2) {
          continue;
        }
      }
    }
    
    return null;
  }

  /**
   * Ejecutar un PLAN completo de herramientas (modo ReAct)
   */
  async _executeToolPlan(plan, callbacks = {}, modelId = null) {
    debugLogger.debug('AIService.MCP', 'Ejecutando plan de herramientas', {
      herramientas: plan.tools.length
    });
    
    const results = [];
    let lastSearchResult = null; // üîß Guardar resultado de search_nodeterm para usar en execute_ssh
    
    for (let i = 0; i < plan.tools.length; i++) {
      const toolSpec = plan.tools[i];
      let toolName = toolSpec.toolName || toolSpec.tool;
      let args = { ...toolSpec.arguments || {} };
      
      debugLogger.debug('AIService.MCP', 'Ejecutando herramienta del plan', {
        indice: i + 1,
        total: plan.tools.length,
        tool: toolName
      });
      
      // üîß CR√çTICO: Si es execute_ssh y no tiene hostId, intentar extraerlo del √∫ltimo search_nodeterm
      if ((toolName.includes('execute_ssh') || toolName === 'execute_ssh') && !args.hostId && lastSearchResult) {
        try {
          // Intentar extraer hostId del resultado de search_nodeterm
          const searchText = typeof lastSearchResult === 'string' ? lastSearchResult : 
                            (lastSearchResult?.content?.[0]?.text || JSON.stringify(lastSearchResult));
          
          // Buscar el nombre del servidor en el resultado (ej: "Kepler")
          const queryMatch = plan.tools.find(t => 
            (t.toolName || t.tool || '').includes('search_nodeterm')
          )?.arguments?.query;
          
          if (queryMatch) {
            args.hostId = queryMatch; // Usar el query original como hostId
            debugLogger.debug('AIService.MCP', 'hostId inyectado desde search_nodeterm', {
              hostId: args.hostId,
              query: queryMatch
            });
          } else {
            // Intentar extraer del resultado parseado
            let parsedResult = null;
            try {
              if (typeof lastSearchResult === 'object' && lastSearchResult._originalResult) {
                parsedResult = lastSearchResult._originalResult;
              } else if (typeof searchText === 'string') {
                const jsonMatch = searchText.match(/\{[\s\S]*?"ssh_results"[\s\S]*?\}/);
                if (jsonMatch) {
                  parsedResult = JSON.parse(jsonMatch[0]);
                }
              }
              
              if (parsedResult?.ssh_results?.[0]?.label) {
                args.hostId = parsedResult.ssh_results[0].label;
                debugLogger.debug('AIService.MCP', 'hostId extra√≠do del resultado parseado', {
                  hostId: args.hostId
                });
              } else if (parsedResult?.ssh_results?.[0]?.name) {
                args.hostId = parsedResult.ssh_results[0].name.split('[')[0].trim();
                debugLogger.debug('AIService.MCP', 'hostId extra√≠do del name', {
                  hostId: args.hostId
                });
              }
            } catch (parseError) {
              debugLogger.warn('AIService.MCP', 'Error parseando resultado de search_nodeterm', {
                error: parseError.message
              });
            }
          }
        } catch (error) {
          debugLogger.warn('AIService.MCP', 'Error extrayendo hostId de search_nodeterm', {
            error: error.message
          });
        }
      }
      
      // Normalizar y resolver serverId
      const normalized = this._normalizeFunctionCall(toolName, args);
      const serverId = normalized.serverId;
      const actualToolName = normalized.toolName;
      const callArgs = normalized.arguments;
      
      // Guardar mensaje de tool call
      conversationService.addMessage('assistant_tool_call', `Llamando herramienta: ${actualToolName}`, {
        isToolCall: true,
        toolName: actualToolName,
        toolArgs: callArgs
      });
      
      // Callback de tool ejecutada
      if (callbacks.onToolResult) {
        callbacks.onToolResult({ toolName: actualToolName, args: callArgs, result: null });
      }
      
      try {
        if (!serverId) {
          throw new Error(`No se pudo resolver el servidor para la herramienta ${actualToolName}`);
        }
        
        const result = await mcpClient.callTool(serverId, actualToolName, callArgs);
        const text = result?.content?.[0]?.text || 'OK';
        
        // üîß CR√çTICO: Guardar resultado de search_nodeterm para usar en execute_ssh siguientes
        if (actualToolName.includes('search_nodeterm') || actualToolName === 'search_nodeterm') {
          lastSearchResult = result;
        }
        
        const planSummary = summarizeToolResult({
          toolName: actualToolName,
          args: callArgs,
          resultText: text
        });
        
        // Guardar resultado de la tool
        conversationService.addMessage('tool', planSummary, {
          isToolResult: true,
          toolName: actualToolName,
          toolArgs: callArgs,
          toolResultText: text,
          toolResultSummary: planSummary
        });
        rememberToolExecution(conversationService.currentConversationId, actualToolName, callArgs, {
          summary: planSummary,
          rawText: text,
          isError: false
        });
        
        // üîß CR√çTICO: Guardar tanto el objeto result completo como el texto
        results.push({ 
          tool: actualToolName, 
          success: true, 
          result: result,  // Objeto completo del MCP
          resultText: text, // Texto extra√≠do
          rawResult: result?.content?.[0]?.text || text // Texto crudo del resultado
        });
        
        debugLogger.debug('AIService.MCP', 'Resultado guardado en plan', {
          tool: actualToolName,
          hasResult: !!result,
          hasResultText: !!text,
          textLength: text?.length || 0
        });
        
        // Callback de tool ejecutada con resultado
        if (callbacks.onToolResult) {
          callbacks.onToolResult({ toolName: actualToolName, args: callArgs, result });
        }
        
        debugLogger.debug('AIService.MCP', 'Herramienta del plan completada', {
          indice: i + 1,
          total: plan.tools.length,
          tool: actualToolName
        });
      } catch (error) {
        const errorMsg = `‚ùå Error ejecutando ${actualToolName}: ${error.message}`;
        const errorSummary = summarizeToolResult({
          toolName: actualToolName,
          args: callArgs,
          resultText: errorMsg,
          isError: true
        });
        conversationService.addMessage('tool', errorSummary, {
          error: true,
          isToolResult: true,
          toolName: actualToolName,
          toolArgs: callArgs,
          toolResultText: errorMsg,
          toolResultSummary: errorSummary
        });
        results.push({ tool: actualToolName, success: false, error: error.message });
        console.error(`   ‚ùå [${i + 1}/${plan.tools.length}] ${actualToolName} fall√≥:`, error.message);
      }
    }
    
    debugLogger.debug('AIService.MCP', 'Plan completado', {
      exitosas: results.filter(r => r.success).length,
      total: results.length
    });
    
    // üîß CR√çTICO: Despu√©s de ejecutar el plan, pedir al modelo que genere una respuesta basada en los resultados
    debugLogger.debug('AIService.MCP', 'Intentando generar respuesta despu√©s del plan', {
      resultsCount: results.length,
      successCount: results.filter(r => r.success).length,
      hasCallbacks: !!callbacks,
      hasModelId: !!modelId
    });
    
    const currentConversation = conversationService.getCurrentConversation();
    if (currentConversation && callbacks) {
      const conversationMessages = currentConversation.messages || [];
      const lastUserMessage = conversationMessages.filter(m => m.role === 'user').slice(-1)[0];
      const lastUserGoal = lastUserMessage?.content || '';
      
      // Obtener el √∫ltimo resultado exitoso (normalmente el de execute_ssh)
      const lastSuccessResult = results.filter(r => r.success).slice(-1)[0];
      debugLogger.debug('AIService.MCP', '√öltimo resultado exitoso', {
        hasResult: !!lastSuccessResult,
        hasResultObj: !!(lastSuccessResult?.result),
        hasResultText: !!(lastSuccessResult?.resultText),
        tool: lastSuccessResult?.tool
      });
      
      if (lastSuccessResult && (lastSuccessResult.result || lastSuccessResult.resultText || lastSuccessResult.rawResult)) {
        // Construir mensaje con el resultado para que el modelo genere una respuesta
        let resultText = '';
        
        // Prioridad 1: rawResult (texto crudo del resultado)
        if (lastSuccessResult.rawResult) {
          resultText = lastSuccessResult.rawResult;
        }
        // Prioridad 2: resultText (texto extra√≠do)
        else if (lastSuccessResult.resultText) {
          resultText = lastSuccessResult.resultText;
        }
        // Prioridad 3: result object
        else if (lastSuccessResult.result) {
          if (typeof lastSuccessResult.result === 'string') {
            resultText = lastSuccessResult.result;
          } else if (lastSuccessResult.result?.content?.[0]?.text) {
            resultText = lastSuccessResult.result.content[0].text;
          } else {
            resultText = JSON.stringify(lastSuccessResult.result, null, 2);
          }
        }
        
        // üîß SOLUCI√ìN DIRECTA: Si es execute_ssh y el resultado tiene informaci√≥n √∫til, devolverlo directamente
        if (lastSuccessResult.tool?.includes('execute_ssh') && resultText && resultText.includes('load average')) {
          // Extraer solo la l√≠nea con el resultado del comando
          const lines = resultText.split('\n');
          const resultLine = lines.find(l => l.includes('load average') || (l.includes('Mem:') && l.includes('total')));
          if (resultLine) {
            return `El servidor Kepler tiene la siguiente informaci√≥n de RAM:\n\n${resultText.split('\n').filter(l => l.trim().length > 0 && !l.includes('Ejecutado en')).join('\n')}`;
          }
        }
        
        if (resultText && resultText.trim().length > 0) {
          debugLogger.debug('AIService.MCP', 'Resultado extra√≠do, generando respuesta', {
            resultTextLength: resultText.length,
            preview: resultText.substring(0, 200)
          });
          
          const followUpMessages = [
            ...conversationMessages.slice(-5), // √öltimos 5 mensajes para contexto
            {
              role: 'user',
              content: `He ejecutado el comando solicitado en el servidor. Resultado:\n\n${resultText}\n\nAhora genera una respuesta natural y √∫til explicando el resultado al usuario. Incluye informaci√≥n relevante del resultado (ej: valores de memoria, estado del sistema, etc.). Objetivo original: ${lastUserGoal}`
            }
          ];
          
          try {
            // Obtener el modelo actual
            const currentModelId = modelId || this.currentModel?.id;
            if (!currentModelId) {
              debugLogger.warn('AIService.MCP', 'No hay modelo actual para generar respuesta', {
                modelId,
                currentModelId: this.currentModel?.id
              });
              throw new Error('No hay modelo actual');
            }
            
            debugLogger.debug('AIService.MCP', 'Llamando al modelo para generar respuesta', {
              modelId: currentModelId,
              messagesCount: followUpMessages.length
            });
            
            // Llamar al modelo para generar respuesta basada en el resultado
            const modelResponse = await this.sendToLocalModelStreamingWithCallbacks(
              currentModelId,
              followUpMessages,
              callbacks,
              { maxTokens: 800, temperature: 0.7 }
            );
            
            debugLogger.debug('AIService.MCP', 'Respuesta del modelo recibida', {
              hasResponse: !!modelResponse,
              length: modelResponse?.length || 0,
              preview: modelResponse?.substring(0, 100) || '(vac√≠o)'
            });
            
            if (modelResponse && modelResponse.trim().length > 0) {
              return modelResponse;
            } else {
              debugLogger.warn('AIService.MCP', 'Modelo devolvi√≥ respuesta vac√≠a');
            }
          } catch (error) {
            debugLogger.error('AIService.MCP', 'Error generando respuesta despu√©s del plan', {
              error: error.message,
              stack: error.stack
            });
          }
        } else {
          debugLogger.warn('AIService.MCP', 'No se pudo extraer texto del resultado', {
            hasResult: !!lastSuccessResult.result,
            hasResultText: !!lastSuccessResult.resultText
          });
        }
      }
    }
    
    // Fallback si no se pudo generar respuesta
    const successCount = results.filter(r => r.success).length;
    if (successCount === results.length) {
      return '‚úÖ Operaci√≥n completada correctamente.';
    } else {
      return `‚ö†Ô∏è Operaci√≥n completada con ${successCount}/${results.length} herramientas exitosas.`;
    }
  }

  /**
   * Detectar si la respuesta del modelo solicita usar una tool
   */
  detectToolCallInResponse(response) {
    if (!response || typeof response !== 'string') return null;
    
    // NUEVO: Log agresivo para ver la respuesta COMPLETA
    debugLogger.debug('AIService.MCP', 'detectToolCallInResponse entrada', {
      tipo: typeof response,
      length: response?.length,
      muestra: response?.substring(0, 200),
      incluyeTool: response?.includes('"tool"'),
      incluyeLlave: response?.includes('{')
    });
    
    try {
      // Estrategia 1: Bloques expl√≠citos con backticks (```json...```)
      const toolCall = this._extractToolCallFromCodeBlock(response);
      if (toolCall) {
        return toolCall;
      }
      
      // Estrategia 2: JSON flexible en cualquier posici√≥n (con pre√°mbulo/epilogo)
      const jsonToolCall = this._extractToolCallFromJSON(response);
      return jsonToolCall;
      
    } catch (error) {
      // Error inesperado en detecci√≥n
      if (response.trim().startsWith('{') && response.trim().endsWith('}')) {
        debugLogger.debug('AIService.MCP', 'JSON inv√°lido detectado al buscar tool call', {
          error: error.message.substring(0, 100)
        });
      }
      console.error('‚ùå [AIService] Error detectando tool call:', error.message);
      return null;
    }
  }

  /**
   * Detectar solicitud de PROMPT MCP en respuesta del modelo
   * Formato esperado:
   * {"prompt": {"server":"<serverId>", "name":"<promptName>", "arguments":{...}}}
   */
  detectPromptCallInResponse(response) {
    if (!response || typeof response !== 'string') return null;
    try {
      // Buscar bloque JSON que contenga "prompt": { ... }
      const re = /\{[\s\S]*?"prompt"\s*:\s*\{[\s\S]*?\}[\s\S]*?\}/g;
      const matches = response.match(re);
      if (!matches) return null;
      for (const jsonStr of matches) {
        try {
          const data = JSON.parse(jsonStr);
          const pr = data.prompt;
          if (pr && typeof pr === 'object' && pr.name) {
            return { serverId: pr.server || pr.serverId, promptName: pr.name, arguments: pr.arguments || {} };
          }
        } catch (_) { /* ignore */ }
      }
      return null;
    } catch (_) {
      return null;
    }
  }

  async _handlePromptCallAndContinue({ serverId, promptName, arguments: args }, messages, callbacks, options, modelId) {
    try {
      const res = await mcpClient.getPrompt(serverId, promptName, args || {});
      const promptText = res?.result?.content?.[0]?.text || res?.content?.[0]?.text || '';
      const nextMessages = [...messages, { role: 'user', content: promptText }];
      return await this.sendToLocalModelStreamingWithCallbacks(modelId, nextMessages, callbacks, { ...options, maxTokens: Math.max(800, options.maxTokens || 1500) });
    } catch (e) {
      return `Error obteniendo prompt ${promptName} de ${serverId || 'desconocido'}: ${e.message}`;
    }
  }

  /**
   * Convertir JSON Schema a el formato de par√°metros de Gemini (tipos en MAY√öSCULAS)
   */
  _toGeminiSchema(schema) {
    if (!schema || typeof schema !== 'object') return { type: 'OBJECT' };

    const upper = (t) => {
      if (!t) return undefined;
      const map = {
        object: 'OBJECT',
        array: 'ARRAY',
        string: 'STRING',
        number: 'NUMBER',
        integer: 'INTEGER',
        boolean: 'BOOLEAN'
      };
      return map[String(t).toLowerCase()] || String(t).toUpperCase();
    };

    const convert = (node) => {
      if (!node || typeof node !== 'object') return node;
      const t = upper(node.type);
      if (t === 'OBJECT') {
        const props = node.properties || {};
        const outProps = {};
        Object.keys(props).forEach((k) => {
          outProps[k] = convert(props[k]);
        });
        return {
          type: 'OBJECT',
          properties: outProps,
          required: Array.isArray(node.required) ? node.required : undefined
        };
      }
      if (t === 'ARRAY') {
        return {
          type: 'ARRAY',
          items: convert(node.items || {})
        };
      }
      // Primitivos
      return { type: t };
    };

    return convert(schema);
  }

  _resolveToolInfo(toolName, serverIdHint = null) {
    // üîí DEBUG: Validar entrada
    if (!toolName) {
      debugLogger.warn('AIService.MCP', '_resolveToolInfo recibi√≥ toolName vac√≠o', { toolName, serverIdHint });
      return { serverId: null, toolName: toolName || '' };
    }
    
    const tools = mcpClient.getAvailableTools() || [];
    
    // üîí ESPECIAL: Si serverIdHint es 'tenable', buscar espec√≠ficamente en Tenable primero
    if (serverIdHint === 'tenable') {
      const tenableMatch = tools.find(t => t.serverId === 'tenable' && t.name === toolName);
      if (tenableMatch) {
        debugLogger.debug('AIService.MCP', 'Herramienta de Tenable encontrada', { toolName, serverId: 'tenable' });
        return { serverId: 'tenable', toolName: tenableMatch.name };
      }
    }
    
    if (serverIdHint) {
      const match = tools.find(t => t.serverId === serverIdHint && t.name === toolName);
      if (match) {
        return { serverId: match.serverId, toolName: match.name };
      }
    }

    const exactMatches = tools.filter(t => t.name === toolName);
    if (exactMatches.length === 1) {
      return { serverId: exactMatches[0].serverId, toolName: exactMatches[0].name };
    }

    if (exactMatches.length > 1) {
      // üîí MEJORADO: Priorizar Tenable si hay m√∫ltiples matches
      const tenableMatch = exactMatches.find(t => t.serverId === 'tenable');
      if (tenableMatch) {
        return { serverId: tenableMatch.serverId, toolName: tenableMatch.name };
      }
      return { serverId: exactMatches[0].serverId, toolName: exactMatches[0].name };
    }

    const namespacedMatch = tools.find(t => `${t.serverId}__${t.name}` === toolName);
    if (namespacedMatch) {
      return { serverId: namespacedMatch.serverId, toolName: namespacedMatch.name };
    }

    const filesystemMatch = tools.find(t => t.serverId === 'filesystem' && t.name === toolName);
    if (filesystemMatch) {
      return { serverId: filesystemMatch.serverId, toolName: filesystemMatch.name };
    }
    
    // üîí ESPECIAL: Buscar en Tenable si el nombre contiene palabras relacionadas
    if (toolName.includes('asset') || toolName.includes('vulnerability') || toolName.includes('tenable')) {
      const tenableTools = tools.filter(t => t.serverId === 'tenable');
      const tenableMatch = tenableTools.find(t => 
        t.name.includes(toolName) || toolName.includes(t.name)
      );
      if (tenableMatch) {
        debugLogger.debug('AIService.MCP', 'Herramienta de Tenable encontrada por nombre relacionado', { 
          toolName, 
          encontrada: tenableMatch.name 
        });
        return { serverId: 'tenable', toolName: tenableMatch.name };
      }
    }

    // üîß NUEVO: Fuzzy matching para nombres similares (correcci√≥n autom√°tica)
    // Buscar herramientas con nombres similares (√∫til cuando el modelo genera nombres incorrectos)
    const similarTools = this._findSimilarToolName(toolName, tools);
    if (similarTools.length > 0) {
      const bestMatch = similarTools[0];
      console.warn(`‚ö†Ô∏è [AIService] Tool "${toolName}" no encontrada, usando similar: "${bestMatch.name}"`);
      return { serverId: bestMatch.serverId, toolName: bestMatch.name };
    }

    return { serverId: serverIdHint, toolName };
  }

  /**
   * Encontrar herramientas con nombres similares usando distancia de Levenshtein
   */
  _findSimilarToolName(targetName, tools, maxDistance = 3) {
    if (!targetName || typeof targetName !== 'string' || !tools || tools.length === 0) {
      return [];
    }

    const targetLower = targetName.toLowerCase();
    const candidates = [];

    for (const tool of tools) {
      const toolNameLower = tool.name.toLowerCase();
      
      // Calcular distancia de Levenshtein simple
      const distance = this._levenshteinDistance(targetLower, toolNameLower);
      
      // Tambi√©n verificar si el nombre contiene el target o viceversa
      const containsMatch = toolNameLower.includes(targetLower) || targetLower.includes(toolNameLower);
      
      if (distance <= maxDistance || containsMatch) {
        candidates.push({
          ...tool,
          distance: containsMatch ? Math.min(distance, 1) : distance
        });
      }
    }

    // Ordenar por distancia (menor es mejor)
    candidates.sort((a, b) => a.distance - b.distance);
    
    return candidates;
  }

  /**
   * Calcular distancia de Levenshtein entre dos strings
   */
  _levenshteinDistance(str1, str2) {
    const m = str1.length;
    const n = str2.length;
    const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));

    for (let i = 0; i <= m; i++) dp[i][0] = i;
    for (let j = 0; j <= n; j++) dp[0][j] = j;

    for (let i = 1; i <= m; i++) {
      for (let j = 1; j <= n; j++) {
        if (str1[i - 1] === str2[j - 1]) {
          dp[i][j] = dp[i - 1][j - 1];
        } else {
          dp[i][j] = Math.min(
            dp[i - 1][j] + 1,     // eliminaci√≥n
            dp[i][j - 1] + 1,     // inserci√≥n
            dp[i - 1][j - 1] + 1  // sustituci√≥n
          );
        }
      }
    }

    return dp[m][n];
  }

  _normalizeFunctionCall(fullName, rawArgs) {
    debugLogger.debug('AIService.MCP', '_normalizeFunctionCall entrada', { fullName, rawArgs });
    let argsObj;
    if (!rawArgs) {
      argsObj = {};
    } else if (typeof rawArgs === 'string') {
      argsObj = { tool: rawArgs };
    } else {
      argsObj = { ...rawArgs };
    }

    let toolName = argsObj.tool || argsObj.name || fullName;
    let serverId = argsObj.server || argsObj.serverId || null;

    // üîß ARREGLO: Manejar nombres namespaced con doble gui√≥n bajo (__) O un solo gui√≥n bajo (_)
    // El modelo a veces genera ssh-terminal_search_noderm en lugar de ssh-terminal__search_nodeterm
    if (!serverId && typeof fullName === 'string') {
      // Intentar con doble gui√≥n bajo primero (formato correcto)
      if (fullName.includes('__')) {
        const idx = fullName.indexOf('__');
        if (idx >= 0) {
          serverId = fullName.slice(0, idx);
          toolName = fullName.slice(idx + 2);
        }
      }
      // Si no se encontr√≥, intentar con un solo gui√≥n bajo (formato incorrecto del modelo)
      else if (fullName.includes('_') && !fullName.includes('__')) {
        // Buscar patrones como "ssh-terminal_search_noderm"
        const parts = fullName.split('_');
        if (parts.length >= 2) {
          const possibleServerId = parts[0];
          const possibleBaseName = parts.slice(1).join('_');
          
          // Verificar si existe un servidor con ese ID
          const tools = mcpClient.getAvailableTools() || [];
          const serverExists = tools.some(t => t.serverId === possibleServerId);
          if (serverExists) {
            serverId = possibleServerId;
            toolName = possibleBaseName;
          }
        }
      }
    }

    if (!serverId) {
      const tools = mcpClient.getAvailableTools() || [];
      const matches = tools.filter(t => t.name === toolName);
      if (matches.length === 1) {
        serverId = matches[0].serverId;
      } else if (matches.length > 1) {
        serverId = matches[0].serverId;
      }
    }

    if (!serverId) {
      const fallbackDir = this._getMcpDefaultDir('filesystem');
      if (fallbackDir) {
        serverId = 'filesystem';
      }
    }

    if (!serverId) {
      const tools = mcpClient.getAvailableTools() || [];
      const fsMatch = tools.find(t => t.serverId === 'filesystem' && t.name === toolName);
      if (fsMatch) {
        serverId = 'filesystem';
      }
    }

    if (!serverId) {
      debugLogger.warn('AIService.MCP', '_normalizeFunctionCall sin serverId resuelto', {
        fullName,
        rawArgs,
        toolName,
        argsObj
      });
    }

    // Construir argumentos limpios
    let finalArgs = argsObj.arguments || argsObj.args || argsObj.parameters;
    if (typeof finalArgs === 'string') {
      finalArgs = { tool: finalArgs };
    }

    if (!finalArgs || typeof finalArgs !== 'object' || Array.isArray(finalArgs)) {
      const copy = { ...argsObj };
      delete copy.tool;
      delete copy.name;
      delete copy.server;
      delete copy.serverId;
      delete copy.arguments;
      delete copy.args;
      delete copy.parameters;
      finalArgs = copy;
    } else {
      finalArgs = { ...finalArgs };
    }

    if (finalArgs.arguments && typeof finalArgs.arguments === 'object' && !Array.isArray(finalArgs.arguments)) {
      finalArgs = { ...finalArgs, ...finalArgs.arguments };
      delete finalArgs.arguments;
    }

    if (finalArgs.server || finalArgs.serverId) {
      serverId = serverId || finalArgs.server || finalArgs.serverId;
      delete finalArgs.server;
      delete finalArgs.serverId;
    }

    const nestedTool = finalArgs.tool || finalArgs.toolName;
    if (nestedTool) {
      toolName = nestedTool;
      delete finalArgs.tool;
      delete finalArgs.toolName;
    }

    if (!serverId) {
      const tools = mcpClient.getAvailableTools() || [];
      const matches = tools.filter(t => t.name === toolName);
      if (matches.length === 1) {
        serverId = matches[0].serverId;
      }
    }

    if (!finalArgs || typeof finalArgs !== 'object') {
      finalArgs = {};
    }

    const defaultInfo = serverId ? this._getMcpDefaultDir(serverId) : this._getMcpDefaultDir('filesystem');
    const defaultPath = defaultInfo?.raw || defaultInfo?.normalized || null;
    if (defaultPath) {
      if (['list_directory', 'directory_tree', 'list_directory_with_sizes'].includes(toolName) && !finalArgs.path) {
        finalArgs.path = defaultPath;
      }
      if (toolName === 'read_text_file' && !finalArgs.path) {
        finalArgs.path = defaultPath;
      }
    }

    const resolved = this._resolveToolInfo(toolName, serverId);
    const normalized = { serverId: resolved.serverId, toolName: resolved.toolName, arguments: finalArgs };
    debugLogger.debug('AIService.MCP', '_normalizeFunctionCall salida', normalized);
    return normalized;
  }

  async _handleRemotePostResponse(rawResponse, conversationMessages, mcpContext, callbacks, options, model) {
    let responseText = rawResponse || '';

    if (mcpContext?.hasTools) {
      if (!this._getMcpDefaultDir('filesystem')) {
        try { await this.getAllowedDirectoriesCached(); } catch (_) {}
      }
      const toolCall = this.detectToolCallInResponse(responseText);
      if (toolCall) {
        if (this.toolOrchestrator) {
          try {
            const orchestratorResult = await this.toolOrchestrator.executeLoop({
              modelId: model.id,
              initialToolCall: toolCall,
              baseProviderMessages: conversationMessages,
              detectToolCallInResponse: (resp) => this.detectToolCallInResponse(resp),
              callModelFn: async () => 'Hecho.',
              callbacks,
              options,
              turnId: options?.turnId
            });
            return orchestratorResult;
          } catch (error) {
            console.error('[MCP] Error en loop remoto:', error);
            return `Error ejecutando herramienta: ${error.message}`;
          }
        }

        try {
          const normalized = this._normalizeFunctionCall(toolCall.toolName || toolCall.tool, toolCall.arguments || {});
          const serverId = normalized.serverId;
          const toolName = normalized.toolName;
          const callArgs = normalized.arguments;
          
          conversationService.addMessage('assistant_tool_call', `Llamando herramienta: ${toolName}`, { isToolCall: true, toolName, toolArgs: callArgs });
          const result = serverId ? await mcpClient.callTool(serverId, toolName, callArgs)
                                  : await mcpClient.callTool(toolName, callArgs);
          const text = result?.content?.[0]?.text || 'OK';
          conversationService.addMessage('tool', text, { isToolResult: true, toolName, toolArgs: callArgs });
          return 'Hecho.';
        } catch (error) {
          conversationService.addMessage('tool', `‚ùå Error ejecutando herramienta: ${error.message}`, { error: true });
          return `Error ejecutando herramienta: ${error.message}`;
        }
      }

      const promptCall = this.detectPromptCallInResponse(responseText);
      if (promptCall) {
        return await this._handlePromptCallAndContinue(promptCall, conversationMessages, callbacks, options, model.id);
      }
    }

    return responseText;
  }

  /**
   * Inferir intenci√≥n b√°sica del usuario para Filesystem a partir del texto
   */
  _inferFilesystemIntent(text) {
    if (!text || typeof text !== 'string') return null;
    const s = text.toLowerCase();
    if (/(mover|mueve|renombrar|renombra|move|rename)/.test(s)) return 'move';
    if (/(copiar|copia|copy)/.test(s)) return 'copy';
    if (/(borrar|eliminar|borra|remove|delete)/.test(s)) return 'delete';
    if (/(crear carpeta|crear directorio|mkdir|create directory)/.test(s)) return 'mkdir';
    if (/(listar|lista|ver contenido|list)/.test(s)) return 'list';
    if (/(leer|read)/.test(s)) return 'read';
    if (/(editar|edit)/.test(s)) return 'edit';
    return null;
  }

  /**
   * Extraer tool call de bloques de c√≥digo (```json...```)
   */
  _extractToolCallFromCodeBlock(response) {
    const jsonBlockRegex = /```(?:json|tool|tool_call)?\s*([\s\S]*?)```/gi;
    let match = jsonBlockRegex.exec(response);
    
    while (match) {
      try {
        const jsonContent = match[1].trim();
        const data = JSON.parse(jsonContent);
        const toolCall = this._normalizeToolCall(data);
        if (toolCall) return toolCall;
      } catch (e) {
        // Este bloque no es v√°lido, intentar siguiente
      }
      match = jsonBlockRegex.exec(response);
    }
    
    return null;
  }

  /**
   * Extraer tool call de JSON flexible (en cualquier posici√≥n con pre√°mbulo/epilogo)
   */
  _extractToolCallFromJSON(response) {
    // NUEVO: Log para debugging
    debugLogger.debug('AIService.MCP', 'Buscando JSON en respuesta', { length: response.length });
    
    // Buscar JSON que contenga "tool" o "use_tool"
    // Permite pre√°mbulo y epilogo alrededor del JSON
    // FIX: Usar [\s\S]* GREEDY (sin ?) para capturar hasta el √öLTIMO } del objeto
    const jsonPattern = /\{[\s\S]*?"(?:tool|use_tool)"[\s\S]*\}/g;
    const matches = response.match(jsonPattern);
    
    if (!matches) {
      debugLogger.debug('AIService.MCP', 'No se encontr√≥ JSON con tool/use_tool');
      return null;
    }
    
    // Intentar cada JSON encontrado (puede haber m√∫ltiples)
    for (let i = 0; i < matches.length; i++) {
      const jsonStr = matches[i];
      debugLogger.debug('AIService.MCP', 'Intentando candidato para tool call', {
        indice: i + 1,
        preview: jsonStr.substring(0, 50).replace(/\n/g, '\\n')
      });
      
      try {
        const data = JSON.parse(jsonStr);
        const toolCall = this._normalizeToolCall(data);
        if (toolCall) {
          debugLogger.debug('AIService.MCP', 'Tool call detectado', { tool: toolCall.toolName });
          return toolCall;
        }
      } catch (e) {
        debugLogger.debug('AIService.MCP', 'JSON inv√°lido durante parseo de tool call', { error: e.message });
        continue;
      }
    }
    
    debugLogger.debug('AIService.MCP', 'Ning√∫n candidato fue un tool call v√°lido');
    return null;
  }

  /**
   * Validar si data es un tool call v√°lido
   */
  _isValidToolCall(data) {
    if (!data || typeof data !== 'object') return false;
    
    const hasToolField = (data.tool && typeof data.tool === 'string') ||
                         (data.use_tool && typeof data.use_tool === 'string');
    
    return hasToolField;
  }

  /**
   * Normalizar tool call a formato est√°ndar
   */
  _normalizeToolCall(data) {
    if (!this._isValidToolCall(data)) return null;
    
    return {
      toolName: data.tool || data.use_tool,
      arguments: data.arguments || {},
      serverId: data.serverId || data.server || null
    };
  }

  /**
   * Manejar loop de tool calls para modelos locales (system prompt)
   * Soporta m√∫ltiples iteraciones, re-inyecci√≥n de resultados, y detecci√≥n de loops
   */
  async handleLocalToolCallLoop(toolCall, messages, callbacks = {}, options = {}, modelId, maxIterations) {
    let iteration = 0;
    let currentToolCall = toolCall;
    let conversationMessages = [...messages];
    let lastToolName = null;
    let consecutiveRepeats = 0;
    const lastUserGoal = (() => {
      try {
        const reversed = [...messages].reverse();
        const lastUser = reversed.find(m => m && m.role === 'user' && typeof m.content === 'string');
        return lastUser ? lastUser.content : null;
      } catch (_) {
        return null;
      }
    })();
    const inferredIntent = this._inferFilesystemIntent(lastUserGoal || '');
    
    const limit = Number.isFinite(maxIterations) ? Math.max(1, maxIterations) : Infinity;
    const limitInfo = Number.isFinite(limit) ? limit : null;
    
    debugLogger.debug('AIService.MCP', 'Iniciando loop de tool calls', { maxIterations: limitInfo });
    
    while (currentToolCall && iteration < limit) {
      iteration++;
      
      debugLogger.debug('AIService.MCP', 'Iteraci√≥n de loop tool call', {
        iteration,
        maxIterations: limitInfo,
        tool: currentToolCall.toolName
      });
      
      // NEW: Detect infinite loops (same tool repeated)
      if (lastToolName === currentToolCall.toolName) {
        consecutiveRepeats++;
        debugLogger.warn('AIService.MCP', 'Mismo tool repetido consecutivamente', {
          repeticiones: consecutiveRepeats,
          tool: currentToolCall.toolName
        });
        
        // Si el mismo tool se pide 2 veces seguidas (es decir, 3 veces en total), probablemente es un loop
        if (consecutiveRepeats >= 2) {
          debugLogger.warn('AIService.MCP', 'Loop infinito detectado, deteniendo ejecuci√≥n', {
            tool: currentToolCall.toolName
          });
          if (callbacks.onStatus) {
            callbacks.onStatus({
              status: 'warning',
              message: `‚ö†Ô∏è Loop infinito detectado (${currentToolCall.toolName} repetido 3 veces)`,
              model: modelId,
              provider: 'local'
            });
          }
          
          // FIX: Return the last meaningful response instead of just breaking
          const lastMessage = conversationMessages[conversationMessages.length - 1];
          const lastContent = lastMessage?.content || '';
          
          return `‚ö†Ô∏è Se detect√≥ un loop infinito con la herramienta "${currentToolCall.toolName}". El modelo solicit√≥ esta herramienta repetidamente sin progresar.

√öltima respuesta del modelo:
${lastContent}

Por favor, intenta un enfoque diferente o simplifica tu solicitud.`;
        }
      } else {
        consecutiveRepeats = 0;
        lastToolName = currentToolCall.toolName;
      }
      
      // Callback de estado: ejecutando herramienta
      if (callbacks.onStatus) {
        callbacks.onStatus({
          status: 'tool-execution',
          message: `üîß Ejecutando herramienta: ${currentToolCall.toolName}...`,
          model: modelId,
          provider: 'local',
          toolName: currentToolCall.toolName,
          toolArgs: currentToolCall.arguments,
          iteration,
          maxIterations: limitInfo
        });
      }
      
      try {
        // Ejecutar la tool via MCP (soportar serverId y nombres namespaced)
        let execResult = null;
        let baseName = currentToolCall.toolName;
        let serverIdHint = currentToolCall.serverId || null;

        if (!serverIdHint && typeof baseName === 'string' && baseName.includes('__')) {
          const idx = baseName.indexOf('__');
          const sid = baseName.slice(0, idx);
          const name = baseName.slice(idx + 2);
          if (sid && name) {
            serverIdHint = sid;
            baseName = name;
          }
        }

        // üîß CRITICAL FIX: Asegurar que arguments siempre sea un objeto v√°lido
        if (!currentToolCall.arguments || typeof currentToolCall.arguments !== 'object') {
          currentToolCall.arguments = {};
        }

        const defaultInfoLocal = this._getMcpDefaultDir(serverIdHint || 'filesystem');
        const defaultPathLocal = defaultInfoLocal?.raw || defaultInfoLocal?.normalized || null;
        
        // ‚úÖ FIXED: Inyectar path ANTES de validar
        if (defaultPathLocal) {
          if (['list_directory', 'directory_tree', 'list_directory_with_sizes'].includes(baseName) && !currentToolCall.arguments.path) {
            currentToolCall.arguments.path = defaultPathLocal;
            debugLogger.debug('AIService.MCP', 'Path inyectado para herramienta', {
              tool: baseName,
              path: defaultPathLocal
            });
          }
          if (baseName === 'read_text_file' && !currentToolCall.arguments.path) {
            currentToolCall.arguments.path = defaultPathLocal;
            debugLogger.debug('AIService.MCP', 'Path inyectado para herramienta', {
              tool: baseName,
              path: defaultPathLocal
            });
          }
        }

        // üîç DEBUG: Validar argumentos antes de ejecutar
        debugLogger.debug('AIService.MCP', 'Ejecutando herramienta', {
          tool: baseName,
          args: currentToolCall.arguments
        });
        
        if (!currentToolCall.arguments || Object.keys(currentToolCall.arguments).length === 0) {
          debugLogger.warn('AIService.MCP', 'Argumentos vac√≠os para herramienta, puede fallar', {
            tool: baseName
          });
        }

        if (serverIdHint) {
          execResult = await mcpClient.callTool(serverIdHint, baseName, currentToolCall.arguments);
        } else {
          execResult = await mcpClient.callTool(baseName, currentToolCall.arguments);
        }
        const result = (execResult && execResult.success === true && execResult.result) ? execResult.result : execResult;
        
        // Verificar si hubo error en la tool
        if (result.isError) {
          const errorText = result.content?.[0]?.text || 'Error desconocido';
          console.error(`‚ùå [MCP] ${currentToolCall.toolName} fall√≥:`, errorText);
          
          // ‚úÖ CR√çTICO: Guardar error como tool result con formato correcto
          const formattedErrorText = `‚ùå Error en ${currentToolCall.toolName}: ${errorText}`;
          if (callbacks && typeof callbacks.onToolResult === 'function') {
            try {
              callbacks.onToolResult({
                toolName: currentToolCall.toolName,
                args: currentToolCall.arguments,
                result: { isError: true, error: errorText, content: [{ text: formattedErrorText }] },
                error: true
              });
            } catch (cbErr) {
              debugLogger.warn('AIService.MCP', 'onToolResult callback lanz√≥ un error al reportar error', {
                error: cbErr?.message
              });
            }
          }
          
          // Callback de error
          if (callbacks.onStatus) {
            callbacks.onStatus({
              status: 'tool-error',
              message: `Error en herramienta ${currentToolCall.toolName}: ${errorText}`,
              model: modelId,
              provider: 'local',
              toolName: currentToolCall.toolName,
              error: errorText
            });
          }
          
          // No devolver inmediatamente, informar al modelo
          conversationMessages.push({
            role: 'user',
            content: `‚ùå Error ejecutando ${currentToolCall.toolName}: ${errorText}`
          });
          
          // Pedir al modelo que intente de otra forma
          const errorFollowUp = await this.sendToLocalModelStreamingWithCallbacks(
            modelId,
            conversationMessages,
            callbacks,
            { ...options, maxTokens: 500, temperature: 0.3 }
          );
          
          // Detectar si hay otra tool call despu√©s del error
          currentToolCall = this.detectToolCallInResponse(errorFollowUp);
          if (!currentToolCall) {
            return errorFollowUp;
          }
          continue;
        }
        
        debugLogger.debug('AIService.MCP', 'Ejecuci√≥n de herramienta completada', {
          tool: currentToolCall.toolName
        });
        
        // ‚úÖ IMPROVED: Detectar lenguaje para archivos de texto
        let detectedLanguage = '';
        if (baseName === 'read_text_file') {
          const filePath = currentToolCall.arguments?.path || '';
          const ext = filePath.split('.').pop()?.toLowerCase() || '';
          const langMap = {
            'js': 'javascript', 'jsx': 'javascript', 'ts': 'typescript', 'tsx': 'typescript',
            'py': 'python', 'java': 'java', 'cpp': 'cpp', 'c': 'c', 'h': 'c', 'hpp': 'cpp',
            'cs': 'csharp', 'php': 'php', 'rb': 'ruby', 'go': 'go', 'rs': 'rust', 'swift': 'swift',
            'kt': 'kotlin', 'scala': 'scala', 'sh': 'bash', 'bash': 'bash', 'zsh': 'bash', 'fish': 'bash',
            'ps1': 'powershell', 'json': 'json', 'yaml': 'yaml', 'yml': 'yaml', 'xml': 'xml',
            'html': 'html', 'htm': 'html', 'css': 'css', 'scss': 'scss', 'sass': 'sass', 'less': 'less',
            'sql': 'sql', 'md': 'markdown', 'mdx': 'markdown', 'txt': 'text', 'log': 'text'
          };
          detectedLanguage = langMap[ext] || '';
        }
        
        // Notificar a la UI con el resultado de la tool
        if (callbacks && typeof callbacks.onToolResult === 'function') {
          try {
            callbacks.onToolResult({
              toolName: currentToolCall.toolName,
              args: currentToolCall.arguments,
              result,
              detectedLanguage,
              filePath: currentToolCall.arguments?.path
            });
          } catch (cbErr) {
            debugLogger.warn('AIService.MCP', 'onToolResult callback lanz√≥ un error', {
              error: cbErr?.message
            });
          }
        }
        
        // Formatear resultado
        const cleanResult = (() => {
          const text = result.content?.[0]?.text || 'OK';
          
          // Operaciones de escritura/modificaci√≥n
          if (text.includes('Successfully wrote') || text.includes('Successfully created')) {
            return '‚úÖ Archivo creado correctamente';
          }
          if (text.includes('Successfully moved')) {
            return '‚úÖ Archivo movido correctamente';
          }
          if (text.includes('DIFF INDEX') || text.includes('---') || text.includes('```diff')) {
            return '‚úÖ Archivo editado correctamente';
          }
          
          // ‚úÖ NO procesar aqu√≠ - dejar para AIChatPanel.js
          // Solo devolver el texto sin formateo para que renderMarkdown lo procese
          if (text.includes('[FILE]') || text.includes('[DIR]')) {
            return text;
          }
          
          // ‚úÖ IMPROVED: Contenido de archivos - detectar lenguaje y formatear como c√≥digo
          // Detectar extensi√≥n si est√° disponible en los metadatos o por patrones
          if (currentToolCall.toolName === 'read_text_file' || currentToolCall.toolName === 'read_file' || baseName === 'read_text_file') {
            const filePath = currentToolCall.arguments?.path || '';
            const ext = filePath.split('.').pop()?.toLowerCase() || '';
            
            // Map extensiones a lenguajes soportados en markdown
            const langMap = {
              'js': 'javascript',
              'jsx': 'javascript',
              'ts': 'typescript',
              'tsx': 'typescript',
              'py': 'python',
              'java': 'java',
              'cpp': 'cpp',
              'c': 'c',
              'h': 'c',
              'hpp': 'cpp',
              'cs': 'csharp',
              'php': 'php',
              'rb': 'ruby',
              'go': 'go',
              'rs': 'rust',
              'swift': 'swift',
              'kt': 'kotlin',
              'scala': 'scala',
              'sh': 'bash',
              'bash': 'bash',
              'zsh': 'bash',
              'fish': 'bash',
              'ps1': 'powershell',
              'json': 'json',
              'yaml': 'yaml',
              'yml': 'yaml',
              'xml': 'xml',
              'html': 'html',
              'htm': 'html',
              'css': 'css',
              'scss': 'scss',
              'sass': 'sass',
              'less': 'less',
              'sql': 'sql',
              'md': 'markdown',
              'mdx': 'markdown',
              'txt': 'text',
              'log': 'text'
            };
            
            const lang = langMap[ext] || '';
            debugLogger.debug('AIService.MCP', 'Lenguaje detectado para archivo', {
              extension: ext,
              lenguaje: lang
            });
            
            // No a√±adir bloques de c√≥digo aqu√≠ - se manejan en AIChatPanel.js
            return text;
          }
          
          // Texto general o resultado de otros comandos
          return text;
        })();
        
        // Marcar si el filesystem fue modificado
        const finalOperations = ['write_file', 'edit_file', 'create_directory', 'move_file'];
        if (finalOperations.includes(currentToolCall.toolName)) {
          this._filesystemModified = true;
        }
        
        // NUEVO: Re-inyectar resultado en conversaci√≥n para que el modelo lo vea
        debugLogger.debug('AIService.MCP', 'Reinyectando resultado en conversaci√≥n');
        const { observation: toolObservation, summary: toolSummary } = this._buildToolObservation({
          toolName: currentToolCall.toolName,
          args: currentToolCall.arguments,
          resultText: cleanResult,
          isError: !!result.isError,
          lastUserGoal,
          inferredIntent
        });
        conversationMessages.push({
          role: 'user',
          content: toolObservation,
          metadata: { isToolObservation: true, toolName: currentToolCall.toolName }
        });
        
        // Preguntar al modelo si necesita m√°s herramientas
        // Aumentar maxTokens para dar espacio a tool calls encadenados
        const followUp = await this.sendToLocalModelStreamingWithCallbacks(
          modelId,
          conversationMessages,
          callbacks,
          { ...options, maxTokens: Math.max(800, options.maxTokens || 500), temperature: 0.3, contextLimit: Math.min(4096, options.contextLimit || 8000) }
        );
        
        // üîß NUEVO: Si la respuesta est√° vac√≠a, reintentar con prompt simplificado
        if (!followUp || followUp.trim().length === 0) {
          debugLogger.warn('AIService.MCP', 'Modelo gener√≥ respuesta vac√≠a tras ejecutar tool; reintentando con prompt simplificado');
          
          conversationMessages.push({
            role: 'user',
            content: `Por favor, responde confirmando que la operaci√≥n se complet√≥ exitosamente o genera el SIGUIENTE tool-call necesario para cumplir el objetivo. ${lastUserGoal ? `Objetivo: ${lastUserGoal}. ` : ''}Recuerda responder s√≥lo con JSON v√°lido cuando uses herramientas.`
          });
          
          const retryResponse = await this.sendToLocalModelStreamingWithCallbacks(
            modelId,
            conversationMessages,
            callbacks,
            { ...options, maxTokens: 1500, temperature: 0.6, contextLimit: Math.min(4096, options.contextLimit || 8000) }
          );
          
          if (retryResponse && retryResponse.trim().length > 0) {
            debugLogger.debug('AIService.MCP', 'Retry exitoso tras respuesta vac√≠a');
            return retryResponse;
          } else {
            debugLogger.warn('AIService.MCP', 'Retry fall√≥ despu√©s de respuesta vac√≠a, retornando mensaje por defecto');
            return `‚úÖ Operaci√≥n completada correctamente.`;
          }
        }
        
        // NUEVO: Detectar si hay otro tool call
        const nextToolCall = this.detectToolCallInResponse(followUp);
        
        // üîß CR√çTICO: Ignorar tool call si es ID√âNTICO al que acabamos de ejecutar
        // Esto previene loops infinitos cuando el modelo menciona la herramienta anterior
        if (nextToolCall) {
          const isSameTool = nextToolCall.toolName === currentToolCall.toolName;
          const isSameArgs = JSON.stringify(nextToolCall.arguments) === JSON.stringify(currentToolCall.arguments);
          
          if (isSameTool && isSameArgs) {
            debugLogger.warn('AIService.MCP', 'Tool call duplicado detectado; terminando loop', {
              tool: nextToolCall.toolName
            });
            // Retornar la respuesta sin el JSON del tool call
            const cleanResponse = followUp.replace(/\{[\s\S]*?"tool"[\s\S]*?\}/g, '').trim();
            return cleanResponse || `‚úÖ Operaci√≥n completada correctamente.`;
          }
        }
        
        currentToolCall = nextToolCall;
        
        if (!currentToolCall) {
          // No hay m√°s tools, el modelo respondi√≥ normalmente
          debugLogger.debug('AIService.MCP', 'Loop completado, el modelo respondi√≥ sin pedir m√°s herramientas');
          
          return followUp;
        }
        
        // Hay otro tool call DIFERENTE, continuar loop
        debugLogger.debug('AIService.MCP', 'Modelo solicita otra herramienta, continuando loop');
        
      } catch (error) {
        debugLogger.error('AIService.MCP', 'Error ejecutando herramienta', {
          tool: currentToolCall.toolName,
          error: error?.message
        });
        
        // Callback de error
        if (callbacks.onStatus) {
          callbacks.onStatus({
            status: 'tool-error',
            message: `Error en herramienta ${currentToolCall.toolName}: ${error.message}`,
            model: modelId,
            provider: 'local',
            toolName: currentToolCall.toolName,
            error: error.message
          });
        }
        
        // Informar error al modelo
        conversationMessages.push({
          role: 'user',
          content: `‚ùå Error t√©cnico ejecutando ${currentToolCall.toolName}: ${error.message}`
        });
        
        // Dar oportunidad al modelo de responder
        try {
          const errorResponse = await this.sendToLocalModelStreamingWithCallbacks(
            modelId,
            conversationMessages,
            callbacks,
            { ...options, maxTokens: 500, temperature: 0.3, contextLimit: Math.min(2048, options.contextLimit || 8000) }
          );
          return errorResponse;
        } catch (recoveryError) {
          throw new Error(`Error ejecutando herramienta ${currentToolCall.toolName}: ${error.message}`);
        }
      }
    }
    
    const limitReached = Number.isFinite(limit) && iteration >= limit && currentToolCall;
    if (limitReached) {
      debugLogger.warn('AIService.MCP', 'L√≠mite de iteraciones alcanzado en loop de herramientas (local)', {
        maxIterations: limit
      });
      
      if (callbacks.onStatus) {
        callbacks.onStatus({
          status: 'warning',
          message: `L√≠mite de herramientas alcanzado (${limit} iteraciones)`,
          model: modelId,
          provider: 'local'
        });
      }
      
      if (conversationMessages.length > 0) {
        const lastMessage = conversationMessages[conversationMessages.length - 1];
        if (lastMessage.content) {
          return lastMessage.content;
        }
      }
      
      return 'Lo siento, alcanc√© el l√≠mite de uso de herramientas configurado para este modelo.';
    }
    
    return 'Operaci√≥n completada.';
  }

  /**
   * Enviar mensaje con callbacks de estado
   */
  async sendMessageWithCallbacks(message, callbacks = {}, options = {}) {
    if (!this.currentModel) {
      throw new Error('No se ha seleccionado ning√∫n modelo');
    }

    // Obtener configuraci√≥n de rendimiento autom√°tica
    const perfConfig = this.getModelPerformanceConfig(this.currentModel, this.modelType);
    
    // Combinar opciones con configuraci√≥n autom√°tica
    const finalOptions = {
      ...perfConfig,
      ...options
    };

    // Obtener historial de la conversaci√≥n actual desde ConversationService
    const currentConversation = conversationService.getCurrentConversation();
    if (!currentConversation) {
      throw new Error('No hay conversaci√≥n activa');
    }

    // üîß VALIDACI√ìN DE SINCRONIZACI√ìN
    debugLogger.debug('AIService.Conversation', 'Validaci√≥n de conversaci√≥n', {
      currentId: currentConversation.id,
      serviceId: conversationService.currentConversationId
    });
    if (currentConversation.id !== conversationService.currentConversationId) {
      debugLogger.warn('AIService.Conversation', 'Desincronizaci√≥n detectada', {
        currentId: currentConversation.id,
        serviceId: conversationService.currentConversationId
      });
    }

    // Obtener mensajes de la conversaci√≥n actual
    const conversationMessages = currentConversation.messages || [];
    // Considerar "primera conversaci√≥n" cuando solo hay 1 mensaje (el del usuario que acabamos de agregar)
    const isFirstMessage = conversationMessages.length === 1;
    
    debugLogger.debug('AIService.Conversation', 'Mensajes en conversaci√≥n actual', {
      total: conversationMessages.length
    });
    
    // ü™ü VENTANA DESLIZANTE INTELIGENTE POR TOKENS (como ChatGPT/Claude)
    const contextualMessages = this._prepareMessagesForContext(conversationMessages);
    let limitedMessages = this.smartTokenBasedHistoryLimit(contextualMessages, finalOptions);
    debugLogger.debug('AIService.Conversation', 'Mensajes despu√©s de limitaci√≥n', {
      total: limitedMessages.length
    });

    // Construir contexto ef√≠mero de archivos adjuntos (RAG ligero)
    const attachedFiles = conversationService.getAttachedFiles();
    const ephemeralContext = fileAnalysisService.buildEphemeralContext(attachedFiles, message, {
      maxChars: Math.min(3000, (finalOptions.contextLimit || 8000) / 2)
    });

    // Mensajes a enviar al proveedor (no se guardan como historial visible)
    const providerMessages = [...limitedMessages];
    
    // Si el √∫ltimo mensaje es del usuario, reemplazarlo para evitar duplicados
    if (providerMessages.length > 0 && providerMessages[providerMessages.length - 1].role === 'user') {
      providerMessages[providerMessages.length - 1] = { role: 'user', content: message };
    } else {
      // Si no hay mensaje del usuario al final, agregarlo
      providerMessages.push({ role: 'user', content: message });
    }
    
    // Contexto ef√≠mero de archivos adjuntos (debe ir antes del mensaje del usuario)
    // Insertar antes del √∫ltimo mensaje (que es el del usuario)
    if (ephemeralContext && ephemeralContext.length > 0) {
      providerMessages.splice(providerMessages.length - 1, 0, { role: 'system', content: ephemeralContext });
    }

    // Si el filesystem fue modificado, agregar nota para invalidar informaci√≥n anterior
    if (this._filesystemModified) {
      providerMessages.splice(providerMessages.length - 1, 0, {
        role: 'system',
        content: '‚ö†Ô∏è FILESYSTEM MODIFICADO. Archivos/directorios anteriores ya NO son v√°lidos. DEBES ejecutar tools de nuevo para obtener informaci√≥n actualizada.'
      });
      this._filesystemModified = false; // Reset flag
    }

    // Log compacto
    debugLogger.debug('AIService.Conversation', 'Enviando mensajes al modelo', {
      mensajes: providerMessages.length
    });

    // Metadatos para la UI: indicar si se us√≥ contexto ef√≠mero y qu√© archivos
    const ephemeralFilesUsed = (ephemeralContext && ephemeralContext.length > 0)
      ? (attachedFiles || []).map(f => f.name)
      : [];

    const startTime = Date.now();
    
    try {
      let response;
      
      // Callback de inicio
      if (callbacks.onStart) {
        callbacks.onStart({
          model: this.currentModel,
          modelType: this.modelType,
          message: message,
          ephemeralContextUsed: ephemeralContext && ephemeralContext.length > 0,
          ephemeralFilesUsed
        });
      }

      if (this.modelType === 'remote') {
        response = await this.sendToRemoteModelWithCallbacks(message, providerMessages, callbacks, finalOptions);
      } else {
        response = await this.sendToLocalModelWithCallbacks(message, providerMessages, callbacks, finalOptions);
      }

      const endTime = Date.now();
      const latency = endTime - startTime;

      // Callback de finalizaci√≥n
      if (callbacks.onComplete) {
        callbacks.onComplete({
          response,
          latency,
          model: this.currentModel,
          modelType: this.modelType,
          ephemeralContextUsed: ephemeralContext && ephemeralContext.length > 0,
          ephemeralFilesUsed
        });
      }

      return response;
    } catch (error) {
      const endTime = Date.now();
      const latency = endTime - startTime;
      
      // Callback de error
      if (callbacks.onError) {
        callbacks.onError({
          error,
          latency,
          model: this.currentModel,
          modelType: this.modelType,
          ephemeralContextUsed: ephemeralContext && ephemeralContext.length > 0,
          ephemeralFilesUsed
        });
      }
      
      console.error('Error enviando mensaje a IA:', error);
      throw error;
    }
  }

  /**
   * Validar y leer JSON de forma segura con l√≠mites de tama√±o
   * Evita que respuestas muy largas causen crash de memoria
   */
  async _safeReadJSON(response, modelId) {
    // L√≠mite de seguridad: 10MB para respuesta JSON
    const MAX_RESPONSE_SIZE = 10 * 1024 * 1024; // 10MB
    
    try {
      // Verificar Content-Length si est√° disponible
      const contentLength = response.headers.get('content-length');
      if (contentLength) {
        const size = parseInt(contentLength, 10);
        if (size > MAX_RESPONSE_SIZE) {
          debugLogger.warn('AIService.RemoteModel', 'Respuesta excede l√≠mite de tama√±o', {
            modelId,
            contentLength: size,
            maxAllowed: MAX_RESPONSE_SIZE
          });
          throw new Error(`La respuesta del modelo es demasiado grande (${Math.round(size / 1024 / 1024)}MB). Intenta con una pregunta m√°s espec√≠fica.`);
        }
      }
      
      // Leer respuesta con timeout de 30 segundos
      const timeoutPromise = new Promise((_, reject) => 
        setTimeout(() => reject(new Error('Timeout al procesar respuesta del modelo (30s)')), 30000)
      );
      
      const data = await Promise.race([
        response.json(),
        timeoutPromise
      ]);
      
      return data;
      
    } catch (error) {
      // Capturar errores de memoria espec√≠ficos
      if (error.message.includes('out of memory') || 
          error.message.includes('allocation failed') ||
          error.message.includes('JavaScript heap')) {
        debugLogger.error('AIService.RemoteModel', 'Error de memoria al leer JSON', {
          modelId,
          error: error.message
        });
        throw new Error('La respuesta del modelo es demasiado grande y caus√≥ un error de memoria. Por favor, intenta con una pregunta m√°s espec√≠fica o divide tu solicitud en partes m√°s peque√±as.');
      }
      
      throw error;
    }
  }

  /**
   * Procesar respuesta de forma segura con l√≠mites de tama√±o
   * Evita que respuestas muy largas causen crash de memoria
   */
  async _safeProcessResponse(response, modelProvider, modelId) {
    const MAX_CONTENT_LENGTH = 500000; // ~500k caracteres para el contenido de texto
    
    try {
      // Leer JSON de forma segura
      const data = await this._safeReadJSON(response, modelId);
      
      // Extraer contenido seg√∫n el proveedor
      let content = '';
      if (modelProvider === 'openai') {
        content = data.choices?.[0]?.message?.content || '';
      } else if (modelProvider === 'anthropic') {
        content = data.content?.[0]?.text || '';
      } else if (modelProvider === 'google') {
        content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
      }
      
      // Verificar tama√±o del contenido extra√≠do
      if (content.length > MAX_CONTENT_LENGTH) {
        debugLogger.warn('AIService.RemoteModel', 'Contenido de respuesta muy largo, truncando', {
          modelId,
          originalLength: content.length,
          maxAllowed: MAX_CONTENT_LENGTH
        });
        
        // Truncar de forma inteligente en el √∫ltimo punto o salto de l√≠nea antes del l√≠mite
        let truncated = content.substring(0, MAX_CONTENT_LENGTH);
        const lastPeriod = truncated.lastIndexOf('.');
        const lastNewline = truncated.lastIndexOf('\n');
        const cutPoint = Math.max(lastPeriod, lastNewline);
        
        if (cutPoint > MAX_CONTENT_LENGTH * 0.8) { // Si el punto est√° en el √∫ltimo 20%
          truncated = truncated.substring(0, cutPoint + 1);
        }
        
        content = truncated + '\n\n[‚ö†Ô∏è Respuesta truncada por exceder el l√≠mite de tama√±o. La respuesta original era muy larga.]';
      }
      
      return content;
      
    } catch (error) {
      // Propagar errores ya formateados
      throw error;
    }
  }
  
  /**
   * Truncar contenido de texto si excede l√≠mites
   */
  _truncateContent(content, maxLength = 500000) {
    if (!content || content.length <= maxLength) {
      return content;
    }
    
    debugLogger.warn('AIService.RemoteModel', 'Truncando contenido largo', {
      originalLength: content.length,
      maxAllowed: maxLength
    });
    
    // Truncar de forma inteligente
    let truncated = content.substring(0, maxLength);
    const lastPeriod = truncated.lastIndexOf('.');
    const lastNewline = truncated.lastIndexOf('\n');
    const cutPoint = Math.max(lastPeriod, lastNewline);
    
    if (cutPoint > maxLength * 0.8) {
      truncated = truncated.substring(0, cutPoint + 1);
    }
    
    return truncated + '\n\n[‚ö†Ô∏è Respuesta truncada por exceder el l√≠mite de tama√±o. La respuesta original era muy larga.]';
  }

  /**
   * Enviar mensaje a modelo remoto
   */
  async sendToRemoteModel(message, options = {}) {
    const model = this.models.remote.find(m => m.id === this.currentModel);
    if (!model) {
      throw new Error('Modelo remoto no encontrado');
    }

    const apiKey = this.getApiKey(model.provider);
    if (!apiKey) {
      throw new Error(`API Key no configurada para ${model.provider}`);
    }

    // Preparar mensajes seg√∫n el proveedor
    let requestBody;
    let headers;
    let endpointWithKey = null;

    if (model.provider === 'openai') {
      headers = {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      };
      
      requestBody = {
        model: model.id,
        messages: this.conversationHistory.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        stream: options.stream || false,
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 2000
      };
    } else if (model.provider === 'anthropic') {
      headers = {
        'Content-Type': 'application/json',
        'x-api-key': apiKey,
        'anthropic-version': '2023-06-01'
      };
      
      requestBody = {
        model: model.id,
        messages: this.conversationHistory.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        max_tokens: options.maxTokens || 2000
      };
    } else if (model.provider === 'google') {
      headers = {
        'Content-Type': 'application/json'
      };
      
      // Gemini usa un formato diferente - necesita el API key como par√°metro de query
      endpointWithKey = `${model.endpoint}?key=${apiKey}`;
      
      // Convertir historial de conversaci√≥n al formato de Gemini
      const contents = this.conversationHistory.map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }]
      }));
      
      requestBody = {
        contents: contents,
        generationConfig: {
          temperature: options.temperature || 0.7,
          maxOutputTokens: options.maxTokens || 2000
        }
      };
    }

    try {
      // Usar la URL correcta seg√∫n el proveedor
      const requestUrl = model.provider === 'google' ? endpointWithKey : model.endpoint;
      
      // Intentar con reintentos para errores 503 (modelo sobrecargado)
      let lastError;
      for (let attempt = 1; attempt <= 3; attempt++) {
        try {
          const response = await fetch(requestUrl, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestBody),
        signal: options.signal
      });

      if (!response.ok) {
        const error = await response.json();
            const errorMessage = error.error?.message || 'Error en la API';
            
            // Si es error 503 (modelo sobrecargado) y no es el √∫ltimo intento, reintentar
            if (response.status === 503 && attempt < 3) {
              debugLogger.warn('AIService.RemoteModel', 'Modelo remoto sobrecargado, reintentando', {
                modelId: model.id,
                intento: attempt,
                delayMs: attempt * 2000
              });
              await new Promise(resolve => setTimeout(resolve, attempt * 2000)); // Esperar 2, 4 segundos
              continue;
            }
            
            throw new Error(errorMessage);
      }

      // üõ°Ô∏è USAR PROCESAMIENTO SEGURO para evitar crashes de memoria con respuestas largas
      const content = await this._safeProcessResponse(response, model.provider, model.id);
      return content;
        } catch (error) {
          lastError = error;
          
          // Si es error de modelo sobrecargado y no es el √∫ltimo intento, reintentar
          if (error.message.includes('overloaded') && attempt < 3) {
            debugLogger.warn('AIService.RemoteModel', 'Modelo remoto sobrecargado (mensaje), reintentando', {
              modelId: model.id,
              intento: attempt,
              delayMs: attempt * 2000
            });
            await new Promise(resolve => setTimeout(resolve, attempt * 2000));
            continue;
          }
          
          // Si no es el √∫ltimo intento, continuar con el siguiente intento
          if (attempt < 3) {
            continue;
          }
          
          throw error;
        }
      }
      
      throw lastError;
    } catch (error) {
      debugLogger.error('AIService.RemoteModel', 'Error llamando a API remota', {
        modelId: model.id,
        error: error?.message
      });
      
      // Si es error de modelo sobrecargado, intentar con otro modelo del mismo proveedor
      if (error.message.includes('overloaded') || error.message.includes('503') || error.message.includes('The model is overloaded')) {
        debugLogger.warn('AIService.RemoteModel', 'Intentando fallback autom√°tico por modelo sobrecargado', {
          modelId: model.id
        });
        
        // Buscar otros modelos del mismo proveedor que no hayan sido intentados
        const alternativeModels = this.models.remote.filter(m => 
          m.provider === model.provider && 
          m.id !== model.id && 
          this.getApiKey(m.provider) // Que tenga API key configurada
        );
        
        if (alternativeModels.length > 0) {
          const fallbackModel = alternativeModels[0];
          debugLogger.info('AIService.RemoteModel', 'Cambiando a modelo fallback', {
            fallbackModel: fallbackModel.name
          });
          
          // Temporalmente cambiar el modelo actual para el fallback
          const originalModel = this.currentModel;
          this.currentModel = fallbackModel.id;
          
          try {
            const result = await this.sendToRemoteModel(message, options);
            
            // Restaurar modelo original
            this.currentModel = originalModel;
            return result;
          } catch (fallbackError) {
            // Restaurar modelo original
            this.currentModel = originalModel;
            console.error('Error en modelo fallback:', fallbackError);
            throw new Error(`Todos los modelos de ${model.provider} est√°n sobrecargados. Por favor, intenta m√°s tarde o cambia a otro proveedor (OpenAI, Anthropic).`);
          }
        }
      }
      
      throw error;
    }
  }

  /**
   * Enviar mensaje a modelo local
   */
  async sendToLocalModel(message, options = {}) {
    const model = this.getAllLocalModels().find(m => m.id === this.currentModel);
    if (!model) {
      throw new Error('Modelo local no encontrado');
    }

    if (!model.downloaded) {
      throw new Error('El modelo local no est√° descargado');
    }

    // Comunicaci√≥n con Ollama usando la API /api/chat
    try {
      // Preparar mensajes en el formato que espera Ollama
      const messages = this.conversationHistory.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' : 'user',
        content: msg.content
      }));

      const ollamaUrl = this.getOllamaUrl();
      
      // Usar streaming si est√° habilitado
      if (options.useStreaming) {
        return await this.sendToLocalModelStreaming(model.id, messages, options);
      } else {
        return await this.sendToLocalModelNonStreaming(model.id, messages, options);
      }
    } catch (error) {
      console.error('Error llamando a modelo local:', error);
      
      // Isajes de error m√°s espec√≠ficos
      if (error.message.includes('Failed to fetch') || error.message.includes('NetworkError')) {
        throw new Error('No se pudo conectar con Ollama. Verifica que est√© ejecut√°ndose en http://localhost:11434');
      } else if (error.message.includes('404')) {
        throw new Error('Modelo no encontrado en Ollama. Verifica que el modelo est√© descargado correctamente.');
      } else {
        throw error;
      }
    }
  }

  /**
   * Enviar mensaje a modelo remoto con callbacks
   */
  async sendToRemoteModelWithCallbacks(message, conversationMessages, callbacks = {}, options = {}) {
    const model = this.models.remote.find(m => m.id === this.currentModel);
    if (!model) {
      throw new Error('Modelo remoto no encontrado');
    }

    const apiKey = this.getApiKey(model.provider);
    if (!apiKey) {
      throw new Error(`API Key no configurada para ${model.provider}`);
    }

    // Callback de estado: conectando
    if (callbacks.onStatus) {
      callbacks.onStatus({
        status: 'connecting',
        message: `Conectando con ${model.name}...`,
        model: model.name,
        provider: model.provider
      });
    }

    try {
      // üîå Inyectar tools MCP como function-calling cuando sea posible
      const mcpEnabled = options.mcpEnabled !== false;
      let mcpContext = { tools: [], hasTools: false };
      if (mcpEnabled) {
        try {
          const ctx = await this.injectMCPContext(message);
          mcpContext = { tools: ctx.tools || [], hasTools: (ctx.tools || []).length > 0 };
        } catch (e) {
          debugLogger.warn('AIService.MCP', 'Error obteniendo contexto MCP (remote)', {
            error: e.message
          });
        }
      }
      // Preparar mensajes seg√∫n el proveedor
      let requestBody;
      let headers;
      let endpointWithKey = null;

      if (model.provider === 'openai') {
        headers = {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        };
        
        requestBody = {
          model: model.id,
          messages: conversationMessages.map(msg => ({
            role: msg.role,
            content: msg.content
          })),
          stream: false,
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 2000
        };
        if (mcpContext.hasTools) {
          const providerTools = this.convertMCPToolsToProviderFormat(mcpContext.tools, 'openai', { namespace: true });
          if (providerTools.length > 0) {
            requestBody.tools = providerTools;
            
            // ‚ú® MEJORADO: Configuraci√≥n inteligente de tool_choice
            // Detectar si la pregunta REQUIERE usar una tool
            const lowerMsg = message.toLowerCase();
            const requiresTool = lowerMsg.match(/lista|lee|crea|busca|muestra|guarda|edita/) && 
                                 lowerMsg.match(/archivo|directorio|carpeta|file/);
            
            if (requiresTool && providerTools.length <= 3) {
              // Si claramente necesita una tool y hay pocas opciones, forzar uso
              requestBody.tool_choice = 'required';
            } else {
              requestBody.tool_choice = 'auto';
            }
            
            // ‚ú® Ajustar temperatura para mejor precisi√≥n con tools
            requestBody.temperature = Math.min(options.temperature || 0.7, 0.3);
            
            // ‚ú® NUEVO: Agregar few-shot examples como mensaje de sistema
            const examples = this.generateToolExamples(mcpContext.tools, 'openai');
            if (examples && examples.length > 0) {
              // Buscar si ya hay un mensaje de sistema
              const systemMsgIndex = requestBody.messages.findIndex(m => m.role === 'system');
              if (systemMsgIndex >= 0) {
                // Agregar ejemplos al mensaje de sistema existente
                requestBody.messages[systemMsgIndex].content += '\n\n' + examples;
              } else {
                // Crear nuevo mensaje de sistema con ejemplos
                requestBody.messages.unshift({
                  role: 'system',
                  content: 'Usa las herramientas disponibles cuando sea necesario.' + examples
                });
              }
            }
          }
        }
      } else if (model.provider === 'anthropic') {
        headers = {
          'Content-Type': 'application/json',
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01'
        };
        
        requestBody = {
          model: model.id,
          messages: conversationMessages.map(msg => ({
            role: msg.role,
            content: msg.content
          })),
          max_tokens: options.maxTokens || 2000
        };
        if (mcpContext.hasTools) {
          const providerTools = this.convertMCPToolsToProviderFormat(mcpContext.tools, 'anthropic', { namespace: true });
          if (providerTools.length > 0) {
            requestBody.tools = providerTools;
            
            // ‚ú® NUEVO: Agregar few-shot examples como system prompt
            const examples = this.generateToolExamples(mcpContext.tools, 'anthropic');
            if (examples && examples.length > 0) {
              // Anthropic usa un campo 'system' separado
              requestBody.system = 'Usa las herramientas disponibles cuando sea necesario.' + examples;
            }
          }
        }
      } else if (model.provider === 'google') {
        headers = {
          'Content-Type': 'application/json'
        };
        
        endpointWithKey = `${model.endpoint}?key=${apiKey}`;
        
        const contents = conversationMessages.map(msg => ({
          role: msg.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: msg.content }]
        }));
        
        requestBody = {
          contents: contents,
          generationConfig: {
            temperature: options.temperature || 0.7,
            maxOutputTokens: options.maxTokens || 2000
          }
        };
        if (mcpContext.hasTools) {
          const providerTools = this.convertMCPToolsToProviderFormat(mcpContext.tools, 'google', { namespace: true }) || [];
          if (providerTools.length > 0) {
            // Gemini espera tools: [{ function_declarations: [ { name, description, parameters } ] }]
            const functionDecls = providerTools.map(fn => ({
              name: fn.name,
              description: fn.description,
              parameters: this._toGeminiSchema(fn.parameters)
            }));
            requestBody.tools = [{ function_declarations: functionDecls }];
            
            // ‚ú® MEJORADO: Configuraci√≥n inteligente del modo de function calling
            const lowerMsg = message.toLowerCase();
            const requiresTool = lowerMsg.match(/lista|lee|crea|busca|muestra|guarda|edita/) && 
                                 lowerMsg.match(/archivo|directorio|carpeta|file/);
            
            // Gemini acepta: AUTO, ANY (required), NONE
            const mode = (requiresTool && functionDecls.length <= 3) ? 'ANY' : 'AUTO';
            requestBody.tool_config = { function_calling_config: { mode } };

            // Inyectar prompt universal como systemInstruction con hints (filesystem)
            try {
              const serverHints = {};
              const hasFilesystem = (mcpContext.tools || []).some(t => t.serverId === 'filesystem');
              if (hasFilesystem) {
                const allowedDirsText = await this.getAllowedDirectoriesCached();
                if (allowedDirsText) {
                  const rawLines = String(allowedDirsText).split('\n').map(l => l.trim()).filter(Boolean);
                  let first = rawLines[0] || '';
                  if (/^Allowed directories:/i.test(first)) {
                    first = first.replace(/^Allowed directories:/i, '').trim();
                  }
                  const primaryDirNormalized = first ? first.replace(/\\/g, '/') : null;
                  if (first) {
                    this._setMcpDefaultDir('filesystem', first);
                  }
                  serverHints['filesystem'] = { allowedDirsText, primaryDirNormalized, defaultRaw: first || null };
                }
              }
              const toolsPrompt = this.generateUniversalMCPSystemPrompt(mcpContext.tools, { maxPerServer: 6, serverHints });
              
              // ‚ú® NUEVO: Agregar few-shot examples para mejorar precisi√≥n
              const examples = this.generateToolExamples(mcpContext.tools, 'google');
              const enhancedPrompt = toolsPrompt + examples;
              
              requestBody.systemInstruction = { role: 'system', parts: [{ text: enhancedPrompt }] };
            } catch (_) {}
          }
        }
      }

      // Callback de estado: generando
      if (callbacks.onStatus) {
        callbacks.onStatus({
          status: 'generating',
          message: `Generando respuesta con ${model.name}...`,
          model: model.name,
          provider: model.provider
        });
      }

      // Usar la URL correcta seg√∫n el proveedor
      const requestUrl = model.provider === 'google' ? endpointWithKey : model.endpoint;
      
      // Intentar con reintentos para errores 503
      let lastError;
      for (let attempt = 1; attempt <= 3; attempt++) {
        try {
          // Callback de reintento si no es el primer intento
          if (attempt > 1 && callbacks.onStatus) {
            callbacks.onStatus({
              status: 'retrying',
              message: `Reintentando con ${model.name}... (${attempt}/3)`,
              model: model.name,
              provider: model.provider,
              attempt
            });
          }

          const response = await fetch(requestUrl, {
            method: 'POST',
            headers: headers,
            body: JSON.stringify(requestBody),
            signal: options.signal
          });

          if (!response.ok) {
            const error = await response.json();
            const errorMessage = error.error?.message || 'Error en la API';
            
            if (response.status === 503 && attempt < 3) {
              await new Promise(resolve => setTimeout(resolve, attempt * 2000));
              continue;
            }
            
            throw new Error(errorMessage);
          }

          // üõ°Ô∏è LEER JSON DE FORMA SEGURA para evitar crashes de memoria
          const data = await this._safeReadJSON(response, model.id);
          
          // Extraer respuesta y manejar tool-calls cuando aplique
          if (model.provider === 'openai') {
            const choice = data.choices?.[0]?.message || {};
            const toolCalls = choice.tool_calls || [];
            if (mcpContext.hasTools && Array.isArray(toolCalls) && toolCalls.length > 0) {
              // Ejecutar tools solicitadas y re-preguntar
              let followMessages = [...requestBody.messages];
              const executionSummaries = [];
              for (const tc of toolCalls) {
                const fn = tc.function || {};
                const fullName = fn.name || '';
                let args = {};
                try { args = fn.arguments ? JSON.parse(fn.arguments) : fn.arguments || {}; } catch (_) { args = fn.arguments || {}; }
                const normalized = this._normalizeFunctionCall(fullName, args);
                const serverId = normalized.serverId;
                const toolName = normalized.toolName;
                const callArgs = normalized.arguments;
                
                // üîí DEBUG: Validar que toolName y serverId est√©n definidos
                if (!toolName || toolName === 'undefined') {
                  console.error(`‚ùå [AIService] ERROR: toolName es undefined despu√©s de normalizar`, {
                    fullName,
                    args,
                    normalized
                  });
                  executionSummaries.push(`‚Ä¢ ERROR: Nombre de herramienta inv√°lido (undefined)`);
                  continue;
                }
                
                if (!serverId) {
                  console.error(`‚ùå [AIService] ERROR: serverId es undefined despu√©s de normalizar`, {
                    fullName,
                    args,
                    normalized
                  });
                  executionSummaries.push(`‚Ä¢ ERROR: Servidor MCP no identificado para herramienta ${toolName}`);
                  continue;
                }
                
                const cachedExecution = getRecentToolExecution(conversationService.currentConversationId, toolName, callArgs);
                if (cachedExecution && !cachedExecution.isError) {
                  executionSummaries.push(`‚Ä¢ ${toolName}: ${cachedExecution.summary || cachedExecution.rawText}`);
                  continue;
                }
                
                let result;
                try {
                  result = serverId
                    ? await mcpClient.callTool(serverId, toolName, callArgs)
                    : await mcpClient.callTool(toolName, callArgs);
                  const text = result?.content?.[0]?.text || 'OK';
                  rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                    summary: summarizeToolResult({
                      toolName,
                      args: callArgs,
                      resultText: text
                    }),
                    rawText: text,
                    isError: false
                  });
                  executionSummaries.push(`‚Ä¢ ${toolName}: ${text.substring(0, 800)}`);
                } catch (e) {
                  rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                    summary: `ERROR ${e.message}`,
                    rawText: e.message,
                    isError: true
                  });
                  executionSummaries.push(`‚Ä¢ ${toolName}: ERROR ${e.message}`);
                }
              }
              followMessages.push({ role: 'user', content: `Resultados de herramientas:\n${executionSummaries.join('\n')}\n\nSi necesitas otra herramienta, prop√≥n el siguiente tool-call en JSON.` });
              // Segunda llamada para respuesta final (con tools a√∫n registradas)
              requestBody.messages = followMessages;
              const response2 = await fetch(requestUrl, {
                method: 'POST', headers, body: JSON.stringify(requestBody), signal: options.signal
              });
              if (!response2.ok) {
                const e2 = await response2.text();
                throw new Error(e2 || 'Error tras tool calls');
              }
              const data2 = await this._safeReadJSON(response2, model.id);
              let finalText = data2.choices?.[0]?.message?.content || '';
              finalText = this._truncateContent(finalText);
              return await this._handleRemotePostResponse(finalText, conversationMessages, mcpContext, callbacks, options, model);
            }
            let finalText = choice.content || '';
            finalText = this._truncateContent(finalText);
            return await this._handleRemotePostResponse(finalText, conversationMessages, mcpContext, callbacks, options, model);
          } else if (model.provider === 'anthropic') {
            // B√∫squeda simple de tool_use en contenido (aprox)
            const content = data.content || [];
            const toolUse = content.find(p => p.type === 'tool_use');
            if (mcpContext.hasTools && toolUse) {
              const normalized = this._normalizeFunctionCall(toolUse.name, toolUse.input || {});
              const serverId = normalized.serverId;
              const toolName = normalized.toolName;
              const callArgs = normalized.arguments;
              const cachedExecution = getRecentToolExecution(conversationService.currentConversationId, toolName, callArgs);
              
              if (cachedExecution && !cachedExecution.isError) {
                const nextBody = { ...requestBody };
                nextBody.messages = [
                  ...requestBody.messages,
                  { role: 'user', content: `Resultado de ${toolName} (reutilizado): ${cachedExecution.summary || cachedExecution.rawText}` }
                ];
                const response2 = await fetch(requestUrl, { method: 'POST', headers, body: JSON.stringify(nextBody), signal: options.signal });
                if (!response2.ok) {
                  const e2 = await response2.text();
                  throw new Error(e2 || 'Error tras tool calls (Anthropic)');
                }
                const data2 = await this._safeReadJSON(response2, model.id);
                let finalText = data2.content?.[0]?.text || '';
                finalText = this._truncateContent(finalText);
                return await this._handleRemotePostResponse(finalText, conversationMessages, mcpContext, callbacks, options, model);
              }
              
              try {
                const result = serverId ? await mcpClient.callTool(serverId, toolName, callArgs)
                                        : await mcpClient.callTool(toolName, callArgs);
                const text = result?.content?.[0]?.text || 'OK';
                rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                  summary: summarizeToolResult({
                    toolName,
                    args: callArgs,
                    resultText: text
                  }),
                  rawText: text,
                  isError: false
                });
                const nextBody = { ...requestBody };
                nextBody.messages = [
                  ...requestBody.messages,
                  { role: 'user', content: `Resultado de ${toolName}: ${text.substring(0, 1000)}` }
                ];
                const response2 = await fetch(requestUrl, { method: 'POST', headers, body: JSON.stringify(nextBody), signal: options.signal });
                if (!response2.ok) {
                  const e2 = await response2.text();
                  throw new Error(e2 || 'Error tras tool calls (Anthropic)');
                }
                const data2 = await this._safeReadJSON(response2, model.id);
                let finalText = data2.content?.[0]?.text || '';
                finalText = this._truncateContent(finalText);
                return await this._handleRemotePostResponse(finalText, conversationMessages, mcpContext, callbacks, options, model);
              } catch (e) {
                rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                  summary: `ERROR ${e.message}`,
                  rawText: e.message,
                  isError: true
                });
                return `Error ejecutando herramienta: ${e.message}`;
              }
            }
            let finalText = data.content?.[0]?.text || '';
            finalText = this._truncateContent(finalText);
            return await this._handleRemotePostResponse(finalText, conversationMessages, mcpContext, callbacks, options, model);
          } else if (model.provider === 'google') {
            // Detectar functionCall y ejecutar tools si aplica
            const candidate = (data.candidates && data.candidates[0]) || {};
            const parts = candidate.content?.parts || candidate.content || [];
            const calls = Array.isArray(parts) ? parts.filter(p => p.functionCall) : [];
            
            // üîí DEBUG: Log de function calls de Google
            if (calls.length > 0) {
              console.log(`üîß [AIService] Google function calls detectados:`, calls.length);
              calls.forEach((call, idx) => {
                console.log(`   Call ${idx}:`, JSON.stringify(call.functionCall || call, null, 2));
              });
            }
            
            if (mcpContext.hasTools && calls.length > 0) {
              // Usar toolOrchestrator para ejecutar herramientas en loop
              if (this.toolOrchestrator && calls.length === 1) {
                const firstCall = calls[0];
                const fc = firstCall.functionCall || {};
                const fullName = fc.name || '';
                
                // üîí DEBUG: Validar que fullName no est√© vac√≠o
                if (!fullName) {
                  console.error(`‚ùå [AIService] Google functionCall sin nombre:`, JSON.stringify(fc, null, 2));
                  console.error(`   firstCall completo:`, JSON.stringify(firstCall, null, 2));
                  return `Error: El modelo no proporcion√≥ un nombre de funci√≥n v√°lido. Function call recibido: ${JSON.stringify(fc)}`;
                }
                
                const normalized = this._normalizeFunctionCall(fullName, fc.args || {});
                
                // üîí DEBUG: Validar resultado de normalizaci√≥n
                if (!normalized.toolName || !normalized.serverId) {
                  console.error(`‚ùå [AIService] Error normalizando function call de Google`, {
                    fullName,
                    fcArgs: fc.args,
                    normalized
                  });
                  return `Error: No se pudo resolver la herramienta "${fullName}". Verifica que el servidor MCP est√© activo.`;
                }
                
                const initialToolCall = {
                  toolName: normalized.toolName,
                  arguments: normalized.arguments,
                  serverId: normalized.serverId
                };
                
                try {
                  const orchestratorResult = await this.toolOrchestrator.executeLoop({
                    modelId: model.id,
                    initialToolCall,
                    baseProviderMessages: conversationMessages,
                    detectToolCallInResponse: (resp) => this.detectToolCallInResponse(resp),
                    callModelFn: async (updatedMessages) => {
                      // Re-llamar a Gemini con mensajes actualizados
                      const nextBody = { ...requestBody };
                      nextBody.contents = updatedMessages.map(msg => ({
                        role: msg.role === 'assistant' ? 'model' : 'user',
                        parts: [{ text: msg.content }]
                      }));
                      const response2 = await fetch(requestUrl, { 
                        method: 'POST', 
                        headers, 
                        body: JSON.stringify(nextBody), 
                        signal: options.signal 
                      });
                      if (!response2.ok) {
                        const e2 = await response2.text();
                        throw new Error(e2 || 'Error en llamada secundaria');
                      }
                      const data2 = await response2.json();
                      const cand2 = (data2.candidates && data2.candidates[0]) || {};
                      const parts2 = cand2.content?.parts || cand2.content || [];
                      return Array.isArray(parts2) ? parts2.map(p => p.text).filter(Boolean).join('\n') : '';
                    },
                    callbacks,
                    options,
                    turnId: options?.turnId
                  });
                  return orchestratorResult;
                } catch (error) {
                  console.error('[MCP] Error en loop remoto (Gemini):', error);
                  return `Error ejecutando herramienta: ${error.message}`;
                }
              }
              
              // Fallback: ejecutar todas las tools solicitadas en secuencia
              for (const call of calls) {
                const fc = call.functionCall || {};
                const fullName = fc.name || '';
                const normalized = this._normalizeFunctionCall(fullName, fc.args || {});
                const serverId = normalized.serverId;
                const toolName = normalized.toolName;
                const callArgs = normalized.arguments;
                const cachedExecution = getRecentToolExecution(conversationService.currentConversationId, toolName, callArgs);
                
                conversationService.addMessage('assistant_tool_call', `Llamando herramienta: ${toolName}`, { 
                  isToolCall: true, 
                  toolName, 
                  toolArgs: callArgs 
                });
                
                if (cachedExecution && !cachedExecution.isError) {
                  conversationService.addMessage('tool', cachedExecution.summary || cachedExecution.rawText, { 
                    isToolResult: true, 
                    toolName, 
                    toolArgs: callArgs,
                    toolResultText: cachedExecution.rawText,
                    toolResultSummary: cachedExecution.summary || cachedExecution.rawText
                  });
                  continue;
                }
                
                try {
                  if (!serverId) {
                    throw new Error(`No se pudo resolver el servidor para la herramienta ${toolName}`);
                  }
                  
                  const result = await mcpClient.callTool(serverId, toolName, callArgs);
                  const text = result?.content?.[0]?.text || 'OK';
                  
                  conversationService.addMessage('tool', text, { 
                    isToolResult: true, 
                    toolName, 
                    toolArgs: callArgs 
                  });
                  rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                    summary: summarizeToolResult({
                      toolName,
                      args: callArgs,
                      resultText: text
                    }),
                    rawText: text,
                    isError: false
                  });
                  
                  if (callbacks.onToolResult) {
                    callbacks.onToolResult({ toolName, args: callArgs, result });
                  }
                } catch (e) {
                  const errorMsg = `‚ùå Error ejecutando herramienta ${toolName}: ${e.message}`;
                  conversationService.addMessage('tool', errorMsg, { error: true });
                  rememberToolExecution(conversationService.currentConversationId, toolName, callArgs, {
                    summary: errorMsg,
                    rawText: errorMsg,
                    isError: true
                  });
                }
              }
              
              return 'Hecho.';
            }
            const text = Array.isArray(parts) ? parts.map(p => p.text).filter(Boolean).join('\n') : '';
            return await this._handleRemotePostResponse(text || '', conversationMessages, mcpContext, callbacks, options, model);
          }
        } catch (error) {
          lastError = error;
          
          if (error.message.includes('overloaded') && attempt < 3) {
            await new Promise(resolve => setTimeout(resolve, attempt * 2000));
            continue;
          }
          
          if (attempt < 3) {
            continue;
          }
          
          throw error;
        }
      }
      
      throw lastError;
    } catch (error) {
      // Callback de error
      if (callbacks.onError) {
        callbacks.onError({
          error,
          model: model.name,
          provider: model.provider
        });
      }
      
      throw error;
    }
  }

  /**
   * Enviar mensaje a modelo local con callbacks
   */
  async sendToLocalModelWithCallbacks(message, conversationMessages, callbacks = {}, options = {}) {
    const model = this.getAllLocalModels().find(m => m.id === this.currentModel);
    if (!model) {
      throw new Error('Modelo local no encontrado');
    }

    if (!model.downloaded) {
      throw new Error('El modelo local no est√° descargado');
    }

    try {
      let messages = conversationMessages.map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' : (msg.role === 'system' ? 'system' : 'user'),
        content: msg.content
      }));

      // üîå INYECTAR TOOLS MCP EN SYSTEM PROMPT (si no est√° desactivado)
      const mcpEnabled = options.mcpEnabled !== false; // Por defecto true
      let mcpContext = { tools: [], resources: [], prompts: [], hasTools: false };
      
      if (mcpEnabled) {
        mcpContext = await this.injectMCPContext(message);
        
        if (mcpContext.hasTools) {

          // Construir hints por servidor (filesystem: directorios permitidos)
          const serverHints = {};
          try {
            const hasFilesystem = (mcpContext.tools || []).some(t => t.serverId === 'filesystem');
            if (hasFilesystem) {
              const allowedDirsText = await this.getAllowedDirectoriesCached();
              if (allowedDirsText) {
                const rawLines = String(allowedDirsText).split('\n').map(l => l.trim()).filter(Boolean);
                let first = rawLines[0] || '';
                if (/^Allowed directories:/i.test(first)) {
                  first = first.replace(/^Allowed directories:/i, '').trim();
                }
                const primaryDirNormalized = first ? first.replace(/\\/g, '/') : null;
                if (first) {
                  this._setMcpDefaultDir('filesystem', first);
                }
                serverHints['filesystem'] = {
                  allowedDirsText,
                  primaryDirNormalized,
                  defaultRaw: first || null
                };
              }
            }
          } catch (e) {
            debugLogger.warn('AIService.MCP', 'No se pudieron obtener directorios permitidos', {
              error: e.message
            });
          }

          const toolsPrompt = this.generateUniversalMCPSystemPrompt(mcpContext.tools, { maxPerServer: 6, serverHints });
          
          const systemIndex = messages.findIndex(m => m.role === 'system');
          if (systemIndex >= 0) {
            messages[systemIndex].content += (messages[systemIndex].content.endsWith('\n') ? '' : '\n\n') + toolsPrompt;
          } else {
            messages.unshift({
              role: 'system',
              content: toolsPrompt
            });
          }
        }
      }

      const ollamaUrl = this.getOllamaUrl();
      
      // Callback de estado: conectando
      if (callbacks.onStatus) {
        callbacks.onStatus({
          status: 'connecting',
          message: `Conectando con ${model.name} local...`,
          model: model.name,
          provider: 'local',
          mcpEnabled: mcpContext.hasTools
        });
      }
      
      // üîß AJUSTE INTELIGENTE DE TOKENS: El modelo necesita espacio para razonar
      const adjustedOptions = { ...options };
      
      // Calcular tama√±o aproximado del contexto actual
      const contextSize = messages.reduce((sum, m) => sum + (m.content?.length || 0), 0);
      const contextTokens = Math.ceil(contextSize / 4); // Aproximaci√≥n: 4 chars = 1 token
      
      if (mcpContext.hasTools) {
        // IMPORTANTE: deepseek-r1 es un modelo de reasoning que necesita espacio para pensar
        // 800 tokens es DEMASIADO BAJO, especialmente despu√©s de m√∫ltiples tool calls
        // Aumentar seg√∫n el tama√±o del contexto:
        const baseTokens = options.maxTokens || 2000;
        const minTokensForTools = 1500; // M√≠nimo para generar tool calls + razonamiento
        const maxTokensForTools = 3000; // M√°ximo para evitar respuestas muy largas
        
        // Si el contexto es grande (>6000 tokens), dar m√°s espacio al modelo
        if (contextTokens > 6000) {
          adjustedOptions.maxTokens = maxTokensForTools;
        } else if (contextTokens > 3000) {
          adjustedOptions.maxTokens = 2000;
        } else {
          adjustedOptions.maxTokens = Math.max(minTokensForTools, Math.min(baseTokens, maxTokensForTools));
        }
        
      }
      
      // üîç DEBUG: Mostrar qu√© se env√≠a al modelo
      if (mcpContext.hasTools) {
        const systemMsg = messages.find(m => m.role === 'system');
        if (systemMsg) {
          const promptLength = systemMsg.content.length;
          const promptPreview = systemMsg.content.substring(0, 500);
          debugLogger.debug('AIService.LocalModel', 'Prompt system enviado', {
            length: promptLength,
            tokensAprox: Math.ceil(promptLength / 4),
            preview: promptPreview + '...',
            toolsCount: mcpContext.tools.length
          });
        }
      }
      
      // Usar streaming si est√° habilitado
      let response;
      if (adjustedOptions.useStreaming) {
        response = await this.sendToLocalModelStreamingWithCallbacks(model.id, messages, callbacks, adjustedOptions);
      } else {
        response = await this.sendToLocalModelNonStreamingWithCallbacks(model.id, messages, callbacks, adjustedOptions);
      }
      
      // üîç DEBUG: Mostrar qu√© responde el modelo
      debugLogger.debug('AIService.LocalModel', 'Respuesta del modelo', {
        isEmpty: !response || response.trim().length === 0,
        length: response?.length || 0,
        preview: response ? response.substring(0, 200) : '(vac√≠o)'
      });
      
      // üîß RETRY AUTOM√ÅTICO: Si la respuesta est√° vac√≠a, reintentar con prompt simplificado
      if ((!response || response.trim().length === 0) && mcpContext.hasTools) {
        debugLogger.warn('AIService.Toolchain', 'Modelo gener√≥ respuesta vac√≠a; reintentando con prompt simplificado');
        
        // Callback de estado: reintentando
        if (callbacks.onStatus) {
          callbacks.onStatus({
            status: 'retrying',
            message: '‚ö†Ô∏è Reintentando solicitud...',
            model: model.name,
            provider: 'local'
          });
        }
        
        // Agregar prompt de ayuda
        const retryMessages = [
          ...messages,
          {
            role: 'user',
            content: 'Por favor, responde usando alguna de las herramientas disponibles o proporciona una respuesta textual.'
          }
        ];
        
        // Reintentar con par√°metros ajustados (m√°s tokens para dar espacio al modelo)
        try {
          const retryResponse = await this.sendToLocalModelStreamingWithCallbacks(
            model.id,
            retryMessages,
            callbacks,
            { ...adjustedOptions, maxTokens: 1500, temperature: 0.6 }
          );
          
          if (retryResponse && retryResponse.trim().length > 0) {
            debugLogger.debug('AIService.Toolchain', 'Retry de modelo exitoso', {
              respuestaLength: retryResponse.length
            });
            response = retryResponse;
          } else {
            debugLogger.warn('AIService.Toolchain', 'Retry fall√≥, usando respuesta por defecto');
            return 'Lo siento, tuve problemas al procesar tu solicitud. Por favor, intenta reformularla.';
          }
        } catch (retryError) {
          debugLogger.error('AIService.Toolchain', 'Error durante retry de modelo', {
            error: retryError?.message
          });
          return 'Lo siento, tuve problemas al procesar tu solicitud. Por favor, intenta de nuevo.';
        }
      }
      
      // üîß DETECTAR SI LA RESPUESTA ES UN PLAN o TOOL CALL
      if (mcpContext.hasTools) {
        // Prioridad 1: Detectar PLAN (m√∫ltiples herramientas)
        const toolPlan = this._detectToolPlan(response);
        if (toolPlan) {
          debugLogger.debug('AIService.Toolchain', 'Plan detectado; ejecutando herramientas', {
            herramientas: toolPlan.tools.length
          });
          return await this._executeToolPlan(toolPlan, callbacks, model.id);
        }
        
        // Prioridad 2: Detectar tool call individual
        const toolCall = this.detectToolCallInResponse(response);
        if (toolCall) {
          debugLogger.info('AIService.Toolchain', 'Tool call detectado; iniciando ejecuci√≥n', {
            tool: toolCall.toolName,
            structuredToolMessages: this.featureFlags?.structuredToolMessages,
            hasOrchestrator: !!this.toolOrchestrator
          });
          
          if (this.featureFlags?.structuredToolMessages && this.toolOrchestrator) {
            debugLogger.debug('AIService.Toolchain', 'Usando toolOrchestrator.executeLoop');
            const callModelFn = async (provMessages, overrides = {}) => {
              const adjusted = { ...options, ...overrides };
              return await this.sendToLocalModelStreamingWithCallbacks(
                model.id,
                provMessages,
                callbacks,
                adjusted
              );
            };
            const orchestratorResult = await this.toolOrchestrator.executeLoop({
              modelId: model.id,
              initialToolCall: toolCall,
              baseProviderMessages: messages,
              detectToolCallInResponse: (resp) => this.detectToolCallInResponse(resp),
              callModelFn,
              callbacks,
              options,
              turnId: options?.turnId
            });
            debugLogger.debug('AIService.Toolchain', 'toolOrchestrator.executeLoop completado', {
              resultadoLength: orchestratorResult?.length || 0
            });
            return orchestratorResult;
          }
          
          debugLogger.debug('AIService.Toolchain', 'Usando handleLocalToolCallLoop');
          const loopResult = await this.handleLocalToolCallLoop(toolCall, messages, callbacks, options, model.id);
          debugLogger.debug('AIService.Toolchain', 'handleLocalToolCallLoop completado', {
            resultadoLength: loopResult?.length || 0
          });
          return loopResult;
        } else {
          // Si no hay tool, intentar detectar PROMPT MCP
          const promptCall = this.detectPromptCallInResponse(response);
          if (promptCall) {
            debugLogger.debug('AIService.MCP', 'Prompt solicitado por el modelo', {
              prompt: promptCall.promptName,
              server: promptCall.serverId || 'sin server'
            });
            const promptResult = await this._handlePromptCallAndContinue(promptCall, messages, callbacks, options, model.id);
            return promptResult;
          }
          debugLogger.debug('AIService.Toolchain', 'No se detect√≥ tool/prompt call; retornando respuesta directa');
        }
      }
      
      return response;
    } catch (error) {
      console.error('Error llamando a modelo local:', error);
      
      if (error.message.includes('Failed to fetch') || error.message.includes('NetworkError')) {
        throw new Error('No se pudo conectar con Ollama. Verifica que est√© ejecut√°ndose en http://localhost:11434');
      } else if (error.message.includes('404')) {
        throw new Error('Modelo no encontrado en Ollama. Verifica que el modelo est√© descargado correctamente.');
      } else {
        throw error;
      }
    }
  }

  /**
   * Enviar mensaje a modelo local sin streaming
   */
  async sendToLocalModelNonStreaming(modelId, messages, options) {
    const ollamaUrl = this.getOllamaUrl();
    
    // Preparar opciones con configuraci√≥n (usar valores de options directamente, sin defaults hardcodeados)
    const ollamaOptions = {
      temperature: options.temperature ?? 0.7,
      num_predict: options.maxTokens ?? 4000,
      num_ctx: options.contextLimit ?? 8000,
      top_k: options.top_k ?? 40,
      top_p: options.top_p ?? 0.9,
      repeat_penalty: options.repeat_penalty ?? 1.1
    };
    
    // Preparar el body completo que se enviar√° a Ollama
    const requestBody = {
      model: modelId,
      messages: messages,
      stream: false,
      options: ollamaOptions
    };
    
    // Log compacto
    
    const response = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody),
      signal: options.signal
    });

    if (!response.ok) {
      const errorText = await response.text();
      debugLogger.error('AIService.LocalModel', 'Error de Ollama', {
        error: errorText
      });
      throw new Error(`Error del servidor Ollama (${response.status})`);
    }

    const data = await response.json();
    
    // NUEVO: Log detallado de respuesta
    debugLogger.debug('AIService.LocalModel', 'Respuesta cruda de Ollama', {
      resumida: JSON.stringify(data).substring(0, 200),
      message: data.message,
      contentLength: data.message?.content?.length || 0,
      hasReasoning: !!(data.message?.reasoning_content || data.reasoning_content)
    });
    
    // ‚úÖ CAPTURAR REASONING UNIVERSAL: Buscar reasoning en TODOS los modelos
    // Ollama puede devolver reasoning en diferentes campos seg√∫n el modelo
    const reasoningContent = data.message?.reasoning_content || 
                             data.reasoning_content || 
                             data.message?.reasoning ||
                             data.reasoning ||
                             data.message?.thinking ||
                             data.thinking ||
                             data.message?.chain_of_thought ||
                             data.chain_of_thought ||
                             null;
    
    // üîç DEBUG: Log solo para modelos reasoning conocidos o cuando se detecta reasoning
    const isReasoningModel = modelId && (modelId.includes('deepseek-r1') || modelId.includes('o1') || modelId.includes('reasoning'));
    if (isReasoningModel || reasoningContent) {
      if (data.message) {
        const messageKeys = Object.keys(data.message);
        debugLogger.debug('AIService.Reasoning', 'Respuesta completa de Ollama (non-streaming)', {
          model: modelId,
          messageKeys: messageKeys,
          hasReasoning: !!reasoningContent,
          reasoningLength: reasoningContent ? reasoningContent.length : 0,
          allKeys: Object.keys(data),
          preview: JSON.stringify(data).substring(0, 500)
        });
      }
    }
    
    // La respuesta de Ollama viene en data.message.content
    if (data.message && data.message.content) {
      // Si hay callbacks y hay reasoning, notificarlo
      if (callbacks && callbacks.onReasoning && reasoningContent) {
        callbacks.onReasoning({
          reasoning: reasoningContent,
          model: modelId,
          provider: 'local',
          isComplete: true
        });
      }
      return data.message.content;
    } else {
      debugLogger.error('AIService.LocalModel', 'Respuesta vac√≠a o inv√°lida de Ollama', {
        message: data.message,
        content: data.message?.content
      });
      throw new Error('Respuesta inv√°lida del modelo local');
    }
  }

  /**
   * Enviar mensaje a modelo local con streaming
   */
  async sendToLocalModelStreaming(modelId, messages, options) {
    const ollamaUrl = this.getOllamaUrl();
    
    // ‚úÖ NUEVO: Contexto din√°mico basado en RAM disponible
    const systemMem = this.memoryService.getSystemMemory();
    const dynamicContext = this._calcDynamicContext(systemMem.freeMB);
    
    // Preparar opciones con configuraci√≥n (usar valores de options directamente, sin defaults hardcodeados)
    const ollamaOptions = {
      temperature: options.temperature ?? 0.7,
      num_predict: options.maxTokens ?? 4000,
      num_ctx: options.contextLimit ?? dynamicContext, // ‚úÖ Din√°mico
      top_k: options.top_k ?? 40,
      top_p: options.top_p ?? 0.9,
      repeat_penalty: options.repeat_penalty ?? 1.1
    };
    
    // Preparar el body completo que se enviar√° a Ollama
    const requestBody = {
      model: modelId,
      messages: messages,
      stream: true,
      options: ollamaOptions
    };
    
    // Log compacto
    
    const response = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody),
      signal: options.signal
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Error de Ollama:', errorText);
      throw new Error(`Error del servidor Ollama (${response.status})`);
    }

    // Leer el stream de respuesta
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let fullResponse = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            if (data.message && data.message.content) {
              fullResponse += data.message.content;
            }
          } catch (e) {
            // Ignorar l√≠neas que no sean JSON v√°lido
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return fullResponse;
  }

  /**
   * Enviar mensaje a modelo local con streaming y callbacks
   */
  async sendToLocalModelStreamingWithCallbacks(modelId, messages, callbacks = {}, options = {}) {
    const ollamaUrl = this.getOllamaUrl();
    
    // Preparar opciones que se enviar√°n a Ollama (usar valores de options directamente)
    const ollamaOptions = {
      temperature: options.temperature ?? 0.7,
      num_predict: options.maxTokens ?? 4000,
      num_ctx: options.contextLimit ?? 8000,
      top_k: options.top_k ?? 40,
      top_p: options.top_p ?? 0.9,
      repeat_penalty: options.repeat_penalty ?? 1.1
    };
    
    // üîç DEBUG: Verificar si es un modelo reasoning
    const isReasoningModel = modelId && (modelId.includes('deepseek-r1') || modelId.includes('o1') || modelId.includes('reasoning'));
    if (isReasoningModel) {
      debugLogger.debug('AIService.Reasoning', 'Modelo reasoning detectado en streaming, esperando reasoning_content', {
        model: modelId
      });
    }
    
    // Preparar el body completo que se enviar√° a Ollama
    const requestBody = {
      model: modelId,
      messages: messages,
      stream: true,
      options: ollamaOptions
    };
    
    // Log compacto
    
    const response = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody),
      signal: options.signal
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Error de Ollama:', errorText);
      throw new Error(`Error del servidor Ollama (${response.status})`);
    }

    // Callback de estado: generando
    if (callbacks.onStatus) {
      callbacks.onStatus({
        status: 'generating',
        message: 'Generando respuesta...',
        model: modelId,
        provider: 'local'
      });
    }

    // Leer el stream de respuesta
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let fullResponse = '';
    let reasoningContent = ''; // ‚úÖ ACUMULAR REASONING durante streaming

    try {
      while (true) {
        const { done, value } = await reader.read();
        
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            
            // ‚úÖ CAPTURAR REASONING UNIVERSAL: Buscar reasoning en TODOS los modelos
            // Ollama puede devolver reasoning en diferentes campos seg√∫n el modelo
            const reasoningChunk = data.message?.reasoning_content || 
                                   data.reasoning_content || 
                                   data.message?.reasoning ||
                                   data.reasoning ||
                                   data.message?.thinking ||
                                   data.thinking ||
                                   data.message?.chain_of_thought ||
                                   data.chain_of_thought ||
                                   null;
            
            if (reasoningChunk && typeof reasoningChunk === 'string' && reasoningChunk.trim().length > 0) {
              reasoningContent += reasoningChunk;
              // Notificar reasoning incremental si hay callback
              if (callbacks.onReasoning) {
                callbacks.onReasoning({
                  reasoning: reasoningContent,
                  model: modelId,
                  provider: 'local',
                  isComplete: false
                });
              }
            }
            
            // üîç DEBUG: Log solo para modelos reasoning conocidos o cuando se detecta reasoning
            const isReasoningModel = modelId && (modelId.includes('deepseek-r1') || modelId.includes('o1') || modelId.includes('reasoning'));
            if (isReasoningModel || reasoningChunk) {
              if (data.message && Object.keys(data.message).length > 0) {
                const messageKeys = Object.keys(data.message);
                debugLogger.debug('AIService.Reasoning', 'Campos detectados en streaming', {
                  model: modelId,
                  keys: messageKeys,
                  hasReasoning: !!reasoningChunk,
                  reasoningLength: reasoningChunk ? reasoningChunk.length : 0,
                  preview: JSON.stringify(data.message).substring(0, 200)
                });
              }
            }
            
            if (data.message && data.message.content) {
              fullResponse += data.message.content;
              
              // Callback de streaming
              if (callbacks.onStream) {
                callbacks.onStream({
                  content: data.message.content,
                  fullResponse,
                  model: modelId,
                  provider: 'local'
                });
              }
            }
          } catch (e) {
            // Ignorar l√≠neas que no sean JSON v√°lido
          }
        }
      }
      
      // ‚úÖ NOTIFICAR REASONING COMPLETO al finalizar streaming
      if (reasoningContent && callbacks.onReasoning) {
        callbacks.onReasoning({
          reasoning: reasoningContent,
          model: modelId,
          provider: 'local',
          isComplete: true
        });
      }
    } finally {
      reader.releaseLock();
    }

    return fullResponse;
  }

  /**
   * Enviar mensaje a modelo local sin streaming con callbacks
   */
  async sendToLocalModelNonStreamingWithCallbacks(modelId, messages, callbacks = {}, options = {}) {
    const ollamaUrl = this.getOllamaUrl();
    
    // Preparar opciones que se enviar√°n a Ollama (usar valores de options directamente)
    const ollamaOptions = {
      temperature: options.temperature ?? 0.7,
      num_predict: options.maxTokens ?? 4000,
      num_ctx: options.contextLimit ?? 8000,
      top_k: options.top_k ?? 40,
      top_p: options.top_p ?? 0.9,
      repeat_penalty: options.repeat_penalty ?? 1.1
    };
    
    // Preparar el body completo que se enviar√° a Ollama
    const requestBody = {
      model: modelId,
      messages: messages,
      stream: false,
      options: ollamaOptions
    };
    
    // Log compacto
    
    // Callback de estado: generando
    if (callbacks.onStatus) {
      callbacks.onStatus({
        status: 'generating',
        message: 'Generando respuesta...',
        model: modelId,
        provider: 'local'
      });
    }

    const response = await fetch(`${ollamaUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(requestBody),
      signal: options.signal
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Error de Ollama:', errorText);
      throw new Error(`Error del servidor Ollama (${response.status})`);
    }

    const data = await response.json();
    
    // ‚úÖ CAPTURAR REASONING UNIVERSAL: Buscar reasoning en TODOS los modelos
    // Ollama puede devolver reasoning en diferentes campos seg√∫n el modelo
    const reasoningContent = data.message?.reasoning_content || 
                             data.reasoning_content || 
                             data.message?.reasoning ||
                             data.reasoning ||
                             data.message?.thinking ||
                             data.thinking ||
                             data.message?.chain_of_thought ||
                             data.chain_of_thought ||
                             null;
    
    // üîç DEBUG: Log solo para modelos reasoning conocidos o cuando se detecta reasoning
    const isReasoningModel = modelId && (modelId.includes('deepseek-r1') || modelId.includes('o1') || modelId.includes('reasoning'));
    if (isReasoningModel || reasoningContent) {
      if (data.message) {
        const messageKeys = Object.keys(data.message);
        debugLogger.debug('AIService.Reasoning', 'Respuesta completa de Ollama (non-streaming with callbacks)', {
          model: modelId,
          messageKeys: messageKeys,
          hasReasoning: !!reasoningContent,
          reasoningLength: reasoningContent ? reasoningContent.length : 0,
          allKeys: Object.keys(data),
          preview: JSON.stringify(data).substring(0, 500)
        });
      }
    }
    
    // Si hay callbacks y hay reasoning, notificarlo
    if (callbacks && callbacks.onReasoning && reasoningContent) {
      callbacks.onReasoning({
        reasoning: reasoningContent,
        model: modelId,
        provider: 'local',
        isComplete: true
      });
    }
    
    // La respuesta de Ollama viene en data.message.content
    if (data.message && data.message.content) {
      return data.message.content;
    } else {
      throw new Error('Respuesta inv√°lida del modelo local');
    }
  }

  /**
   * Descargar modelo local usando Ollama
   */
  async downloadLocalModel(modelId, onProgress = null) {
    const model = this.getAllLocalModels().find(m => m.id === modelId);
    if (!model) {
      throw new Error('Modelo no encontrado');
    }

    try {
       // Usar la API de Ollama para descargar el modelo
       const ollamaUrl = this.getOllamaUrl();
       const response = await fetch(`${ollamaUrl}/api/pull`, {
         method: 'POST',
         headers: {
           'Content-Type': 'application/json'
         },
        body: JSON.stringify({
          name: modelId,
          stream: true
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Error de Ollama al descargar:', errorText);
        throw new Error(`Error descargando modelo (${response.status})`);
      }

      // Leer el stream de respuesta para mostrar progreso
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let downloadComplete = false;

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) {
          downloadComplete = true;
          break;
        }

        // Decodificar el chunk y procesar el progreso
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n').filter(line => line.trim());
        
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            
            // Llamar callback de progreso si est√° disponible
            if (onProgress && data.status) {
              const progress = {
                status: data.status,
                total: data.total,
                completed: data.completed,
                percent: data.total ? (data.completed / data.total) * 100 : 0
              };
              onProgress(progress);
            }
            
            // Verificar si la descarga est√° completa
            if (data.status === 'success' || data.status === 'complete') {
              downloadComplete = true;
            }
          } catch (e) {
            // Ignorar l√≠neas que no sean JSON v√°lido
          }
        }
      }

      if (downloadComplete) {
        // Marcar como descargado
        model.downloaded = true;
        this.saveConfig();
        
        // Refrescar la lista de modelos instalados
        await this.detectOllamaModels();
        
        return true;
      } else {
        throw new Error('La descarga no se complet√≥ correctamente');
      }
    } catch (error) {
      console.error('Error descargando modelo local:', error);
      
      if (error.message.includes('Failed to fetch')) {
        throw new Error('No se pudo conectar con Ollama. Verifica que est√© ejecut√°ndose.');
      }
      
      throw error;
    }
  }

  /**
   * Eliminar modelo local
   */
  async deleteLocalModel(modelId) {
    const model = this.getAllLocalModels().find(m => m.id === modelId);
    if (!model) {
      throw new Error('Modelo no encontrado');
    }

    try {
       // Eliminar modelo usando Ollama
       const ollamaUrl = this.getOllamaUrl();
       const response = await fetch(`${ollamaUrl}/api/delete`, {
         method: 'DELETE',
         headers: {
           'Content-Type': 'application/json'
         },
        body: JSON.stringify({
          name: modelId
        })
      });

      if (!response.ok) {
        throw new Error('Error eliminando modelo');
      }

      // Marcar como no descargado
      model.downloaded = false;
      this.saveConfig();

      return true;
    } catch (error) {
      console.error('Error eliminando modelo local:', error);
      throw error;
    }
  }

  /**
   * Limpiar historial de conversaci√≥n
   */
  clearHistory() {
    this.conversationHistory = [];
    this._filesystemModified = false; // Reset flag al limpiar historial
  }

  /**
   * Obtener historial de conversaci√≥n
   */
  getHistory() {
    return this.conversationHistory;
  }

  /**
   * Cargar historial desde localStorage
   */
  loadHistory(conversationId) {
    try {
      const histories = JSON.parse(localStorage.getItem('ai-conversations') || '{}');
      this.conversationHistory = histories[conversationId] || [];
    } catch (error) {
      debugLogger.error('AIService.History', 'Error cargando historial', {
        error: error?.message
      });
    }
  }

  /**
   * Guardar historial en localStorage
   */
  saveHistory(conversationId) {
    try {
      const histories = JSON.parse(localStorage.getItem('ai-conversations') || '{}');
      histories[conversationId] = this.conversationHistory;
      localStorage.setItem('ai-conversations', JSON.stringify(histories));
    } catch (error) {
      debugLogger.error('AIService.History', 'Error guardando historial', {
        error: error?.message
      });
    }
  }

  _prepareMessagesForContext(messages = []) {
    if (!Array.isArray(messages)) return [];
    const MAX_FULL_CONTEXT_CHARS = 4000;
    
    return messages.map((msg) => {
      if (!msg || typeof msg !== 'object') return msg;
      const metadata = msg.metadata || {};
      const clone = { ...msg };
      if (metadata.isToolResult) {
        const toolName = (metadata.toolName || '').toLowerCase();
        if (TOOLS_REQUIRE_FULL_CONTEXT.has(toolName)) {
          const raw = metadata.toolResultText || clone.content || '';
          clone.content = raw.slice(0, MAX_FULL_CONTEXT_CHARS);
          return clone;
        }
        const summary = metadata.toolResultSummary || summarizeToolResult({
          toolName: metadata.toolName || 'tool',
          args: metadata.toolArgs || {},
          resultText: metadata.toolResultText || clone.content || '',
          isError: metadata.error === true,
          maxResultChars: 240
        });
        clone.content = summary;
        return clone;
      }
      if (metadata.isToolObservation && typeof clone.content === 'string') {
        clone.content = clone.content.trim();
        return clone;
      }
      return clone;
    });
  }

  _buildToolObservation({ toolName, args, resultText, isError, lastUserGoal, inferredIntent }) {
    const summary = summarizeToolResult({
      toolName,
      args,
      resultText,
      isError,
      maxResultChars: 320
    });

    const readableName = toolName || 'herramienta';
    const lines = [
      `${isError ? '‚ö†Ô∏è' : 'üîß'} ${isError ? 'Error en' : 'Resultado de'} ${readableName}`,
      summary
    ];

    if (lastUserGoal) {
      lines.push(`üéØ Objetivo del usuario: ${lastUserGoal}`);
    }

    if (!isError && inferredIntent === 'move') {
      lines.push('PISTA: Si ya ves el origen y el destino en el resultado anterior, usa la herramienta "move_file" con los par√°metros EXACTOS del schema (from/to, source/destination u old/new).');
    }
    
    if (toolName && TOOLS_REQUIRE_FULL_CONTEXT.has(toolName.toLowerCase())) {
      const raw = (resultText || '').trim();
      if (raw.length > 0) {
        lines.push('üìã Detalle completo:\n' + raw.slice(0, 4000));
      }
    }

    if (isError) {
      lines.push('Describe claramente el fallo y propone al usuario el siguiente paso o sugiere otra herramienta con JSON v√°lido si es necesario.');
    } else {
      // CR√çTICO: Si el objetivo requiere m√°s acciones, genera el tool call DIRECTAMENTE
      // NO generes texto explicativo como "He encontrado..." o "Ahora ejecutar√©..."
      // Si necesitas otra herramienta ‚Üí SOLO genera el JSON del tool call, sin texto
      if (lastUserGoal) {
        const goalLower = lastUserGoal.toLowerCase();
        const hasMultipleActions = /\s+y\s+|\s+,\s+|\s+then\s+|y\s+(luego|despu√©s|ahora)/i.test(goalLower);
        if (hasMultipleActions) {
          lines.push('‚ö†Ô∏è El objetivo tiene m√∫ltiples acciones. Si solo completaste una, genera el tool call para la siguiente. NO expliques, solo genera el JSON.');
        }
      }
      lines.push('Formato tool call: {"tool":"<server>__<name>","arguments":{...}}');
      lines.push('‚ùå NO generes texto como "He encontrado..." o "Ahora ejecutar√©...". Si necesitas otra herramienta, genera SOLO el tool call JSON.');
    }

    return {
      observation: lines.filter(Boolean).join('\n'),
      summary
    };
  }

  /**
   * ü™ü VENTANA DESLIZANTE INTELIGENTE POR TOKENS
   * Sistema como ChatGPT/Claude - trunca autom√°ticamente sin bloquear al usuario
   * @param {Array} messages - Todos los mensajes de la conversaci√≥n
   * @param {Object} options - Configuraciones de modelo (contextLimit, etc.)
   * @returns {Array} Mensajes limitados por tokens
   */
  smartTokenBasedHistoryLimit(messages, options) {
    if (!messages || messages.length === 0) return [];

    const contextLimit = options.contextLimit || 16000; // L√≠mite en tokens
    const reserveTokensForResponse = 2000; // Reservar espacio para la respuesta
    const targetLimit = contextLimit - reserveTokensForResponse;

    // Calcular tokens por mensaje usando funci√≥n simple
    const messagesWithTokens = messages.map(msg => {
      const content = msg.content || '';
      // Detecci√≥n simple de idioma espa√±ol para c√°lculo preciso
      const hasSpanish = /[√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú¬ø¬°]/.test(content);
      const ratio = hasSpanish ? 3.5 : 4; // tokens por caracter
      const tokens = Math.ceil(content.length / ratio);
      
      return {
        ...msg,
        estimatedTokens: tokens
      };
    });

    // Calcular tokens totales
    let totalTokens = messagesWithTokens.reduce((sum, msg) => sum + msg.estimatedTokens, 0);

    // Si estamos dentro del l√≠mite, devolver todos los mensajes
    if (totalTokens <= targetLimit) {
      return messages;
    }

    // üî™ TRUNCAMIENTO INTELIGENTE (como los grandes modelos)
    debugLogger.debug('AIService.History', 'Ventana deslizante activada', {
      totalTokens,
      targetLimit
    });

    // Estrategia: mantener los mensajes m√°s recientes hasta alcanzar el l√≠mite
    let truncatedMessages = [];
    let runningTotal = 0;
    let truncatedCount = 0;

    // Empezar desde el final (mensajes m√°s recientes)
    for (let i = messagesWithTokens.length - 1; i >= 0; i--) {
      const msg = messagesWithTokens[i];
      
      // Si agregar este mensaje nos pasar√≠a del l√≠mite
      if (runningTotal + msg.estimatedTokens > targetLimit) {
        truncatedCount = i + 1; // Contar mensajes eliminados
        break;
      }
      
      runningTotal += msg.estimatedTokens;
      truncatedMessages.unshift(msg); // Agregar al principio
    }

    // Intentar preservar coherencia de pares (user-assistant)
    if (truncatedMessages.length > 0) {
      const firstMsg = truncatedMessages[0];
      
      // Si el primer mensaje es de assistant, intentar incluir el user anterior
      if (firstMsg.role === 'assistant' && truncatedCount > 0) {
        const previousMsg = messagesWithTokens[truncatedCount - 1];
        if (previousMsg.role === 'user' && 
            runningTotal + previousMsg.estimatedTokens <= targetLimit * 1.05) { // 5% de tolerancia
          truncatedMessages.unshift(previousMsg);
          truncatedCount--;
        }
      }
    }

    // Registro para transparencia (como ChatGPT - opcional y sutil)
    if (truncatedCount > 0) {
      debugLogger.debug('AIService.History', 'Contexto optimizado', {
        mensajesArchivados: truncatedCount,
        tokensLiberados: totalTokens - runningTotal
      });
      
      // Notificaci√≥n sutil opcional (muy discreta, como los grandes modelos)
      this.lastContextOptimization = {
        messagesArchived: truncatedCount,
        tokensFreed: totalTokens - runningTotal,
        timestamp: Date.now()
      };
    }

    return truncatedMessages;
  }


  /**
   * Detectar archivos mencionados en la respuesta - VERSI√ìN SIMPLIFICADA
   * Solo extrae bloques de c√≥digo de la respuesta actual
   * 
   * üîí AUDITOR√çA DE SEGURIDAD:
   * - SOLO procesa 'content' (respuesta actual de la IA)
   * - NUNCA incluye historial de conversaciones anteriores
   * - NUNCA busca en contenido del usuario
   * - 'userMessage' solo se usa para detectar INTENCI√ìN (edici√≥n vs archivo nuevo)
   * - Flujo: AIChatPanel.js l√≠nea 326 ‚Üí data.response (respuesta actual)
   * - data.response viene de sendMessageWithCallbacks (l√≠nea 981-986)
   * - sendMessageWithCallbacks retorna SOLO respuesta nueva, NO historial
   */
  detectFilesInResponse(content, userMessage = '') {
    if (!content) return [];
    
    const files = [];
    const seenFiles = new Set();
    
    // PASO 1: Extraer SOLO bloques de c√≥digo formales: ```lenguaje\ncode```
    // Regex cr√≠tica: esto es lo √öNICO que se procesa
    const codeBlockRegex = /```(\w+)?\s*\n([\s\S]*?)```/g;
    let match;
    let blockIndex = 0;
    
    while ((match = codeBlockRegex.exec(content)) !== null) {
      blockIndex++;
      const language = (match[1] || 'txt').trim().toLowerCase();
      const code = match[2].trim();
      const blockStartPosition = match.index;
      
      // Solo aceptar bloques con contenido real (m√°s de 20 caracteres)
      if (code.length < 20) continue;
      
      // Solo aceptar lenguajes de programaci√≥n v√°lidos
      const validLanguages = {
        'python': 'py',
        'javascript': 'js',
        'typescript': 'ts',
        'html': 'html',
        'css': 'css',
        'jsx': 'jsx',
        'tsx': 'tsx',
        'json': 'json',
        'xml': 'xml',
        'yaml': 'yml',
        'markdown': 'md',
        'bash': 'sh',
        'shell': 'sh',
        'sh': 'sh',
        'powershell': 'ps1',
        'batch': 'bat',
        'cmd': 'cmd',
        'java': 'java',
        'cpp': 'cpp',
        'c': 'c',
        'csharp': 'cs',
        'go': 'go',
        'rust': 'rs',
        'php': 'php',
        'ruby': 'rb',
        'perl': 'pl',
        'swift': 'swift',
        'kotlin': 'kt',
        'scala': 'scala',
        'dart': 'dart',
        'lua': 'lua',
        'r': 'r',
        'julia': 'jl',
        'haskell': 'hs',
        'erlang': 'erl',
        'elixir': 'ex',
        'clojure': 'clj',
        'fsharp': 'fs',
        'ocaml': 'ml',
        'prolog': 'pl',
        'lisp': 'lisp',
        'scheme': 'scm',
        'racket': 'rkt',
        'd': 'd',
        'nim': 'nim',
        'crystal': 'cr',
        'zig': 'zig',
        'v': 'v',
        'sql': 'sql',
        'matlab': 'm',
        'octave': 'm',
        'fortran': 'f90',
        'assembly': 'asm',
        'vhdl': 'vhdl',
        'verilog': 'v',
        'tcl': 'tcl',
        'ada': 'adb',
        'cobol': 'cob',
        'pascal': 'pas'
      };
      
      if (!(language in validLanguages)) continue;
      
      // PASO 2: PRIMERO buscar t√≠tulo markdown antes del bloque de c√≥digo
      const titleFromMarkdown = this.extractTitleFromMarkdown(content, blockStartPosition);
      let fileName;
      
      if (titleFromMarkdown) {
        const extension = this.getLanguageExtension(language);
        fileName = `${titleFromMarkdown}.${extension}`;
      } else {
        // Si no hay t√≠tulo markdown, usar la l√≥gica original
        fileName = this.generateDescriptiveFileName(code, language, blockIndex, userMessage);
      }
      
      // Si generateDescriptiveFileName retorna null, IGNORAR este bloque (ej: comandos bash simples)
      if (fileName === null) continue;
      
      // Evitar duplicados - si el nombre ya existe, agregar sufijo √∫nico
      let uniqueFileName = fileName;
      let counter = 1;
      while (seenFiles.has(uniqueFileName)) {
        const fileParts = fileName.split('.');
        const name = fileParts.slice(0, -1).join('.');
        const extension = fileParts[fileParts.length - 1];
        uniqueFileName = `${name}_${counter}.${extension}`;
        counter++;
      }
      
      files.push(uniqueFileName);
      seenFiles.add(uniqueFileName);
    }
    
    return files;
  }

  /**
   * Generar nombre simple de archivo basado en el contenido
   * Detecta si es una edici√≥n o un archivo nuevo
   */
  generateSimpleFileName(code, language, blockIndex, userMessage) {
    const extensions = {
      'python': 'py',
      'javascript': 'js',
      'typescript': 'ts',
      'html': 'html',
      'css': 'css',
      'jsx': 'jsx',
      'tsx': 'tsx',
      'json': 'json',
      'xml': 'xml',
      'bash': 'sh',
      'shell': 'sh',
      'sh': 'sh',
      'java': 'java',
      'cpp': 'cpp',
      'c': 'c',
      'go': 'go',
      'rust': 'rs',
      'php': 'php',
      'ruby': 'rb',
      'sql': 'sql'
    };
    
    const ext = extensions[language] || language;
    
    // FILTRO ESPECIAL: Para bash/sh, diferenciar entre comandos simples y scripts
    if (language === 'bash' || language === 'shell' || language === 'sh') {
      // Si es un comando simple (una l√≠nea o pocas l√≠neas de comandos), IGNORAR
      const lines = code.split('\n').filter(l => l.trim() && !l.trim().startsWith('#'));
      
      // Verificar si es un script real (tiene estructura de script)
      const isRealScript = code.includes('#!/') || // Shebang
                          code.includes('function ') || // Definici√≥n de funci√≥n
                          code.includes('for ') || // Loops
                          code.includes('while ') ||
                          code.includes('if ') || // Condicionales
                          code.includes('case ') ||
                          lines.length > 3; // M√°s de 3 l√≠neas de verdadero c√≥digo
      
      // Si es solo comandos simples (una o dos l√≠neas), NO generar archivo
      if (!isRealScript && lines.length <= 2) {
        return null; // Retornar null para ignorar este bloque
      }
    }
    
    // DETECTAR EDICIONES: Si es c√≥digo incompleto o fragmento, probablemente es edici√≥n
    const isEditionIndicators = [
      'document.getElementById', // Edici√≥n de HTML existente
      'addEventListener', // Edici√≥n de script existente
      'querySelector', // Edici√≥n de HTML/CSS
      'fetch(', // Edici√≥n de API
      'const', // Edici√≥n de funci√≥n existente
      'function ', // Nueva funci√≥n en archivo existente
      'export ', // Edici√≥n de m√≥dulo
      'import ', // Edici√≥n de imports
    ];
    
    const hasEditionIndicators = isEditionIndicators.some(indicator => code.includes(indicator));
    
    // Si tiene indicadores de edici√≥n Y no es un archivo completo, es una edici√≥n
    const isCompleteFile = code.includes('<!DOCTYPE') || 
                          code.includes('if __name__') ||
                          code.includes('function main') ||
                          code.includes('package main');
    
    const isEdition = hasEditionIndicators && !isCompleteFile;
    
    // 1. Si el usuario pidi√≥ expl√≠citamente algo, usar ese nombre
    if (userMessage) {
      const userLower = userMessage.toLowerCase();
      
      // Palabras clave que indican edici√≥n
      const editKeywords = ['a√±ade', 'agrega', 'add', 'edit', 'modifica', 'update', 'edita', 'improve', 'mejorar', 'incluye', 'include'];
      const isEditRequest = editKeywords.some(kw => userLower.includes(kw));
      
      // Si es solicitud de edici√≥n y el c√≥digo parece una edici√≥n, retornar archivo conocido
      if (isEditRequest && isEdition) {
        // Detectar qu√© archivo es probablemente una edici√≥n
        if (language === 'html' && code.includes('<')) return 'index.html';
        if (language === 'javascript' && code.includes('document.')) return 'index.js';
        if (language === 'javascript' && code.includes('app.on')) return 'main.js';
        if (language === 'css') return 'styles.css';
        if (language === 'json' && code.includes('"')) return 'package.json';
      }
      
      // Electron
      if (userLower.includes('electron') && language === 'javascript') {
        if (code.includes('BrowserWindow') || code.includes('app.on')) return 'main.js';
        if (code.includes('<html') || code.includes('<!DOCTYPE')) return 'index.html';
      }
      
      // React
      if ((userLower.includes('react') || userLower.includes('componente')) && language === 'jsx') {
        return `Component.${ext}`;
      }
      
      // Package.json
      if (userLower.includes('package.json') && language === 'json') {
        return 'package.json';
      }
    }
    
    // 2. Detectar archivos especiales por contenido
    if (language === 'html' && code.includes('<!DOCTYPE')) {
      return 'index.html';
    }
    if (language === 'json' && code.includes('"name"') && code.includes('"version"')) {
      return 'package.json';
    }
    if (language === 'javascript' && code.includes('module.exports')) {
      return 'index.js';
    }
    if (language === 'css' && code.split('\n').length > 2) {
      return 'styles.css';
    }
    
    // 3. Si parece una edici√≥n pero no sabemos cu√°l archivo, usar nombre gen√©rico
    if (isEdition) {
      const typeNames = {
        'python': 'script',
        'javascript': 'index',
        'typescript': 'index',
        'html': 'index',
        'css': 'styles',
        'json': 'config',
        'java': 'App',
        'cpp': 'main',
        'sql': 'query',
        'bash': 'script',
        'shell': 'script',
        'sh': 'script'
      };
      
      const baseName = typeNames[language] || 'file';
      return `${baseName}.${ext}`; // Sin n√∫meros para ediciones
    }
    
    // 4. Nombre gen√©rico basado en tipo para archivos nuevos
    const typeNames = {
      'python': 'script',
      'javascript': 'script',
      'typescript': 'script',
      'html': 'page',
      'css': 'styles',
      'json': 'config',
      'java': 'App',
      'cpp': 'main',
      'sql': 'query',
      'bash': 'script',
      'shell': 'script',
      'sh': 'script'
    };
    
    const baseName = typeNames[language] || 'file';
    return `${baseName}_${blockIndex}.${ext}`;
  }

  /**
   * Verificar si el c√≥digo es significativo y merece ser un archivo
   */
  isSignificantCode(code, language) {
    // Criterios m√°s flexibles para detectar scripts completos
    const minLength = 20; // Muy bajo para scripts peque√±os pero completos
    const lines = code.split('\n');
    const lineCount = lines.length;
    
    // Para Python, ser m√°s inclusivo con scripts de prueba
    if (language === 'python') {
      const hasPythonStructure = (
        code.includes('def ') || 
        code.includes('class ') || 
        code.includes('import ') ||
        code.includes('if __name__') ||
        code.includes('main()') ||
        code.includes('assert ') ||
        code.includes('print(') ||
        code.includes('for ') ||
        code.includes('if ') ||
        code.includes('while ') ||
        code.includes('try:') ||
        code.includes('except:') ||
        code.includes('input(') ||
        code.includes('return ')
      );
      
      // Un script Python es significativo si:
      // 1. Tiene estructura Python Y (es suficientemente largo O tiene m√∫ltiples l√≠neas)
      // 2. O es un script completo con main() o if __name__
      const isCompleteScript = code.includes('if __name__') || code.includes('main()');
      const hasGoodLength = code.length > minLength;
      const hasMultipleLines = lineCount > 2;
      
      return hasPythonStructure && (isCompleteScript || (hasGoodLength && hasMultipleLines));
    }
    
    // Para otros lenguajes, mantener criterios m√°s estrictos
    const hasStructure = (
      code.includes('import ') || 
      code.includes('def ') || 
      code.includes('class ') || 
      code.includes('function ') || 
      code.includes('const ') || 
      code.includes('let ') || 
      code.includes('var ') ||
      code.includes('public class') ||
      code.includes('#include') ||
      code.includes('package ') ||
      code.includes('export ') ||
      code.includes('module.exports') ||
      code.includes('require(') ||
      code.includes('from ') ||
      code.includes('@') // Decoradores
    );
    
    const hasContent = code.length > minLength;
    const hasMultipleLines = lineCount > 3;
    
    return hasStructure && hasContent && hasMultipleLines;
  }

  /**
   * Calcular la significancia del c√≥digo para seleccionar el mejor
   */
  calculateCodeSignificance(code, language) {
    let score = 0;
    
    // Puntuaci√≥n base por longitud (m√°s c√≥digo = m√°s significativo)
    score += Math.min(code.length / 100, 10);
    
    // Puntuaci√≥n por estructura - m√°s espec√≠fica para Python
    const structureKeywords = {
      'python': [
        'def ', 'class ', 'import ', 'if __name__', 'main()',
        'assert ', 'print(', 'for ', 'if ', 'while ', 'try:', 'except:',
        'return ', 'yield ', 'lambda ', 'with ', 'as '
      ],
      'javascript': ['function', 'const ', 'class ', 'export ', 'import '],
      'java': ['public class', 'public static void main', 'import '],
      'cpp': ['int main', 'class ', '#include']
    };
    
    const keywords = structureKeywords[language] || structureKeywords['python'];
    keywords.forEach(keyword => {
      if (code.includes(keyword)) {
        score += 2;
      }
    });
    
    // Puntuaci√≥n especial para scripts Python completos
    if (language === 'python') {
      // Script completo con main
      if (code.includes('if __name__') && code.includes('main()')) {
        score += 10; // Puntuaci√≥n alta para scripts completos
      }
      
      // Script con funciones definidas
      if (code.includes('def ')) {
        score += 5;
      }
      
      // Script con pruebas (assert)
      if (code.includes('assert ')) {
        score += 3;
      }
      
      // Script con bucles o condicionales
      if (code.includes('for ') || code.includes('while ') || code.includes('if ')) {
        score += 2;
      }
      
      // Script con manejo de errores
      if (code.includes('try:') || code.includes('except:')) {
        score += 2;
      }
      
      // Script con input del usuario
      if (code.includes('input(')) {
        score += 2;
      }
    }
    
    // Puntuaci√≥n por funcionalidad espec√≠fica
    const functionalityKeywords = {
      'python': ['random', 'randint', 'suma', 'resta', 'multiplicacion', 'division', 'celsius', 'fahrenheit', 'prueba', 'test'],
      'javascript': ['express', 'react', 'vue', 'angular', 'api', 'server'],
      'java': ['@SpringBootApplication', '@RestController', '@Service', '@Entity'],
      'cpp': ['iostream', 'vector', 'string', 'algorithm']
    };
    
    const funcKeywords = functionalityKeywords[language] || functionalityKeywords['python'];
    funcKeywords.forEach(keyword => {
      if (code.toLowerCase().includes(keyword.toLowerCase())) {
        score += 3;
      }
    });
    
    // Puntuaci√≥n por comentarios y documentaci√≥n
    const commentCount = (code.match(/#|\/\/|\/\*/g) || []).length;
    score += Math.min(commentCount, 5);
    
    // Puntuaci√≥n por l√≠neas de c√≥digo
    const lineCount = code.split('\n').length;
    score += Math.min(lineCount / 10, 5);
    
    return score;
  }

  /**
   * Verificar si se debe crear un archivo (evitar duplicados innecesarios)
   */
  shouldCreateFile(code, language, existingFiles) {
            const extension = this.getLanguageExtension(language);
    
    // Si ya hay archivos del mismo tipo, ser m√°s selectivo
    const sameTypeFiles = existingFiles.filter(f => f.endsWith(`.${extension}`));
    if (sameTypeFiles.length >= 1) {
      // Solo crear si es realmente √∫nico o importante
      return this.isUniqueCode(code, language) || this.isImportantCode(code, language);
    }
    
    // Si es el primer archivo de este tipo, permitir si es significativo
    return this.isSignificantCode(code, language);
  }

  /**
   * Verificar si el c√≥digo es √∫nico (no duplicado)
   */
  isUniqueCode(code, language) {
    // Buscar caracter√≠sticas √∫nicas del c√≥digo
    const uniquePatterns = {
      'python': [/def\s+\w+/, /class\s+\w+/, /import\s+\w+/],
      'javascript': [/function\s+\w+/, /const\s+\w+/, /class\s+\w+/],
      'java': [/public\s+class\s+\w+/, /public\s+static\s+void\s+main/],
      'cpp': [/int\s+main\s*\(/, /class\s+\w+/, /#include/]
    };
    
    const patterns = uniquePatterns[language] || [];
    return patterns.some(pattern => pattern.test(code));
  }

  /**
   * Verificar si el c√≥digo es importante (merece ser archivo)
   */
  isImportantCode(code, language) {
    const importantKeywords = {
      'python': ['def ', 'class ', 'import ', 'if __name__'],
      'javascript': ['function', 'const ', 'class ', 'export '],
      'java': ['public class', 'public static void main'],
      'cpp': ['int main', 'class ', '#include'],
      'html': ['<!DOCTYPE', '<html', '<head', '<body'],
      'css': ['@media', '@keyframes', 'body', 'html'],
      'sql': ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE']
    };
    
    const keywords = importantKeywords[language] || [];
    return keywords.some(keyword => code.includes(keyword));
  }

  /**
   * Generar nombre de archivo descriptivo basado en el contenido del c√≥digo
   */
  generateDescriptiveFileName(code, language, index, userMessage = '') {
    const extension = this.getLanguageExtension(language);
    
    // FILTRO ESPECIAL: Para bash/sh, diferenciar entre comandos simples y scripts
    if (language === 'bash' || language === 'shell' || language === 'sh') {
      // Si es un comando simple (una l√≠nea o pocas l√≠neas de comandos), IGNORAR
      const lines = code.split('\n').filter(l => l.trim() && !l.trim().startsWith('#'));
      
      // Verificar si es un script real (tiene estructura de script)
      const isRealScript = code.includes('#!/') || // Shebang
                          code.includes('function ') || // Definici√≥n de funci√≥n
                          code.includes('for ') || // Loops
                          code.includes('while ') ||
                          code.includes('if ') || // Condicionales
                          code.includes('case ') ||
                          lines.length > 3; // M√°s de 3 l√≠neas de verdadero c√≥digo
      
      // Si es solo comandos simples (una o dos l√≠neas), NO generar archivo
      if (!isRealScript && lines.length <= 2) {
        return null; // Retornar null para ignorar este bloque
      }
    }
    
    // 1. PRIMERO: Intentar generar nombre basado en la solicitud del usuario
    const nameFromUserRequest = this.extractNameFromUserRequest(userMessage, language);
    if (nameFromUserRequest) {
      return `${nameFromUserRequest}.${extension}`;
    }
    
    // 2. Buscar t√≠tulos y descripciones en comentarios
    const titleFromComments = this.extractTitleFromComments(code, language);
    if (titleFromComments) {
      return `${titleFromComments}.${extension}`;
    }
    
    // 3. Buscar descripci√≥n del prop√≥sito en el contexto (m√°s espec√≠fico)
    const purposeFromContext = this.extractPurposeFromContext(code, language);
    if (purposeFromContext) {
      return `${purposeFromContext}.${extension}`;
    }
    
    // 4. Buscar patrones espec√≠ficos de funcionalidad
    const functionalityName = this.extractFunctionalityName(code, language);
    if (functionalityName) {
      return `${functionalityName}.${extension}`;
    }
    
    // 5. Buscar nombres de funciones principales
    const mainFunctionName = this.extractMainFunctionName(code, language);
    if (mainFunctionName) {
      return `${mainFunctionName}.${extension}`;
    }
    
    // 6. Analizar el contenido del c√≥digo para generar nombre descriptivo
    const contentBasedName = this.generateNameFromCodeContent(code, language, userMessage);
    if (contentBasedName) {
      return `${contentBasedName}.${extension}`;
    }
    
    // 7. Si no se encuentra nada descriptivo, usar un nombre gen√©rico pero m√°s espec√≠fico
    const genericNames = {
      'python': 'script_python',
      'javascript': 'script_js',
      'typescript': 'script_ts',
      'java': 'script_java',
      'cpp': 'script_cpp',
      'c': 'script_c',
      'html': 'page_html',
      'css': 'styles_css',
      'sql': 'query_sql'
    };
    
    const baseName = genericNames[language] || 'script';
    return `${baseName}.${extension}`;
  }

  /**
   * Generar nombre basado en el contenido del c√≥digo
   */
  generateNameFromCodeContent(code, language, userMessage = '') {
    const codeLower = code.toLowerCase();
    const messageLower = userMessage.toLowerCase();
    
    // Patrones espec√≠ficos para diferentes tipos de c√≥digo
    if (codeLower.includes('csv') || messageLower.includes('csv')) {
      return 'procesar_csv';
    }
    if (codeLower.includes('pandas') || codeLower.includes('dataframe')) {
      return 'analisis_datos';
    }
    if (codeLower.includes('import csv') || codeLower.includes('csv.reader')) {
      return 'lector_csv';
    }
    if (codeLower.includes('def ') && codeLower.includes('csv')) {
      return 'funciones_csv';
    }
    if (codeLower.includes('class ') && codeLower.includes('csv')) {
      return 'clase_csv';
    }
    if (codeLower.includes('pandas') && codeLower.includes('read_csv')) {
      return 'pandas_csv';
    }
    if (codeLower.includes('to_excel') || codeLower.includes('excel')) {
      return 'exportar_excel';
    }
    if (codeLower.includes('json') && codeLower.includes('load')) {
      return 'procesar_json';
    }
    if (codeLower.includes('api') || codeLower.includes('requests')) {
      return 'cliente_api';
    }
    if (codeLower.includes('web') || codeLower.includes('scraping')) {
      return 'web_scraper';
    }
    if (codeLower.includes('database') || codeLower.includes('sql')) {
      return 'base_datos';
    }
    if (codeLower.includes('test') || codeLower.includes('unittest')) {
      return 'test_unitario';
    }
    if (codeLower.includes('main') && codeLower.includes('if __name__')) {
      return 'script_principal';
    }
    
    return null;
  }

  /**
   * Extraer nombre basado en la solicitud del usuario
   */
  extractNameFromUserRequest(userMessage, language) {
    if (!userMessage) return null;
    
    const message = userMessage.toLowerCase();
    
    // Patrones de solicitudes comunes del usuario
    const requestPatterns = {
      'calculadora': ['calculadora', 'calculadora basica', 'operaciones basicas', 'sumar restar multiplicar dividir', 'calculadora simple'],
      'generador_numeros': ['generar numeros', 'numeros aleatorios', 'random', 'generar numero', 'numero aleatorio'],
      'sumador': ['sumar numeros', 'suma', 'sumar', 'suma de numeros'],
      'conversor_temperatura': ['conversor', 'temperatura', 'celsius fahrenheit', 'convertir temperatura'],
      'promedio': ['promedio', 'calcular promedio', 'media'],
      'manejador_archivos': ['manejar archivos', 'leer archivo', 'escribir archivo', 'archivos'],
      'web_scraper': ['scraper', 'web scraping', 'extraer datos', 'scraping'],
      'api_client': ['api', 'cliente api', 'llamar api', 'consumir api'],
      'base_datos': ['base de datos', 'database', 'sql', 'consulta'],
      'automatizacion': ['automatizar', 'automatizacion', 'tarea automatica', 'cron'],
      'escaner_redes': ['escanear redes', 'escanear red', 'redes locales', 'escanear dispositivos', 'red local', 'escanear red local', 'dispositivos red', 'redes', 'escanear'],
      'monitor_sistema': ['monitor', 'monitorear', 'sistema', 'recursos', 'cpu', 'memoria', 'disco'],
      'backup_archivos': ['backup', 'respaldo', 'copiar archivos', 'respaldo archivos'],
      'conversor_archivos': ['convertir archivos', 'conversor archivos', 'formato archivo'],
      'generador_passwords': ['generar contrase√±as', 'passwords', 'contrase√±as', 'generar password'],
      'analizador_logs': ['analizar logs', 'logs', 'analizar archivos log', 'log files']
    };
    
    // Buscar el patr√≥n que mejor coincida con la solicitud del usuario
    let bestMatch = null;
    let bestScore = 0;
    
    for (const [name, keywords] of Object.entries(requestPatterns)) {
      let score = 0;
      keywords.forEach(keyword => {
        if (message.includes(keyword)) {
          score += keyword.length; // Puntuaci√≥n basada en la longitud de la palabra clave
        }
      });
      
      if (score > bestScore) {
        bestScore = score;
        bestMatch = name;
      }
    }
    
    // Solo devolver si hay una coincidencia significativa
    return bestScore > 5 ? bestMatch : null;
  }

  /**
   * Extraer nombre basado en la funcionalidad espec√≠fica del c√≥digo
   */
  extractFunctionalityName(code, language) {
    const functionalityPatterns = {
      'python': {
        'generador_numeros': ['random', 'randint', 'aleatorio', 'generar_numero', 'numero_aleatorio'],
        'calculadora_basica': ['suma', 'resta', 'multiplicacion', 'division', 'calculadora', 'operaciones', 'opcion', 'elija'],
        'sumador_numeros': ['sumar', 'suma', 'numeros', 'cantidad', 'ingrese'],
        'promedio_numeros': ['promedio', 'calcular_promedio', 'numeros', 'promediar'],
        'conversor_temperatura': ['celsius', 'fahrenheit', 'convertir', 'temperatura', 'grados'],
        'manejador_archivos': ['open', 'read', 'write', 'file', 'path', 'os'],
        'web_scraper': ['requests', 'beautifulsoup', 'scrape', 'url', 'html'],
        'api_client': ['requests', 'api', 'http', 'get', 'post', 'json'],
        'base_datos': ['sqlite', 'mysql', 'postgres', 'database', 'db'],
        'automatizacion': ['schedule', 'cron', 'automate', 'task', 'job']
      },
      'javascript': {
        'web_app': ['express', 'react', 'vue', 'angular', 'dom', 'html'],
        'api_server': ['express', 'fastify', 'koa', 'api', 'server'],
        'procesador_datos': ['json', 'array', 'map', 'filter', 'reduce'],
        'utilidad': ['util', 'helper', 'common', 'shared', 'tool']
      },
      'java': {
        'aplicacion_spring': ['@SpringBootApplication', '@RestController', '@Service'],
        'modelo_datos': ['@Entity', '@Table', '@Column', 'model', 'entity'],
        'utilidad': ['util', 'helper', 'common', 'shared', 'tool']
      }
    };
    
    const patterns = functionalityPatterns[language] || functionalityPatterns['python'];
    
    // Buscar el patr√≥n con m√°s coincidencias
    let bestMatch = null;
    let bestScore = 0;
    
    for (const [name, keywords] of Object.entries(patterns)) {
      let score = 0;
      keywords.forEach(keyword => {
        if (code.toLowerCase().includes(keyword.toLowerCase())) {
          score++;
        }
      });
      
      if (score > bestScore) {
        bestScore = score;
        bestMatch = name;
      }
    }
    
    // Solo devolver si hay al menos 2 coincidencias
    return bestScore >= 2 ? bestMatch : null;
  }

  /**
   * Extraer nombre de la funci√≥n principal
   */
  extractMainFunctionName(code, language) {
    const mainFunctionPatterns = {
      'python': [
        /def\s+(\w+)\s*\([^)]*\):/,  // def function_name():
        /def\s+main\s*\([^)]*\):/,   // def main():
        /def\s+(\w+)\s*\([^)]*\):\s*"""/  // def function_name(): """
      ],
      'javascript': [
        /function\s+(\w+)\s*\([^)]*\)/,  // function functionName()
        /const\s+(\w+)\s*=\s*\([^)]*\)\s*=>/,  // const functionName = () =>
        /export\s+(?:default\s+)?function\s+(\w+)/  // export function functionName
      ],
      'java': [
        /public\s+static\s+void\s+(\w+)\s*\([^)]*\)/,  // public static void methodName()
        /public\s+class\s+(\w+)/  // public class ClassName
      ]
    };
    
    const patterns = mainFunctionPatterns[language] || mainFunctionPatterns['python'];
    
    // Lista de nombres de funciones gen√©ricas que no deben usarse
    const genericFunctionNames = [
      'main', 'test', 'example', 'demo', 'sample', 'temp', 'tmp', 'func', 'function', 'method'
    ];
    
    // Lista de nombres de funciones espec√≠ficas que S√ç deben usarse
    const specificFunctionNames = [
      'fahrenheit_a_celsius', 'celsius_a_fahrenheit', 'conversor_temperatura',
      'calcular_promedio', 'verificar_par_impar', 'generar_contrasena'
    ];
    
    for (const pattern of patterns) {
      const match = code.match(pattern);
      if (match) {
        const functionName = match[1];
        if (functionName && 
            (specificFunctionNames.includes(functionName.toLowerCase()) ||
             (!genericFunctionNames.includes(functionName.toLowerCase()) && functionName.length > 3))) {
          return functionName.toLowerCase();
        }
      }
    }
    
    return null;
  }

  /**
   * Extraer t√≠tulo de markdown que aparece antes del bloque de c√≥digo
   */
  extractTitleFromMarkdown(content, blockStartPosition) {
    // Obtener el texto antes del bloque de c√≥digo
    const textBeforeBlock = content.substring(0, blockStartPosition);
    
    // Buscar t√≠tulos markdown hacia atr√°s desde la posici√≥n del bloque
    const lines = textBeforeBlock.split('\n').reverse();
    
    for (let i = 0; i < Math.min(lines.length, 10); i++) { // Buscar hasta 10 l√≠neas atr√°s
      const line = lines[i].trim();
      
      // Patrones de t√≠tulos markdown - CORREGIDOS seg√∫n los logs reales
      const titlePatterns = [
        // **Script 1: Conversor de temperatura** (formato real de tus logs)
        /^\*\*Script\s*\d*:\s*(.+?)\*\*$/i,
        // **Ejemplo 1: Calculadora**
        /^\*\*Ejemplo\s*\d*:\s*(.+?)\*\*$/i,
        // ## Script 1: Par o Impar
        /^#+\s*Script\s*\d*:\s*(.+)$/i,
        // ## Ejemplo 1: Calculadora  
        /^#+\s*Ejemplo\s*\d*:\s*(.+)$/i,
        // ## 1. Conversor de temperatura
        /^#+\s*\d+\.\s*(.+)$/i,
        // ## Conversor de temperatura
        /^#+\s*([A-Z][^#\n]{3,100})$/i,
        // Script 1: Par o Impar (sin formato)
        /^Script\s*\d*:\s*(.+)$/i,
        // Ejemplo: Calculadora (sin formato) 
        /^Ejemplo\s*\d*:\s*(.+)$/i,
        // Casos espec√≠ficos 
        /^(Juego\s+de\s+Adivina\s+el\s+N√∫mero)$/i,
        /^(Generador\s+de\s+n√∫meros?\s+aleatorios?)$/i,
        /^(Conversor\s+de\s+temperatura)$/i,
        // Cualquier t√≠tulo descriptivo con palabras clave
        /^(.*(?:juego|generador|calculadora|conversor|sistema|programa).*[a-zA-Z\s]{5,60})$/i,
        // T√≠tulo que empiece con may√∫scula y tenga al menos 3 palabras
        /^([A-Z][a-z]+\s+[a-z]+\s+[A-Z][a-z]+.*?)$/i,
        // T√≠tulo simple con palabras descriptivas
        /^([A-Z][a-zA-Z\s]{8,60})$/i
      ];
      
      for (const pattern of titlePatterns) {
        const match = line.match(pattern);
        if (match && match[1]) {
          const titleText = match[1].trim();
          return this.sanitizeFileName(titleText);
        }
      }
      
      // Si encontramos una l√≠nea que no est√° vac√≠a y no es un t√≠tulo, dejar de buscar
      if (line.length > 0 && !line.match(/^```/) && !line.match(/^\s*$/)) {
        break;
      }
    }
    
    return null;
  }

  /**
   * Convertir texto a nombre de archivo v√°lido
   */
  sanitizeFileName(text) {
    return text
      .toLowerCase()
      .replace(/[^\w\s\-]/g, '') // Remover caracteres especiales
      .replace(/\s+/g, '_') // Espacios a guiones bajos
      .replace(/_+/g, '_') // M√∫ltiples guiones bajos a uno solo
      .replace(/^_|_$/g, '') // Remover guiones bajos al inicio y final
      .substring(0, 50); // Limitar longitud
  }

  /**
   * Extraer t√≠tulo de comentarios en el c√≥digo
   */
  extractTitleFromComments(code, language) {
    // Buscar t√≠tulos m√°s espec√≠ficos y descriptivos
    const titlePatterns = {
      'python': [
        /#\s*Ejemplo:\s*([^#\n]+)/,        // Ejemplo: T√≠tulo
        /#\s*Script:\s*([^#\n]+)/,         // Script: T√≠tulo
        /#\s*Programa:\s*([^#\n]+)/,       // Programa: T√≠tulo
        /#\s*Calculadora\s+([^#\n]+)/,      // Calculadora T√≠tulo
        /#\s*Sumador\s+([^#\n]+)/,         // Sumador T√≠tulo
        /#\s*Conversor\s+([^#\n]+)/,       // Conversor T√≠tulo
        /#\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)/,  // T√≠tulos con may√∫sculas
        /"""\s*([^"]{3,30})\s*"""/         // Docstrings
      ],
      'javascript': [
        /\/\/\s*Ejemplo:\s*([^\/\n]+)/,    // Ejemplo: T√≠tulo
        /\/\/\s*Script:\s*([^\/\n]+)/,     // Script: T√≠tulo
        /\/\/\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)/,  // T√≠tulos con may√∫sculas
        /\/\*\s*([^*]{3,30})\s*\*\//       // Comentarios de bloque
      ],
      'java': [
        /\/\/\s*Ejemplo:\s*([^\/\n]+)/,    // Ejemplo: T√≠tulo
        /\/\/\s*Class:\s*([^\/\n]+)/,      // Class: T√≠tulo
        /\/\/\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)/,  // T√≠tulos con may√∫sculas
        /\/\*\s*([^*]{3,30})\s*\*\//       // Comentarios de bloque
      ],
      'cpp': [
        /\/\/\s*Ejemplo:\s*([^\/\n]+)/,    // Ejemplo: T√≠tulo
        /\/\/\s*Program:\s*([^\/\n]+)/,    // Program: T√≠tulo
        /\/\/\s*([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)/,  // T√≠tulos con may√∫sculas
        /\/\*\s*([^*]{3,30})\s*\*\//       // Comentarios de bloque
      ]
    };
    
    const patterns = titlePatterns[language] || titlePatterns['python'];
    
    for (const pattern of patterns) {
      const match = code.match(pattern);
      if (match) {
        let title = match[1].trim();
        
        // Limpiar y formatear el t√≠tulo de manera m√°s inteligente
        title = title
          .toLowerCase()
          .replace(/[^a-z0-9\s]/g, '')  // Solo letras, n√∫meros y espacios
          .replace(/\s+/g, '_')         // Espacios a guiones bajos
          .replace(/_+/g, '_')          // M√∫ltiples guiones bajos a uno
          .replace(/^_|_$/g, '')        // Quitar guiones al inicio y final
          .substring(0, 30);           // Limitar longitud
        
        // Solo usar si es un t√≠tulo v√°lido (no muy corto ni gen√©rico)
        if (title.length > 3 && !this.isGenericTitle(title)) {
          return title;
        }
      }
    }
    
    return null;
  }

  /**
   * Verificar si un t√≠tulo es gen√©rico y no debe usarse
   */
  isGenericTitle(title) {
    const genericTitles = [
      'script', 'program', 'code', 'example', 'ejemplo',
      'import', 'definir', 'funcion', 'function', 'class',
      'main', 'principal', 'basic', 'basico', 'simple',
      'configuracion', 'configuracion de', 'configuracion de la',
      'configuracion de la interfaz', 'configuracion de la red',
      'configuracion de la red local', 'configuracion de la red local',
      'configuracion de la red local', 'configuracion de la red local'
    ];
    
    return genericTitles.some(generic => 
      title.toLowerCase().includes(generic) || 
      title.toLowerCase() === generic
    );
  }

  /**
   * Extraer prop√≥sito del contexto del c√≥digo
   */
  extractPurposeFromContext(code, language) {
    // Buscar palabras clave que indiquen el prop√≥sito espec√≠fico
    const purposeKeywords = {
      'python': {
        'prueba_for': ['for ', 'frutas', 'bucle', 'iteracion', 'lista', 'append'],
        'verificador_par_impar': ['% 2', 'par', 'impar', 'numero % 2', 'es par', 'es impar'],
        'prueba_if': ['if ', 'condicional', 'assert', 'verificar', 'validar'],
        'prueba_while': ['while ', 'bucle', 'iteracion', 'condicion'],
        'prueba_funciones': ['def ', 'funcion', 'parametros', 'return'],
        'prueba_clases': ['class ', 'objeto', 'metodo', 'constructor'],
        'prueba_manejo_errores': ['try:', 'except:', 'error', 'excepcion'],
        'analizador_csv': ['csv', 'analizar', 'archivo', 'columnas', 'filas', 'frecuencia'],
        'lista_tareas': ['tareas', 'agregar', 'mostrar', 'pendientes', 'opcion', 'menu'],
        'calculadora_interactiva': ['calcular', 'operacion', 'ingrese', 'numeros', 'resultado'],
        'calculadora_basica': ['suma', 'resta', 'multiplicacion', 'division', 'calculadora', 'operaciones'],
        'sumador_numeros': ['sumar', 'suma', 'numeros', 'cantidad', 'ingrese'],
        'promedio_numeros': ['promedio', 'calcular_promedio', 'numeros', 'promediar'],
        'conversor_temperatura': ['celsius', 'fahrenheit', 'convertir', 'temperatura', 'grados'],
        'data_analysis': ['pandas', 'numpy', 'dataframe', 'csv', 'json', 'analysis'],
        'web_scraper': ['requests', 'beautifulsoup', 'scrape', 'url', 'html'],
        'api_client': ['requests', 'api', 'http', 'get', 'post', 'json'],
        'file_handler': ['open', 'read', 'write', 'file', 'path', 'os'],
        'database': ['sqlite', 'mysql', 'postgres', 'database', 'db'],
        'automation': ['schedule', 'cron', 'automate', 'task', 'job']
      },
      'javascript': {
        'web_app': ['express', 'react', 'vue', 'angular', 'dom', 'html'],
        'api_server': ['express', 'fastify', 'koa', 'api', 'server'],
        'data_processor': ['json', 'array', 'map', 'filter', 'reduce'],
        'utility': ['util', 'helper', 'common', 'shared', 'tool']
      },
      'java': {
        'spring_app': ['@SpringBootApplication', '@RestController', '@Service'],
        'data_model': ['@Entity', '@Table', '@Column', 'model', 'entity'],
        'utility': ['util', 'helper', 'common', 'shared', 'tool']
      }
    };
    
    const keywords = purposeKeywords[language] || purposeKeywords['python'];
    
    // Buscar el prop√≥sito m√°s espec√≠fico primero - ordenar por especificidad
    const sortedPurposes = Object.entries(keywords).sort((a, b) => {
      // Contar cu√°ntas palabras clave coinciden para cada prop√≥sito
      const aMatches = a[1].filter(word => code.toLowerCase().includes(word.toLowerCase())).length;
      const bMatches = b[1].filter(word => code.toLowerCase().includes(word.toLowerCase())).length;
      return bMatches - aMatches; // M√°s coincidencias primero
    });
    
    for (const [purpose, words] of sortedPurposes) {
      const matchingWords = words.filter(word => 
        code.toLowerCase().includes(word.toLowerCase())
      );
      
      // Solo considerar si tiene al menos 2 coincidencias O es muy espec√≠fico
      if (matchingWords.length >= 2 || 
          (matchingWords.length >= 1 && ['celsius', 'fahrenheit', 'temperatura', 'pandas', 'numpy', 'beautifulsoup'].some(specific => 
            matchingWords.some(match => match.includes(specific))))) {
        return purpose;
      }
    }
    
    // Si no encuentra prop√≥sito espec√≠fico, buscar patrones generales
    const generalPatterns = {
      'python': {
        'script_prueba': ['assert ', 'print(', 'prueba', 'test'],
        'script_basico': ['def ', 'if __name__', 'main()'],
        'script_bucle': ['for ', 'while ', 'bucle'],
        'script_condicional': ['if ', 'elif ', 'else:', 'condicional'],
        'script_interactivo': ['input(', 'while True', 'opcion', 'menu'],
        'script_tareas': ['tareas', 'agregar', 'mostrar', 'pendientes'],
        'script_calculadora': ['calcular', 'operacion', 'numeros', 'resultado'],
        'calculadora': ['suma', 'resta', 'multiplicacion', 'division'],
        'sumador': ['sumar', 'suma', 'numeros'],
        'conversor': ['convertir', 'celsius', 'fahrenheit', 'grados']
      }
    };
    
    const patterns = generalPatterns[language] || generalPatterns['python'];
    
    for (const [purpose, words] of Object.entries(patterns)) {
      const hasPatterns = words.some(word => 
        code.toLowerCase().includes(word.toLowerCase())
      );
      
      if (hasPatterns) {
        return purpose;
      }
    }
    
    return null;
  }

  /**
   * Verificar si el c√≥digo es relevante al contexto de la solicitud del usuario
   */
  isRelevantToContext(code, userContext, language) {
    if (!userContext || userContext.trim() === '') return true; // Si no hay contexto, aceptar
    
    const codeLower = code.toLowerCase();
    const userContextLower = userContext.toLowerCase();
    
    debugLogger.debug('AIService.Relevance', 'Validando relevancia', {
      userContext: userContext.substring(0, 50),
      language,
      codePreview: code.substring(0, 50)
    });
    
    // Validar por tipo de solicitud espec√≠fica CON MAYOR PRECISI√ìN
    if (userContextLower.includes('electron')) {
      const isRelevant = codeLower.includes('electron') || 
                         codeLower.includes('app.on') || 
                         codeLower.includes('browserwindow') ||
                         codeLower.includes('createwindow') ||
                         codeLower.includes('const { app, browserwindow }') ||
                         codeLower.includes('loadurl') ||
                         codeLower.includes('index.html') ||
                         codeLower.includes('<!doctype html>') ||
                         codeLower.includes('<html') ||
                         codeLower.includes('mi aplicaci√≥n') ||
                         codeLower.includes('aplicaci√≥n electr√≥nica');
      
      debugLogger.debug('AIService.Relevance', 'Electron validation', {
        isRelevant,
        hasElectron: codeLower.includes('electron'),
        hasAppOn: codeLower.includes('app.on'),
        hasBrowserWindow: codeLower.includes('browserwindow'),
        hasLoadURL: codeLower.includes('loadurl'),
        hasIndexHTML: codeLower.includes('index.html'),
        hasHTML: codeLower.includes('<!doctype html>') || codeLower.includes('<html')
      });
      
      return isRelevant;
    }
    
    if (userContextLower.includes('react')) {
      const isRelevant = codeLower.includes('react') || 
                         codeLower.includes('import react') ||
                         codeLower.includes('from "react"');
      
      debugLogger.debug('AIService.Relevance', 'React validation', { isRelevant });
      return isRelevant;
    }
    
    if (userContextLower.includes('vue')) {
      const isRelevant = codeLower.includes('vue') || 
                         codeLower.includes('import') && codeLower.includes('vue');
      
      debugLogger.debug('AIService.Relevance', 'Vue validation', { isRelevant });
      return isRelevant;
    }
    
    if (userContextLower.includes('python') || userContextLower.includes('pandas')) {
      const isRelevant = codeLower.includes('import') || 
                         codeLower.includes('def ') || 
                         codeLower.includes('pandas');
      
      debugLogger.debug('AIService.Relevance', 'Python validation', { isRelevant });
      return isRelevant;
    }
    
    if (userContextLower.includes('web scraper') || userContextLower.includes('scraper')) {
      const isRelevant = codeLower.includes('scraper') || 
                         codeLower.includes('requests') ||
                         codeLower.includes('beautifulsoup') ||
                         codeLower.includes('fetch(');
      
      debugLogger.debug('AIService.Relevance', 'Web scraper validation', { isRelevant });
      return isRelevant;
    }
    
    if (userContextLower.includes('data analysis') || userContextLower.includes('analisis de datos')) {
      const isRelevant = codeLower.includes('pandas') || 
                         codeLower.includes('numpy') ||
                         codeLower.includes('dataframe') ||
                         codeLower.includes('csv');
      
      debugLogger.debug('AIService.Relevance', 'Data analysis validation', { isRelevant });
      return isRelevant;
    }
    
    // Detectar archivos JavaScript/HTML gen√©ricos si el contexto es de desarrollo
    if (userContextLower.includes('proyecto') || userContextLower.includes('aplicaci√≥n') || 
        userContextLower.includes('app') || userContextLower.includes('desarrollo')) {
      
      const isRelevant = (language === 'javascript' && (
        codeLower.includes('function') || 
        codeLower.includes('const ') || 
        codeLower.includes('let ') ||
        codeLower.includes('var ') ||
        codeLower.includes('import ') ||
        codeLower.includes('export ')
      )) || (language === 'html' && (
        codeLower.includes('<!doctype') ||
        codeLower.includes('<html') ||
        codeLower.includes('<head') ||
        codeLower.includes('<body')
      ));
      
      debugLogger.debug('AIService.Relevance', 'Generic project validation', {
        isRelevant,
        language,
        hasJS: language === 'javascript',
        hasHTML: language === 'html'
      });
      return isRelevant;
    }
    
    // Si no hay contexto espec√≠fico conocido, RECHAZAR por defecto (m√°s restrictivo)
    debugLogger.debug('AIService.Relevance', 'Contexto no reconocido; rechazando archivo');
    return false;
  }

  /**
   * Obtener extensi√≥n de archivo basada en el lenguaje
   */
  getLanguageExtension(language) {
    const extensions = {
      'python': 'py',
      'javascript': 'js',
      'typescript': 'ts',
      'jsx': 'jsx',
      'tsx': 'tsx',
      'java': 'java',
      'cpp': 'cpp',
      'c': 'c',
      'csharp': 'cs',
      'perl': 'pl',
      'ruby': 'rb',
      'swift': 'swift',
      'kotlin': 'kt',
      'scala': 'scala',
      'rust': 'rs',
      'dart': 'dart',
      'php': 'php',
      'lua': 'lua',
      'r': 'r',
      'matlab': 'm',
      'octave': 'm',
      'fortran': 'f90',
      'haskell': 'hs',
      'erlang': 'erl',
      'elixir': 'ex',
      'clojure': 'clj',
      'fsharp': 'fs',
      'ocaml': 'ml',
      'prolog': 'pl',
      'lisp': 'lisp',
      'scheme': 'scm',
      'racket': 'rkt',
      'd': 'd',
      'nim': 'nim',
      'crystal': 'cr',
      'zig': 'zig',
      'v': 'v',
      'sql': 'sql',
      'matlab': 'm',
      'octave': 'm',
      'fortran': 'f90',
      'assembly': 'asm',
      'vhdl': 'vhdl',
      'verilog': 'v',
      'tcl': 'tcl',
      'ada': 'adb',
      'cobol': 'cob',
      'pascal': 'pas',
      'smalltalk': 'st',
      'forth': 'fth',
      'apl': 'apl',
      'j': 'ijs',
      'k': 'k',
      'q': 'q',
      'wolfram': 'wl',
      'maxima': 'mac',
      'sage': 'sage',
      'maple': 'mpl',
      'mathematica': 'nb',
      'go': 'go',
      'bash': 'sh',
      'shell': 'sh',
      'sql': 'sql',
      'html': 'html',
      'css': 'css',
      'json': 'json',
      'yaml': 'yml',
      'xml': 'xml',
      'markdown': 'md',
      'txt': 'txt'
    };
    return extensions[language] || 'txt';
  }

  /**
   * ‚ÑπÔ∏è NOTA: Validaci√≥n de memoria ahora es PASIVA
   * - El monitoreo solo reporta datos
   * - Las descargas son MANUALES (widget)
   * - Sin auto-descarga autom√°tica
   */

  /**
   * ‚úÖ NUEVO: Calcular contexto din√°mico seg√∫n RAM disponible
   */
  _calcDynamicContext(freeRAMMB) {
    return this.memoryService.calcDynamicContext(freeRAMMB);
  }

  /**
   * üìù Cambiar modelo - SIN auto-descarga
   * La descarga del modelo anterior es MANUAL (widget)
   */
  async switchModel(newModelId, newModelType) {
    const oldModel = this.currentModel;
    const oldType = this.modelType;

    this.currentModel = newModelId;
    this.modelType = newModelType;

    this.saveConfig();
  }

  /**
   * ‚úÖ NUEVO: Cargar modelo autom√°ticamente al reiniciar
   * Intenta cargar el √∫ltimo modelo usado
   */
  async autoLoadLastModel() {
    try {
      const config = JSON.parse(localStorage.getItem('ai-service-config') || '{}');
      
      if (!config.currentModel || !config.modelType) {
        return false;
      }

      const modelId = config.currentModel;
      const modelType = config.modelType;

      // Si es modelo local, verificar que existe
      if (modelType === 'local') {
        const localModel = this.getAllLocalModels().find(m => m.id === modelId);
        
        if (!localModel) {
          console.warn(`[AIService] ‚ö†Ô∏è Modelo local ${modelId} no encontrado`);
          return false;
        }

        if (!localModel.downloaded) {
          console.warn(`[AIService] ‚ö†Ô∏è Modelo ${modelId} no est√° descargado`);
          return false;
        }
      }

      // Si es modelo remoto, verificar que existe
      if (modelType === 'remote') {
        const remoteModel = this.models.remote.find(m => m.id === modelId);
        
        if (!remoteModel) {
          console.warn(`[AIService] ‚ö†Ô∏è Modelo remoto ${modelId} no encontrado`);
          return false;
        }
      }

      // Cargar el modelo
      this.currentModel = modelId;
      this.modelType = modelType;

      // Si es local, usar ModelMemoryService para cargarlo en memoria
      if (modelType === 'local') {
        try {
          // Usar loadModelToMemory que usa /api/generate con keep_alive
          const loaded = await this.memoryService.loadModelToMemory(modelId);
          if (!loaded) {
            console.warn(`[AIService] ‚ö†Ô∏è No se pudo precargar ${modelId}, pero Ollama lo cargar√° autom√°ticamente`);
          }
        } catch (error) {
          console.warn(`[AIService] ‚ö†Ô∏è Error precargando modelo: ${error.message}`);
          // No es cr√≠tico, Ollama cargar√° autom√°ticamente cuando se use
        }
      }

      return true;

    } catch (error) {
      console.error('[AIService] ‚ùå Error al cargar modelo autom√°ticamente:', error);
      return false;
    }
  }
}

// Exportar instancia singleton
export const aiService = new AIService();
export default aiService;

