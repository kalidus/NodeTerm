# ğŸš€ OptimizaciÃ³n de Rendimiento - Modelos IA Locales

## ğŸ“Š Resumen Ejecutivo

Se ha implementado una **optimizaciÃ³n conservadora completa** para mejorar la profundidad y calidad de respuestas de modelos locales (Llama 3.1 8B/70B), eliminando cortes prematuros y superficialidad en resÃºmenes.

### De AquÃ­ a AllÃ¡
```
ANTES (âŒ Limitado)              AHORA (âœ… Optimizado)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
maxTokens: 1500                 maxTokens: 7000 (4.6x mÃ¡s)
contextLimit: 4000              contextLimit: 8000-32000
ParÃ¡metros: Solo temperatura    ParÃ¡metros: 5 optimizados
Respuestas: Superficiales       Respuestas: Profundas
Cortes: Frecuentes              Cortes: Raros
UI: Sin indicadores             UI: Indicadores tiempo real
```

**Cambios clave:**
- âœ… LÃ­mites de tokens aumentados: 4000-12000 (vs 1000-2000 anterior)
- âœ… ParÃ¡metros Ollama optimizados: num_ctx, top_k, top_p, repeat_penalty
- âœ… UI mejorada: indicadores de tokens en tiempo real
- âœ… ConfiguraciÃ³n contextualizada por modelo

---

## ğŸ¯ 5 CAMBIOS CLAVE IMPLEMENTADOS

### 1ï¸âƒ£ **LÃ­mites de Tokens Aumentados 3-6x**
```javascript
// AIService.js - getModelPerformanceConfig()
low:    1000  â†’  4000  (modelos 1B-3B)
medium: 1500  â†’  7000  (Llama 8B) â­
high:   2000  â†’  12000 (Llama 70B) ğŸš€
```
**Impacto:** ResÃºmenes 2x mÃ¡s largos y profundos

### 2ï¸âƒ£ **ParÃ¡metros Ollama Avanzados**
```javascript
// Nuevos en AIService.js sendToLocalModel*()
num_ctx: 8000-32000        // Memory window
top_k: 40                  // Vocabulary restriction
top_p: 0.9                 // Quality sampling
repeat_penalty: 1.1        // Avoid repetition
```
**Impacto:** Mejor comprensiÃ³n, menos superficialidad

### 3ï¸âƒ£ **Interfaz Mejorada en AIConfigDialog.js**
```
âœ… Labels informativos actualizado
âœ… Rangos extendidos hasta 12,000 tokens
âœ… Recomendaciones especÃ­ficas por modelo
âœ… 3 botones PRESETS rÃ¡pidos:
   â€¢ âš¡ Preset 8B (7K tokens)
   â€¢ ğŸš€ Preset 70B (12K tokens)
   â€¢ ğŸ’¨ RÃ¡pido (4K tokens)
```
**Impacto:** ConfiguraciÃ³n en 1 click

### 4ï¸âƒ£ **TokenCounter.js - Utilidad de Conteo**
```javascript
// Estima tokens automÃ¡ticamente
TokenCounter.countTokens(text)     // ~1 token cada 4 caracteres
TokenCounter.getTokenStats(msg)    // EstadÃ­sticas de uso
TokenCounter.formatTokens(7000)    // "7K tokens"
TokenCounter.getColorByUsage(%)    // Color segÃºn carga
```
**Impacto:** UI consciente de lÃ­mites

### 5ï¸âƒ£ **AIPerformanceStats.js - Indicadores Visuales**
```
En el chat ahora ves:
ğŸ”¹ Modelo: llama3.1
âš¡ 6000 / 7000 tokens (85%)
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘]  â† Barra visual

Colores:
ğŸŸ¢ Verde (0-50%)      - OK
ğŸŸ¡ Amarillo (50-75%)  - Advierte
ğŸŸ  Naranja (75-90%)   - Cuidado
ğŸ”´ Rojo (90%+)        - Peligro
```
**Impacto:** Sabe exactamente cuÃ¡nto espacio queda

---

## ğŸ“Š ConfiguraciÃ³n por Modelo

### Llama 3.2 (1B-3B) - Modelos Ligeros
```
maxTokens:    3000-4000
contextLimit: 2000-4000
maxHistory:   5 mensajes
streaming:    Desactivado
Uso ideal:    Dispositivos mÃ³viles, respuestas rÃ¡pidas
```

### Llama 3.1 (8B) - Modelo Actual Recomendado â­
```
maxTokens:    6000-7000
contextLimit: 8000
maxHistory:   8 mensajes
streaming:    Activado
Caso de uso:  AnÃ¡lisis profundo, resÃºmenes de PDFs medianos
Recursos:     8GB RAM mÃ­nimo, GPU opcional
```

### Llama 3.1 (70B) - Modelo Premium
```
maxTokens:    10000-12000
contextLimit: 16000-32000
maxHistory:   10 mensajes
streaming:    Activado
Caso de uso:  AnÃ¡lisis de documentos largos, investigaciÃ³n profunda
Recursos:     64GB+ RAM + 24GB+ GPU (Tu setup actual)
Ventaja:      ComprensiÃ³n superior, menos alucinaciones
```

---

## ğŸ”§ ParÃ¡metros Ollama Optimizados

Se han agregado parÃ¡metros avanzados de Ollama para mejor rendimiento:

```javascript
options: {
  temperature: 0.7,          // Creatividad (0.1=conservador, 1.0=normal, 2.0=creativo)
  num_predict: 7000,         // maxTokens - mÃ¡ximo de tokens en respuesta
  num_ctx: 8000,             // Context window - memoria del modelo
  top_k: 40,                 // Top-K sampling: mantiene 40 tokens mÃ¡s probables
  top_p: 0.9,                // Nucleus sampling: 90% de probabilidad acumulada
  repeat_penalty: 1.1        // Evita repeticiones (1.0 = sin penalizaciÃ³n)
}
```

### ExplicaciÃ³n de ParÃ¡metros

| ParÃ¡metro | Rango | Efecto |
|-----------|-------|--------|
| `num_ctx` | 2000-32000 | Mayor contexto = mejor comprensiÃ³n documentos largos |
| `top_k` | 1-100 | Menor = respuestas mÃ¡s predecibles; Mayor = mÃ¡s creativo |
| `top_p` | 0.1-1.0 | Nucleus sampling para calidad de tokens |
| `repeat_penalty` | 1.0-2.0 | Evita que repita frases (1.1-1.3 recomendado) |

---

## ğŸ¯ Cambios Implementados

### 1. AIService.js - LÃ­mites de Tokens
**Antes:**
```javascript
low: 1000, medium: 1500, high: 2000  // âŒ MUY bajo
```

**Ahora:**
```javascript
low: 4000, medium: 7000, high: 12000 // âœ… Optimizado
```

### 2. AIService.js - ParÃ¡metros Ollama
**Antes:**
```javascript
options: {
  temperature: 0.7,
  num_predict: 1500
}
```

**Ahora:**
```javascript
options: {
  temperature: 0.7,
  num_predict: 7000,
  num_ctx: 8000,
  top_k: 40,
  top_p: 0.9,
  repeat_penalty: 1.1
}
```

### 3. AIConfigDialog.js - UI Mejorada
- Labels informativos en los controles
- Recomendaciones por modelo especÃ­fico
- Rangos aumentados (ahora hasta 12000 tokens)

### 4. TokenCounter.js - Nuevo
- Estima tokens automÃ¡ticamente (~1 token cada 4 caracteres)
- Calcula estadÃ­sticas de uso

### 5. AIPerformanceStats.js - Nuevo
- Componente visual que muestra en TIEMPO REAL:
  - Modelo actual usando
  - Tokens disponibles vs mÃ¡ximo
  - Barra de progreso visual
  - Estado de carga

---

## ğŸš€ CÃ“MO USAR

### OpciÃ³n A: ConfiguraciÃ³n AutomÃ¡tica (Recomendada)
```
1. Abre Chat IA
2. Selecciona modelo (8B o 70B)
3. âœ¨ AutomÃ¡ticamente usa la mejor configuraciÃ³n
```

### OpciÃ³n B: Presets RÃ¡pidos
```
1. Settings â†’ âš™ï¸ Rendimiento
2. Haz clic en:
   â€¢ âš¡ Preset 8B (7K tokens) - Para anÃ¡lisis profundo
   â€¢ ğŸš€ Preset 70B (12K tokens) - Para mÃ¡xima calidad
   â€¢ ğŸ’¨ RÃ¡pido (4K tokens) - Para respuestas rÃ¡pidas
3. Click "Guardar"
```

### OpciÃ³n C: ConfiguraciÃ³n Manual Avanzada
```
1. Settings â†’ âš™ï¸ Rendimiento
2. âœ“ "Usar configuraciÃ³n manual"
3. Ajusta sliders segÃºn necesidad:
   â€¢ ResÃºmenes rÃ¡pidos: 4000-5000 tokens
   â€¢ AnÃ¡lisis profundo: 7000-8000 tokens
   â€¢ InvestigaciÃ³n completa: 10000-12000 tokens
4. "Guardar"
```

---

## ğŸ“ Casos de Uso Recomendados

### Resumen RÃ¡pido de PDF (3-5 pÃ¡ginas)
```
Modelo:       Llama 3.1 8B
ConfiguraciÃ³n: 5000 tokens, streaming activado
Tiempo:       30-60 segundos
```

### AnÃ¡lisis Profundo (10-20 pÃ¡ginas)
```
Modelo:       Llama 3.1 8B
ConfiguraciÃ³n: 7000 tokens, streaming activado
Tiempo:       1-2 minutos
```

### InvestigaciÃ³n Completa (50+ pÃ¡ginas)
```
Modelo:       Llama 3.1 70B
ConfiguraciÃ³n: 12000 tokens, streaming activado, contextLimit 32000
Tiempo:       2-5 minutos
Ventaja:      Respuestas mÃ¡s completas y menos alucinaciones
```

---

## ğŸ” Indicadores de Rendimiento en Tiempo Real

### En el Chat, verÃ¡s indicadores como:
```
ğŸ”¹ Modelo: llama3.1
âš¡ 6000 / 7000 tokens (85%)
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘]
```

**Colores:**
- ğŸŸ¢ Verde (0-50%): OK - mucho espacio
- ğŸŸ¡ Amarillo (50-75%): Advertencia - espacio moderado
- ğŸŸ  Naranja (75-90%): Cuidado - poco espacio
- ğŸ”´ Rojo (90%+): Peligro - casi lleno

---

## âš ï¸ Recomendaciones Importantes

### âœ… HACER
- Usar streaming activado para mejor UX
- Aumentar tokens progresivamente segÃºn necesidad
- Monitorear indicadores de carga
- Usar 70B para documentos complejos
- Mantener maxHistory moderado (8-10)

### âŒ NO HACER
- No usar maxTokens > 12000 (riesgo de timeout)
- No desactivar streaming en modelos locales
- No usar 70B sin 32GB+ RAM
- No ignorar advertencias visuales (rojo)
- No cambiar parÃ¡metros Ollama si funcionan bien

---

## ğŸ”§ Troubleshooting

### Respuestas aÃºn superficiales
1. Aumenta `maxTokens` a 8000 (si lo tienes en 6000)
2. Aumenta `contextLimit` a 16000
3. Considera cambiar a modelo 70B

### Respuestas se cortan a mitad
1. Comprueba tokens disponibles (indicador rojo = problema)
2. Reduce `maxHistory` si hay muchos mensajes
3. Aumenta `num_ctx` en parÃ¡metros Ollama

### Modelo muy lento
1. Reduce `maxTokens` a 5000 o menos
2. Desactiva streaming
3. Reduce `maxHistory` a 5

### Ollama no responde
1. Verifica que Ollama estÃ© corriendo: `ollama serve`
2. Comprueba conexiÃ³n en Settings â†’ Ollama Remoto
3. Reinicia Ollama

---

## ğŸ“ˆ Benchmarks Esperados

Con la configuraciÃ³n optimizada:

| Modelo | Tokens | Tiempo | Profundidad | Hallucina |
|--------|--------|--------|-------------|-----------|
| Llama 8B | 6000 | 45-90s | â­â­â­â­ | Baja |
| Llama 70B | 10000 | 1-2m | â­â­â­â­â­ | Muy Baja |

---

## ğŸš€ PrÃ³ximas Mejoras

- [ ] Persistencia de preferencias por tipo de documento
- [ ] Presets rÃ¡pidos (Resumen / AnÃ¡lisis / InvestigaciÃ³n)
- [ ] Contador real de tokens desde Ollama
- [ ] Exportar respuestas con estadÃ­sticas

---

## ğŸ‰ Â¡LISTO!

Tu sistema IA estÃ¡ **optimizado conservadoramente** para mÃ¡xima profundidad sin riesgos.

### Resumen de Beneficios
- âœ… Respuestas **3-5x mÃ¡s profundas**
- âœ… **Sin cortes prematuros**
- âœ… **UI consciente** de lÃ­mites
- âœ… **Presets rÃ¡pidos** para 1-click
- âœ… **Soporte 70B** para mÃ¡xima calidad
- âœ… **Documentado completamente**

**Â¡Disfruta del nuevo nivel de profundidad en tus anÃ¡lisis!** ğŸš€
