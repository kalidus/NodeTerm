# üìä AN√ÅLISIS PROFUNDO: Gesti√≥n de Memoria de Modelos de IA

## üéØ RESUMEN EJECUTIVO

Actualmente el proyecto **NO tiene control activo de memoria** en modelos de IA. La gesti√≥n es **pasiva y desorganizada**:

- ‚úÖ **Modelos Cloud**: No ocupan RAM local (se alojan en servidores remotos)
- ‚ùå **Modelos Locales (Ollama)**: Se cargan completamente en RAM y se quedan all√≠ indefinidamente
- ‚ùå **No hay monitoreo**: No sabemos cu√°nta memoria usa cada modelo
- ‚ùå **No hay liberaci√≥n**: Los modelos nunca se descargan de la memoria
- ‚ùå **No hay indicadores**: El usuario no sabe qu√© modelos est√°n cargados

---

## üîç FLUJO ACTUAL DE GESTI√ìN

### 1. CARGA DE MODELOS LOCALES (Ollama)

```
Usuario selecciona modelo local
        ‚Üì
AIService.setModel(modelId, 'local')
        ‚Üì
(No ocurre nada especial)
        ‚Üì
Usuario env√≠a mensaje
        ‚Üì
AIService.sendToLocalModel()
        ‚Üì
fetch(`http://localhost:11434/api/chat`)
        ‚Üì
Ollama carga el modelo en RAM (si no est√° cargado)
        ‚Üì
El modelo PERMANECE en memoria indefinidamente
```

**Problema**: Ollama carga autom√°ticamente los modelos. Si cargaste `llama2` de 7B, `mistral` de 7B 
y `neural-chat` de 7B (21GB total), **todos se quedan en RAM**, consumiendo recursos.

---

### 2. API DE OLLAMA QUE USAMOS

**Endpoints actuales:**
```javascript
// ‚úÖ Detectar modelos instalados
GET /api/tags
Response: { models: [{ name: 'llama2:7b', size: 3B, ... }, ...] }

// ‚úÖ Chat con modelo (carga autom√°tica si no est√°)
POST /api/chat
Body: { model: 'llama2', messages: [...], stream: true, options: {...} }

// ‚ùå NUNCA USAMOS: Descargar modelo de memoria
DELETE /api/models/:name
```

---

### 3. FLUJO EN `AIService.sendToLocalModelStreaming()`

```javascript
// src/services/AIService.js (l√≠nea 4406)
async sendToLocalModelStreaming(modelId, messages, options) {
  const ollamaUrl = this.getOllamaUrl();
  
  const ollamaOptions = {
    temperature: options.temperature ?? 0.7,
    num_predict: options.maxTokens ?? 4000,
    num_ctx: options.contextLimit ?? 8000,
    top_k: options.top_k ?? 40,
    top_p: options.top_p ?? 0.9,
    repeat_penalty: options.repeat_penalty ?? 1.1
  };
  
  const requestBody = {
    model: modelId,
    messages: messages,
    stream: true,
    options: ollamaOptions  // üëà Configuraci√≥n para optimizar memoria
  };
  
  // Fetch a Ollama - Ollama carga el modelo si no est√° cargado
  const response = await fetch(`${ollamaUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(requestBody),
    signal: options.signal  // Para cancelaci√≥n
  });
  
  // Streaming de respuesta (lectura en chunks)
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let fullResponse = '';
  
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    const chunk = decoder.decode(value);
    // Procesar chunk...
    fullResponse += data.message.content;
  }
  
  reader.releaseLock();  // üëà Liberar el reader (no la memoria del modelo)
  return fullResponse;
}
```

**Problema**: `reader.releaseLock()` solo libera el reader del stream, **NO la memoria del modelo**.

---

## üìà PROBLEMAS ACTUALES

### Problema 1: Sin monitoreo de memoria
```
No sabemos cu√°nta RAM usa cada modelo:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Modelo          ‚îÇ Par√°metros‚îÇ RAM Est.‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ llama2:7b       ‚îÇ 7B       ‚îÇ 4GB     ‚îÇ
‚îÇ mistral:7b      ‚îÇ 7B       ‚îÇ 4GB     ‚îÇ
‚îÇ neural-chat:7b  ‚îÇ 7B       ‚îÇ 4GB     ‚îÇ
‚îÇ dolphin:7b      ‚îÇ 7B       ‚îÇ 4GB     ‚îÇ
‚îÇ TOTAL POSIBLE   ‚îÇ 28B      ‚îÇ 16GB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Si todo est√° cargado: ¬°16GB de RAM usado! 
El usuario no tiene idea.
```

### Problema 2: Sin control de descarga
```
No hay forma de liberar memoria:

Usuario cambia de modelo ‚Üí Modelo anterior sigue en RAM
Aplicaci√≥n cierra ‚Üí Modelos se descargan (si SO libera memoria)
Usuario reinicia ‚Üí Todos los modelos deben cargarse de nuevo
```

### Problema 3: Sin informaci√≥n en UI
```
El usuario no ve:
- Cu√°l es el modelo actual cargado
- Cu√°nta memoria usa
- Cu√°les otros modelos est√°n en memoria
- Cu√°nta RAM total est√° disponible
```

### Problema 4: Sin optimizaci√≥n de contexto
```
La configuraci√≥n actual es hardcodeada:
num_ctx: 8000  // Contexto m√°ximo (consume M√ÅS memoria)
num_predict: 4000  // Tokens m√°ximos

En sistemas de bajo consumo, esto es excesivo.
No hay forma de ajustar seg√∫n RAM disponible.
```

---

## üîß SOLUCIONES PROPUESTAS

### SOLUCI√ìN 1: Servicio de Gesti√≥n de Memoria
```javascript
// NUEVO: src/services/ModelMemoryService.js

class ModelMemoryService {
  constructor() {
    this.ollamaUrl = 'http://localhost:11434';
    this.loadedModels = new Map(); // { modelName: { size, memory, timestamp } }
    this.memoryLimit = 12000; // MB (ajustable por usuario)
    this.monitoringInterval = null;
  }
  
  // 1. Obtener info de memoria del sistema
  async getSystemMemory() {
    const os = require('os');
    const totalMemory = os.totalmem();
    const freeMemory = os.freemem();
    return {
      total: Math.round(totalMemory / 1024 / 1024), // MB
      free: Math.round(freeMemory / 1024 / 1024),
      used: Math.round((totalMemory - freeMemory) / 1024 / 1024),
      usagePercent: Math.round(((totalMemory - freeMemory) / totalMemory) * 100)
    };
  }
  
  // 2. Obtener modelos cargados en Ollama
  async getLoadedModels() {
    try {
      const response = await fetch(`${this.ollamaUrl}/api/ps`);
      // ‚ö†Ô∏è Nota: /api/ps est√° disponible en Ollama v0.1.20+
      const data = await response.json();
      
      this.loadedModels.clear();
      for (const model of data.models) {
        this.loadedModels.set(model.name, {
          size: model.size,  // Tama√±o total del modelo
          memory: model.memory,  // RAM usado aproximadamente
          timestamp: Date.now()
        });
      }
      
      return this.loadedModels;
    } catch (error) {
      console.warn('No se puede obtener modelos cargados:', error.message);
      return new Map();
    }
  }
  
  // 3. Descargar modelo de memoria (liberar RAM)
  async unloadModel(modelName) {
    try {
      const response = await fetch(
        `${this.ollamaUrl}/api/delete`,
        {
          method: 'DELETE',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ name: modelName, delete_model: false })
          // delete_model: false = solo quitar de RAM, no borrar archivo
        }
      );
      
      if (!response.ok) {
        throw new Error(`Error: ${response.status}`);
      }
      
      this.loadedModels.delete(modelName);
      console.log(`‚úÖ Modelo ${modelName} descargado de RAM`);
      return true;
    } catch (error) {
      console.error(`‚ùå Error descargando ${modelName}:`, error);
      return false;
    }
  }
  
  // 4. Gesti√≥n autom√°tica de memoria (LRU - Least Recently Used)
  async enforceMemoryLimit() {
    const systemMem = await this.getSystemMemory();
    const usedByModels = Array.from(this.loadedModels.values())
      .reduce((sum, m) => sum + (m.memory || 0), 0);
    
    if (usedByModels > this.memoryLimit) {
      console.warn(`‚ö†Ô∏è Memoria excedida: ${usedByModels}MB > ${this.memoryLimit}MB`);
      
      // Ordenar por timestamp (m√°s antiguos primero)
      const toUnload = Array.from(this.loadedModels.entries())
        .sort((a, b) => a[1].timestamp - b[1].timestamp)
        .slice(0, Math.ceil(this.loadedModels.size / 2)); // Descargar 50%
      
      for (const [modelName] of toUnload) {
        await this.unloadModel(modelName);
      }
    }
  }
  
  // 5. Obtener estad√≠sticas detalladas
  getMemoryStats() {
    const models = Array.from(this.loadedModels.entries()).map(([name, info]) => ({
      name,
      sizeGB: (info.size / 1024 / 1024 / 1024).toFixed(2),
      memoryMB: info.memory,
      loadedSince: new Date(info.timestamp).toLocaleTimeString()
    }));
    
    const totalMemory = models.reduce((sum, m) => sum + m.memoryMB, 0);
    
    return { models, totalMemory };
  }
  
  // 6. Monitoreo continuo
  startMonitoring(intervalMs = 30000) {
    this.monitoringInterval = setInterval(async () => {
      await this.getLoadedModels();
      await this.enforceMemoryLimit();
    }, intervalMs);
    
    console.log('‚úÖ Monitoreo de memoria iniciado');
  }
  
  stopMonitoring() {
    if (this.monitoringInterval) {
      clearInterval(this.monitoringInterval);
      this.monitoringInterval = null;
      console.log('‚õî Monitoreo de memoria detenido');
    }
  }
  
  // 7. Limpieza al cerrar
  async cleanup() {
    this.stopMonitoring();
    // Opcional: descargar todos los modelos
    // for (const modelName of this.loadedModels.keys()) {
    //   await this.unloadModel(modelName);
    // }
  }
}
```

---

### SOLUCI√ìN 2: Integraci√≥n en AIService

```javascript
// En src/services/AIService.js

class AIService {
  constructor() {
    // ...existing...
    this.memoryService = new ModelMemoryService();
  }
  
  // Al seleccionar modelo local
  setModel(modelId, modelType) {
    // ...existing...
    
    if (modelType === 'local') {
      // Mostrar informaci√≥n de memoria
      const stats = this.memoryService.getMemoryStats();
      console.log('Modelos en memoria:', stats);
    }
  }
  
  // Antes de usar modelo
  async sendToLocalModel(message, options = {}) {
    // Actualizar informaci√≥n de modelos cargados
    await this.memoryService.getLoadedModels();
    
    // Si estamos muy ajustados de memoria, descargar un modelo antiguo
    if (this.memoryService.getMemoryStats().totalMemory > 10000) {
      console.warn('Memoria alta, liberando modelo antiguo...');
      await this.memoryService.enforceMemoryLimit();
    }
    
    // Continuar con el env√≠o normal...
    return await this.sendToLocalModelStreaming(...);
  }
  
  // Al cambiar de modelo
  async switchModel(newModelId, newModelType) {
    if (this.modelType === 'local' && newModelType === 'local') {
      // Opcional: descargar modelo anterior despu√©s de 1 minuto
      if (this.currentModel !== newModelId) {
        setTimeout(async () => {
          const confirmed = await userConfirm(
            `¬øDescargar ${this.currentModel} para liberar ${getModelSize(this.currentModel)}MB?`
          );
          if (confirmed) {
            await this.memoryService.unloadModel(this.currentModel);
          }
        }, 60000);
      }
    }
    
    this.currentModel = newModelId;
    this.modelType = newModelType;
  }
}
```

---

### SOLUCI√ìN 3: Indicador en UI

```javascript
// NUEVO: src/components/ModelMemoryIndicator.jsx

import React, { useState, useEffect } from 'react';

const ModelMemoryIndicator = () => {
  const [memoryStats, setMemoryStats] = useState(null);
  const [systemMem, setSystemMem] = useState(null);
  
  useEffect(() => {
    const updateMemory = async () => {
      const stats = memoryService.getMemoryStats();
      const system = await memoryService.getSystemMemory();
      
      setMemoryStats(stats);
      setSystemMem(system);
    };
    
    updateMemory();
    const interval = setInterval(updateMemory, 5000);
    return () => clearInterval(interval);
  }, []);
  
  if (!memoryStats) return null;
  
  const totalUsed = memoryStats.totalMemory;
  const systemUsagePercent = systemMem?.usagePercent || 0;
  
  return (
    <div style={{
      background: 'rgba(0, 0, 0, 0.2)',
      borderRadius: '8px',
      padding: '8px 12px',
      marginBottom: '12px',
      fontSize: '12px',
      fontFamily: 'monospace'
    }}>
      {/* Sistema */}
      <div style={{ marginBottom: '8px', color: '#888' }}>
        üíª Sistema: <strong>{systemMem?.used}MB / {systemMem?.total}MB</strong>
        <div style={{
          background: '#333',
          height: '4px',
          borderRadius: '2px',
          marginTop: '2px',
          overflow: 'hidden'
        }}>
          <div style={{
            background: systemUsagePercent > 80 ? '#ff6b6b' : '#4eccf0',
            height: '100%',
            width: `${systemUsagePercent}%`,
            transition: 'width 0.3s'
          }} />
        </div>
      </div>
      
      {/* Modelos */}
      {memoryStats.models.length > 0 && (
        <div style={{ color: '#4eccf0' }}>
          üß† Modelos en RAM: <strong>{memoryStats.models.length}</strong>
          <div style={{ marginTop: '4px', paddingLeft: '12px' }}>
            {memoryStats.models.map(model => (
              <div key={model.name} style={{
                display: 'flex',
                justifyContent: 'space-between',
                padding: '4px 0',
                borderBottom: '1px solid #333',
                fontSize: '11px'
              }}>
                <span>{model.name}</span>
                <span style={{ color: '#666' }}>
                  {model.sizeGB}GB ({model.memoryMB}MB)
                </span>
              </div>
            ))}
          </div>
          <div style={{
            marginTop: '8px',
            paddingTop: '8px',
            borderTop: '1px solid #333',
            textAlign: 'right',
            color: '#ffd700',
            fontWeight: 'bold'
          }}>
            Total: {(totalUsed / 1024).toFixed(2)}GB
          </div>
        </div>
      )}
      
      {memoryStats.models.length === 0 && (
        <div style={{ color: '#666' }}>
          ‚úÖ Sin modelos en memoria (listo para cargar)
        </div>
      )}
    </div>
  );
};

export default ModelMemoryIndicator;
```

---

### SOLUCI√ìN 4: Configuraci√≥n de L√≠mites

```javascript
// Agregar a AIConfigDialog.js

const memoryLimitOptions = [
  { label: 'Bajo (2GB)', value: 2000, icon: 'üíæ', desc: 'Ideal para laptops' },
  { label: 'Medio (6GB)', value: 6000, icon: 'üñ•Ô∏è', desc: 'Desktop est√°ndar' },
  { label: 'Alto (12GB)', value: 12000, icon: 'üñ•Ô∏èüñ•Ô∏è', desc: 'Workstation' },
  { label: 'Muy Alto (24GB)', value: 24000, icon: 'üî•', desc: 'Server' }
];

// En el di√°logo de rendimiento:
<div style={{ marginBottom: '16px' }}>
  <label style={{ fontWeight: 'bold', display: 'block', marginBottom: '8px' }}>
    üß† L√≠mite de Memoria para Modelos
  </label>
  
  {memoryLimitOptions.map(opt => (
    <button key={opt.value}
      onClick={() => {
        memoryService.memoryLimit = opt.value;
        aiService.saveConfig();
      }}
      style={{
        padding: '8px 12px',
        margin: '4px',
        background: memoryService.memoryLimit === opt.value 
          ? '#4eccf0' 
          : '#222',
        color: '#fff',
        border: 'none',
        borderRadius: '4px',
        cursor: 'pointer'
      }}
    >
      {opt.icon} {opt.label}
    </button>
  ))}
</div>
```

---

## üìä IMPACTO ESPERADO

| Aspecto | Antes | Despu√©s |
|--------|-------|---------|
| **Monitoreo de memoria** | ‚ùå Nada | ‚úÖ Autom√°tico cada 30s |
| **Control de modelos** | ‚ùå Manual/nunca | ‚úÖ Autom√°tico (LRU) |
| **Visibilidad en UI** | ‚ùå No sabe nada | ‚úÖ Widget en tiempo real |
| **Liberaci√≥n de RAM** | ‚ùå Solo cierre app | ‚úÖ A demanda o autom√°tica |
| **Optimizaci√≥n contexto** | ‚ùå Fijo (8000) | ‚úÖ Din√°mico seg√∫n RAM |
| **Duraci√≥n sesiones largas** | ‚ùå RAM se agota | ‚úÖ Equilibrio din√°mico |

---

## üöÄ PLAN DE IMPLEMENTACI√ìN

### Fase 1: Servicio Base
1. Crear `ModelMemoryService.js`
2. Implementar `/api/ps` polling
3. Implementar `/api/delete` para descarga

### Fase 2: Integraci√≥n
1. Integrar en `AIService.js`
2. Agregar monitoreo autom√°tico
3. Implementar LRU

### Fase 3: UI
1. Crear `ModelMemoryIndicator.jsx`
2. Integrar en `AIChatPanel.js`
3. Agregar configuraci√≥n en `AIConfigDialog.js`

### Fase 4: Optimizaci√≥n
1. Contexto din√°mico seg√∫n RAM
2. Predicci√≥n de carga
3. Recomendaciones inteligentes

---

## ‚ö†Ô∏è NOTAS IMPORTANTES

1. **Ollama v0.1.20+**: `/api/ps` requiere versi√≥n reciente de Ollama
2. **No es destructivo**: Descarga de RAM, no borra archivos del modelo
3. **Fallback**: Si Ollama no soporta `/api/ps`, usar cach√© local
4. **Configuraci√≥n**: Guardar l√≠mites de memoria en `config.json`
5. **Hotkeys**: Agregar Ctrl+M para ver estad√≠sticas r√°pidas


